[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Operating Systems and Networs SoSe 25 Solutions",
    "section": "",
    "text": "Preface",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "sh01/01.html",
    "href": "sh01/01.html",
    "title": "1  Blatt 01",
    "section": "",
    "text": "Aufgabe 1\nLearning how to Learn:\nJohn Cleese:",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Blatt 01</span>"
    ]
  },
  {
    "objectID": "sh01/01.html#aufgabe-1",
    "href": "sh01/01.html#aufgabe-1",
    "title": "1  Blatt 01",
    "section": "",
    "text": "Zwei Denkmodi aus „Learning How to Learn“\n\nFokussierter Modus: Zielgerichtetes, konzentriertes Denken. Gut für bekannte Aufgaben und Übung.\nDiffuser Modus: Entspanntes, offenes Denken. Hilft bei neuen Ideen und kreativen Verknüpfungen.\n\nAufgaben und passende Denkmodi\n\nFokussierter Modus\nWarum: Erfordert Konzentration und gezieltes Einprägen.\nZuerst diffuser, dann fokussierter Modus\nWarum: Erst Überblick und Verständnis aufbauen, dann vertiefen.\n\nFokussierter Modus\nWarum: Klare, schrittweise Übung – ideal für fokussiertes Denken.\nBeide Modi\nWarum: Fokussiert für Details & Übungen, diffus für Überblick & Vernetzung.\n\n\n\n\nZwei Denkmodi:\n\nOffener Modus: Locker, spielerisch, kreativ.\nBeispiel: Ideen für eine Geschichte sammeln.\nWarum: Offenheit fördert neue Einfälle.\n\nGeschlossener Modus: Zielgerichtet, angespannt, entscheidungsfreudig.\nBeispiel: Bericht überarbeiten und fertigstellen.\nWarum: Präzises Arbeiten und klare Entscheidungen nötig.\n\n\nVergleich mit „Learning How to Learn“\n\nOffen \\(\\Leftrightarrow\\) Diffus: Für Kreativität und Überblick.\n\nGeschlossen \\(\\Leftrightarrow\\) Fokussiert: Für Detailarbeit und Umsetzung.\n\nAlexander Fleming:\n\nModus: Offen\nWarum: Fleming entdeckte Penicillin zufällig, weil er offen und entspannt war – neugierig statt zielgerichtet. Im geschlossenen Modus hätte er die verschimmelte Petrischale wohl einfach weggeschmissen – zu fokussiert für zufällige Entdeckungen.\n\nAlfred Hitchcock:\n\nModus: Offen\n\nWie: Er erzählte lustige Anekdoten, um das Team zum Lachen zu bringen – so schuf er eine entspannte Atmosphäre, die kreatives Denken förderte.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Blatt 01</span>"
    ]
  },
  {
    "objectID": "sh01/01.html#aufgabe-2",
    "href": "sh01/01.html#aufgabe-2",
    "title": "1  Blatt 01",
    "section": "Aufgabe 2",
    "text": "Aufgabe 2\n\n\nx64: 16 64 Bit GPRs1 \\(\\Rightarrow\\) 16 x 64 b = 16 x 8 B = \\(2^7\\) B.\nAVX2: 16 256 Bit GPRs2 \\(\\Rightarrow\\) 16 x 256 b = 16 x 32 B = \\(2^9\\) B\n\n\nx64: \\(\\frac{2^7}{2^{30}} = \\frac{1}{2^{23}}\\)\nAVX2: \\(\\frac{2^9}{2^{30}} = \\frac{1}{2^{21}}\\)\n\nallgemein gilt: \\(10^3 \\approx 2^{10}\\), und \\(\\frac{2^x}{2^y} = \\frac{1}{2^{y-x}}\\)",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Blatt 01</span>"
    ]
  },
  {
    "objectID": "sh01/01.html#aufgabe-3",
    "href": "sh01/01.html#aufgabe-3",
    "title": "1  Blatt 01",
    "section": "Aufgabe 3",
    "text": "Aufgabe 3\n\nDer Zugriff scheitert, weil der Arbeitsspeicher durch die Memory Protection (z. B. Paging mit Zugriffsrechten) vom Betriebssystem isoliert wird. Nur der Kernel darf die Speicherbereiche aller Prozesse sehen und verwalten.\nEin Prozess kann trotzdem auf Ressourcen anderer Prozesse zugreifen über kontrollierte Schnittstellen wie IPC (Inter-Process Communication), Dateisysteme, Sockets oder Shared Memory, die vom Betriebssystem verwaltet und überwacht werden.\nWelche Risiken entstehen bei höchstem Privileg für alle Prozesse?\n\nSicherheitslücken: Jeder Prozess könnte beliebige Speicherbereiche lesen/schreiben.\n\nStabilitätsprobleme: Fehlerhafte Prozesse könnten das System zum Absturz bringen.\n\nKeine Isolation: Malware hätte vollen Systemzugriff, keine Schutzmechanismen.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Blatt 01</span>"
    ]
  },
  {
    "objectID": "sh01/01.html#aufgabe-4",
    "href": "sh01/01.html#aufgabe-4",
    "title": "1  Blatt 01",
    "section": "Aufgabe 4",
    "text": "Aufgabe 4\nKernel-Code benötigt einen sicheren, kontrollierten Speicherbereich (seinen eigenen Stack), um zu vermeiden:\n\nBeschädigung durch Benutzerprozesse\nAbstürze oder Rechteausweitung (Privilege Escalation)\n\nDaher hat jeder Prozess:\n\nEinen User-Mode-Stack (wird bei normaler Ausführung verwendet)\nEinen Kernel-Mode-Stack (wird bei System Calls und Interrupts verwendet)",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Blatt 01</span>"
    ]
  },
  {
    "objectID": "sh01/01.html#aufgabe-5",
    "href": "sh01/01.html#aufgabe-5",
    "title": "1  Blatt 01",
    "section": "Aufgabe 5",
    "text": "Aufgabe 5\nEntfernte Systemaufrufe\n\n\n\n\n\n\n\nSystemaufruf\nGrund für Entfernung\n\n\n\n\ncreat\nEntspricht vollständig open(path, O_CREAT | O_WRONLY | O_TRUNC, mode).\n\n\ndup\nEntspricht vollständig fcntl(fd, F_DUPFD, 0).\n\n\n\nAlle übrigen Systemaufrufe bieten essenzielle Funktionen, die nicht exakt durch andere ersetzt werden können.\nSie decken ab:\n\nDatei- und Verzeichnisoperationen (open, read, write, unlink, mkdir, etc.)\nProzessmanagement (fork, exec, wait, exit, etc.)\nMetadatenverwaltung (chmod, chown, utime, etc.)\nKommunikation und Steuerung (pipe, kill, ioctl, etc.)\nZeit- und Systemabfragen (time, times, stat, etc.)\n\nOhne sie wären bestimmte Kernfunktionen unmöglich.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Blatt 01</span>"
    ]
  },
  {
    "objectID": "sh01/01.html#aufgabe-6",
    "href": "sh01/01.html#aufgabe-6",
    "title": "1  Blatt 01",
    "section": "Aufgabe 6",
    "text": "Aufgabe 6\nscript.sh auch im Zip:\ncd $1\nwhile :\ndo\n    echo \"5 biggest files in $1:\"\n    ls -S | head -5\n    echo \"5 last modified files starting with '$2' in $1:\"\n    ls -t | grep ^$2 | head -5\n    sleep 5\ndone",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Blatt 01</span>"
    ]
  },
  {
    "objectID": "sh01/01.html#aufgabe-7",
    "href": "sh01/01.html#aufgabe-7",
    "title": "1  Blatt 01",
    "section": "Aufgabe 7",
    "text": "Aufgabe 7\nVorteile:\n\nKomplexitätsreduktion: Abstraktionen verbergen technische Details und erleichtern das Entwickeln und Verstehen von Systemen.\n\nWiederverwendbarkeit: Einmal geschaffene Abstraktionen (z.B. Dateisystem, Prozesse) können flexibel in verschiedenen Programmen genutzt werden.\n\nNachteile:\n\nLeistungsaufwand: Abstraktionsschichten können zusätzliche Rechenzeit und Speicherverbrauch verursachen.\n\nFehlerverdeckung: Probleme in tieferen Schichten bleiben oft verborgen und erschweren Fehlersuche und Optimierung.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Blatt 01</span>"
    ]
  },
  {
    "objectID": "sh01/01.html#footnotes",
    "href": "sh01/01.html#footnotes",
    "title": "1  Blatt 01",
    "section": "",
    "text": "https://www.wikiwand.com/en/articles/X86-64↩︎\nhttps://www.wikiwand.com/en/articles/Advanced_Vector_Extensions↩︎",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Blatt 01</span>"
    ]
  },
  {
    "objectID": "sh02/02.html",
    "href": "sh02/02.html",
    "title": "2  Blatt 02",
    "section": "",
    "text": "Aufgabe 1\nDie Datenstruktur task_struct ist im Linux-Kernel-Quellcode (Linux kernel Version 6.15.0) definiert unter:\ninclude/linux/sched.h\nDie Definition erstreckt sich über die Zeilen 813 bis 1664.\nDarin befinden sich etwa 320 Member-Variablen.\nBei einer Annahme von 8 Byte pro Variable ergibt sich eine geschätzte Größe von:\n2.560 Byte \\(\\approx\\) 2,5 KB",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Blatt 02</span>"
    ]
  },
  {
    "objectID": "sh02/02.html#aufgabe-2",
    "href": "sh02/02.html#aufgabe-2",
    "title": "2  Blatt 02",
    "section": "Aufgabe 2",
    "text": "Aufgabe 2\nDer Systemaufruf fork() erzeugt einen neuen Prozess, der eine Kopie des aufrufenden Prozesses ist (Kindprozess).\nRückgabewert:\n\n0 im Kindprozess\n\nPID des Kindes im Elternprozess\n\n−1 bei Fehler\n\n\nMit dem program:\n#include &lt;stdio.h&gt;\n\nint main(int argc, char const *argv[])\n{\n    int i = 0;\n    if (fork() != 0) i++;\n    if (i != 1) fork();\n    fork();\n    return 0;\n}\nwerden insgesammt 6 Prozesse erzeugt. Graph der enstehenden Prozess hierarchie:\nP1  \n├── P1.1  \n│   └── P1.1.1  \n│       └── P1.1.1.1  \n│   └── P1.1.2  \n└── P1.2  \nSchrittweise Erzeugung der Prozesse:\n\nP1 startet das Programm. Der Wert von i ist anfangs 0.\nDie erste fork()-Anweisung wird ausgeführt:\n\nP1 ist der Elternprozess, der einen neuen Kindprozess P1.1 erzeugt.\nIm Elternprozess (P1) ist das Rückgabewert von fork() ≠ 0 → i wird auf 1 gesetzt.\nIm Kindprozess (P1.1) ist das Rückgabewert 0 → i bleibt 0.\n\nDanach folgt die Bedingung if (i != 1) fork();:\n\nP1 hat i == 1 → keine Aktion.\nP1.1 hat i == 0 → führt eine fork() aus → erzeugt P1.1.1.\n\nSchließlich wird eine letzte fork(); von allen existierenden Prozessen ausgeführt:\n\nP1 erzeugt P1.2\nP1.1 erzeugt P1.1.2\nP1.1.1 erzeugt P1.1.1.1\n\n\n\nDas Programm führt fork() aus, bis ein Kindprozess mit einer durch 10 teilbaren PID entsteht. Jeder fork() erzeugt ein Kind, das sofort endet (die Rückgabe von fork() is 0 bei einem Kind), außer die Bedingung ist erfüllt. Da etwa jede zehnte PID durch 10 teilbar ist, liegt die maximale Prozessanzahl (inkl. Elternprozess) typischerweise bei etwa 11.\nDa PIDs vom Kernel in aufsteigender Reihenfolge als nächste freie Zahl vergeben werden, ist garantiert, dass früher oder später eine durch 10 teilbare PID erzeugt wird. Das Programm terminiert daher immer. Wären PIDs zufällig, könnte es theoretisch unendlich laufen.\nStartende oder endende Prozesse können die PID-Vergabe beeinflussen, da sie die Reihenfolge freier PIDs verändern – dadurch variiert die genaue Prozessanzahl je nach Systemzustand.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Blatt 02</span>"
    ]
  },
  {
    "objectID": "sh02/02.html#aufgabe-3",
    "href": "sh02/02.html#aufgabe-3",
    "title": "2  Blatt 02",
    "section": "Aufgabe 3",
    "text": "Aufgabe 3\n\nErklärung zur Ausgabe von ps -T -H\nDas C-Programm:\n#include &lt;stdio.h&gt;\n#include &lt;stdlib.h&gt;\n#include &lt;unistd.h&gt;\n\nint main(int argc, char const *argv[])\n{\n    if (fork() &gt; 0) sleep(1000);\n    else exit(0);\n    return 0;\n}\nerzeugt einen Kindprozess. Das Kind beendet sich sofort (exit(0)), während der Elternprozess 1000 Sekunden schläft (sleep(1000)).\nAblauf der Kommandos:\n\nDas Ausführen von ./test &:\n\nDas Programm läuft im Hintergrund.\nDie Shell gibt [1] 136620 aus → Prozess-ID (PID) 136620.\nDer Kindprozess wird erzeugt und terminiert sofort.\nDer Elternprozess schläft weiter.\nDa wait() nicht aufgerufen wird, wird der Kindprozess zu einem Zombie-Prozess.\n\nDas Ausführen von ./test und das drücken von &lt;Strg&gt;+Z danach:\n\nDas Programm startet im Vordergrund.\nMit &lt;Strg&gt;+Z wird es gestoppt.\nDie Shell zeigt: [2]+  Stopped ./test.\nAuch hier terminiert der Kindprozess sofort → Zombie-Prozess entsteht erneut.\n\n\nAusgabe von ps -T -H:\n    PID TTY      STAT   TIME COMMAND\n   1025 pts/0    Ss     0:00 /bin/bash --posix\n 136620 pts/0    S      0:00   ./test\n 136621 pts/0    Z      0:00     [test] &lt;defunct&gt;\n 136879 pts/0    T      0:00   ./test\n 136880 pts/0    Z      0:00     [test] &lt;defunct&gt;\n 136989 pts/0    R+     0:00   ps T -H\nErklärung:\n\n1025: Die Shell (bash), läuft im Terminal pts/0.\n136620: Erstes ./test-Programm, läuft im Hintergrund, schläft (S).\n136621: Dessen Kindprozess (Zombie, Z), da exit() aufgerufen wurde, aber vom Elternprozess nicht abgeholt.\n136879: Zweites ./test-Programm, wurde mit &lt;Strg+Z&gt; gestoppt (T).\n136880: Auch hier: Kindprozess wurde beendet, aber nicht „abgeholt“ → Zombie.\n136989: Der ps-Prozess selbst, der gerade die Ausgabe erzeugt (R+ = laufend im Vordergrund).\n\nDie Spalten\n\nPID: Prozess-ID.\nTTY: Terminal, dem der Prozess zugeordnet ist.\nSTAT: Prozessstatus:\n\nS: sleeping – schläft.\nT: stopped – gestoppt (z. B. durch SIGSTOP).\nZ: zombie – beendet, aber noch nicht „aufgeräumt“.\nR: running – aktuell laufend auf der CPU.\n+: Teil der Vordergrund-Prozessgruppe im Terminal.\n\nTIME: CPU-Zeit, die der Prozess verbraucht hat.\nCOMMAND: Der auszuführende Befehl.\n\n[test] &lt;defunct&gt; heißt, es handelt sich um einen Zombie-Prozess, dessen Kommandozeile nicht mehr verfügbar ist.\n\n\n\n\nProcess state Codes\nProzesszustände (erste Buchstaben):\n\n\n\n\n\n\n\n\nCode\nMeaning\nDescription\n\n\n\n\nR\nRunning\nCurrently running or ready to run (on CPU)\n\n\nS\nSleeping\nWaiting for an event (e.g., input, timer)\n\n\nD\nUninterruptible sleep\nWaiting for I/O (e.g., disk), cannot be killed easily\n\n\nT\nStopped\nProcess has been stopped (e.g., SIGSTOP, Ctrl+Z)\n\n\nZ\nZombie\nTerminated, but not yet cleaned up by its parent\n\n\nX\nDead\nProcess is terminated and should be gone (rarely shown)\n\n\n\nZusätzliche flags:\n\n\n\nFlag\nMeaning\n\n\n\n\n&lt;\nHigh priority (not nice to others)\n\n\nN\nLow priority (nice value &gt; 0)\n\n\nL\nHas pages locked in memory\n\n\ns\nSession leader\n\n\n+\nIn the foreground process group\n\n\nl\nMulti-threaded (using CLONE_THREAD)\n\n\np\nIn a separate process group\n\n\n\nZ.B. Ss+ beduetet: Sleeping (S), Session leader (s) & Foreground process (+).\n\n\nTiefe der Aktuellen Sitzung\nZuerst finden wir die PID der Aktuellen sitzung mit\necho $$\nheraus. Output: 1025.\nDanch führen wir das Command ps -eH | less aus und suchen im pager nach “1025”. In unserer Sitzung befand sich “bash” unter der Hierarchie:\n1 systemd\n    718 ssdm\n        766 ssdm-helper\n            859 i3\n                884 kitty\n                    1025 bash\nDas entspricht der Tiefe 5 des Prozessbaums.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Blatt 02</span>"
    ]
  },
  {
    "objectID": "sh02/02.html#aufgabe-4",
    "href": "sh02/02.html#aufgabe-4",
    "title": "2  Blatt 02",
    "section": "Aufgabe 4",
    "text": "Aufgabe 4\nÜbersicht der Varianten mit Signaturen:\n\n\n\n\n\n\n\nFunktion\nSignatur\n\n\n\n\nexecl\nint execl(const char *path, const char *arg0, ..., NULL);\n\n\nexecle\nint execle(const char *path, const char *arg0, ..., NULL, char *const envp[]);\n\n\nexeclp\nint execlp(const char *file, const char *arg0, ..., NULL);\n\n\nexecv\nint execv(const char *path, char *const argv[]);\n\n\nexecvp\nint execvp(const char *file, char *const argv[]);\n\n\nexecvpe\nint execvpe(const char *file, char *const argv[], char *const envp[]);\n\n\nexecve\nint execve(const char *filename, char *const argv[], char *const envp[]);\n\n\n\nWichtige Unterschiede:\n\nl = Argumente als Liste (z. B. execl)\nv = Argumente als Array (vector) (z. B. execv)\np = PATH-Suche aktiv (z. B. execvp)\ne = eigene Umgebung (envp[]) möglich (z. B. execle, execvpe)\nKein p = voller Pfad zur Datei nötig\nKein e = aktuelle Umgebungsvariablen werden übernommen\n\nWann welche Variante?\n\n\n\n\n\n\n\nVariante\nTypischer Einsatzzweck\n\n\n\n\nexecl\nFester Pfad und Argumente direkt im Code als Liste\n\n\nexecle\nWie execl, aber mit eigener Umgebung\n\n\nexeclp\nWie execl, aber PATH-Suche aktiviert (z. B. ls statt /bin/ls)\n\n\nexecv\nPfad bekannt, Argumente liegen als Array vor (z. B. aus main)\n\n\nexecvp\nWie execv, aber mit PATH-Suche (typisch für Shells)\n\n\nexecvpe\nWie execvp, aber mit eigener Umgebung (GNU-spezifisch)\n\n\nexecve\nLow-Level, volle Kontrolle über Pfad, Argumente und Umgebung",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Blatt 02</span>"
    ]
  },
  {
    "objectID": "sh02/02.html#aufgabe-5",
    "href": "sh02/02.html#aufgabe-5",
    "title": "2  Blatt 02",
    "section": "Aufgabe 5",
    "text": "Aufgabe 5\nEin Prozesswechsel (Context Switch) tritt auf, wenn das Betriebssystem (OS) die Ausführung eines Prozesses stoppt und zu einem anderen wechselt. Dabei entsteht Overhead, weil:\n\nDer aktuelle CPU-Zustand (Register, Programmzähler etc.) gespeichert werden muss\nDieser Zustand im Prozesskontrollblock (PCB) abgelegt wird\nDer Zustand des neuen Prozesses aus seinem PCB geladen wird\nDie Speicherverwaltungsstrukturen (z. B. Seitentabellen der MMU) aktualisiert werden müssen\nDer TLB (Translation Lookaside Buffer) meist ungültig wird und geleert werden muss\nWeitere OS-Daten wie Datei-Deskriptoren oder Signale angepasst werden müssen\n\nDer PCB enthält:\n\nProzess-ID, Zustand\nRegister, Programmzähler\nSpeicherinfos, geöffnete Dateien\nScheduling-Infos\n\nBeim Prozesswechsel speichert das OS den PCB des alten Prozesses und lädt den neuen, um eine korrekte Fortsetzung zu ermöglichen. Da jeder Prozess einen eigenen Adressraum besitzt, ist der Aufwand für das Umschalten entsprechend hoch.\nThreads desselben Prozesses teilen sich hingegen denselben Adressraum (also denselben Code, Heap, offene Dateien etc.). Das bedeutet:\n\nEs ist kein Wechsel des Adressraums nötig\nDie MMU- und TLB-Einträge bleiben gültig\nNur der Thread-spezifische Kontext (Register, Stack-Pointer etc.) muss gespeichert werden\n\nFazit: Ein Threadwechsel ist viel leichter und schneller**, da kein teurer Speicherverwaltungswechsel nötig ist.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Blatt 02</span>"
    ]
  },
  {
    "objectID": "sh02/02.html#aufgabe-6",
    "href": "sh02/02.html#aufgabe-6",
    "title": "2  Blatt 02",
    "section": "Aufgabe 6",
    "text": "Aufgabe 6\n\nIn der ursprünglichen Version werden alle Threads schnell hintereinander gestartet, ohne aufeinander zu warten. Da die Ausführung der Threads vom Scheduler (Betriebssystem) abhängt und parallel erfolgt, kann die Ausgabe beliebig vermischt erscheinen – z. B. kann ein Thread seine Nachricht „number: i“ ausgeben, noch bevor die Hauptfunktion „creating thread i“ gedruckt hat.\nIn der überarbeiteten Version hingegen wird jeder Thread direkt nach dem Start mit pthread_join wieder eingesammelt. Dadurch läuft immer nur ein Thread zur Zeit, und seine Ausgabe erfolgt vollständig, bevor der nächste beginnt. So entsteht eine streng sequentielle Ausgabe:\n\n„creating thread i“\n„number: i“\n„ending thread i“\n\nDiese einfache Struktur vermeidet Race Conditions und benötigt keine zusätzlichen Synchronisationsmechanismen wie Semaphoren oder Locks.\nÜberarbeitete Version (auch im zip als threads_example.c enthalten):\n\n\nthreads_example.c\n\n#include &lt;pthread.h&gt;\n#include &lt;stdio.h&gt; \n#include &lt;stdlib.h&gt;\n#include &lt;assert.h&gt;\n\n#define NUM_THREADS 200000\n\nvoid* TaskCode (void* argument)\n{\n   int tid = *((int*) argument);\n   printf(\"number: %d\\n\", tid);\n   printf(\"ending thread %d\\n\", tid);\n   return NULL;\n}\n\nint main()\n{\n   pthread_t thread;\n   int thread_arg;\n\n   for (int i = 0; i &lt; NUM_THREADS; i++) {\n      thread_arg = i;\n      printf(\"creating thread %d\\n\", i);\n      int rc = pthread_create(&thread, NULL, TaskCode, &thread_arg);\n      assert(rc == 0);\n      rc = pthread_join(thread, NULL);\n      assert(rc == 0);\n   }\n\n   return 0;\n}\n\nIn unserem System \\(N_{\\text{max}} \\approx 200000\\).\nIm folgenden Program wird TaskCode() \\(N_\\text{max}\\) mal in einer einfachen Schleife aufgerufen:\n#include &lt;pthread.h&gt;\n#include &lt;stdio.h&gt; \n#include &lt;stdlib.h&gt;\n#include &lt;assert.h&gt;\n\n#define NUM_THREADS 200000\n\nvoid* TaskCode (void* argument)\n{\n   int tid = *((int*) argument);\n   printf(\"number: %d\\n\", tid);\n   printf(\"ending thread %d\\n\", tid);\n   return NULL;\n}\n\nint main()\n{\n   for (int i = 0; i &lt; NUM_THREADS; i++) {\n      TaskCode(&i);\n   }\n\n   return 0;\n}\nDie Ausführung dieses Programs dauerte c. 2 Sekunden auf unserem System. D.h. die fehlenden zwei pthread_* aufrufe kosten\n\n8 Sekunden für 200000 Schleifen. Das entspricht c. 20 millisekunden pro pthread_* Aufruf.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Blatt 02</span>"
    ]
  },
  {
    "objectID": "sh03/03.html",
    "href": "sh03/03.html",
    "title": "3  Blatt 03",
    "section": "",
    "text": "Aufgabe 1\nErklärung:\nDieses Programm vermeidet das Race Condition-Problem, indem beide Threads einen synchronized-Block verwenden, der auf Counter.class synchronisiert ist. Das bedeutet:",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Blatt 03</span>"
    ]
  },
  {
    "objectID": "sh03/03.html#aufgabe-1",
    "href": "sh03/03.html#aufgabe-1",
    "title": "3  Blatt 03",
    "section": "",
    "text": "Die Ausgabe ist inkonsistent – bei mehreren Programmausführungen erscheinen unterschiedliche Werte für counter. Dies liegt an einer Race Condition, da beide Threads gleichzeitig und ohne Synchronisation auf die gemeinsame Variable counter zugreifen. Dadurch können Zwischenergebnisse überschrieben oder verloren gehen, je nachdem, wie der Scheduler die Threads abwechselnd ausführt.\nSynchronisierte Lösung (Java-Code):\n\npublic class Counter {\n    static int counter = 0;\n\n    public static class Counter_Thread_A extends Thread {\n        public void run() {\n            synchronized (Counter.class) {\n                counter = 5;\n                counter++;\n                counter++;\n                System.out.println(\"A-Counter: \" + counter);\n            }\n        }\n    }\n\n    public static class Counter_Thread_B extends Thread {\n        public void run() {\n            synchronized (Counter.class) {\n                counter = 6;\n                counter++;\n                counter++;\n                counter++;\n                counter++;\n                System.out.println(\"B-Counter: \" + counter);\n            }\n        }\n    }\n\n    public static void main(String[] args) {\n        Thread a = new Counter_Thread_A();\n        Thread b = new Counter_Thread_B();\n        a.start();\n        b.start();\n    }\n}\n\n\n\nNur ein Thread darf gleichzeitig den Block betreten.\nDer andere Thread muss warten, bis der erste fertig ist und den Lock freigibt.\nDadurch wird sichergestellt, dass keine gleichzeitigen Zugriffe auf die gemeinsame Variable counter stattfinden.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Blatt 03</span>"
    ]
  },
  {
    "objectID": "sh03/03.html#aufgabe-3",
    "href": "sh03/03.html#aufgabe-3",
    "title": "3  Blatt 03",
    "section": "Aufgabe 3",
    "text": "Aufgabe 3\n\nUnten folgt der Quellcode zur verbesserten Lösung des Producer-Consumer-Problems (pc2.c am Ende des Dokuments). In dieser Version wird Busy Waiting durch eine effiziente Synchronisation mithilfe eines Mutexes und einer Condition Variable ersetzt.\nDer Code befindet sich auch im beigefügten Zip-Archiv im Ordner A3. Dort kann das Programm wie folgt kompiliert und ausgeführt werden:\n\nmake  \n./pc2\nDiese Implementierung gewährleistet eine korrekte und effiziente Koordination zwischen Producer- und Consumer-Threads:\n\nDie gemeinsame Warteschlange wird durch einen Mutex geschützt.\nThreads, die auf eine Bedingung warten, verwenden pthread_cond_wait() innerhalb einer while-Schleife, um Spurious Wakeups korrekt zu behandeln.\nIst die Warteschlange leer, schlafen die Consumer, bis sie ein Signal erhalten; ist sie voll, wartet der Producer entsprechend.\nDurch das gezielte Aufwecken via pthread_cond_signal() oder pthread_cond_broadcast() wird unnötiger CPU-Verbrauch durch aktives Warten vermieden.\n\nInsgesamt ist diese Lösung robuster und skalierbarer als die ursprüngliche Variante mit Busy Waiting – insbesondere bei mehreren Consumer-Threads und höherer Auslastung.\n\n\n\npc2.c\n\n#include &lt;stdio.h&gt;\n#include &lt;stdlib.h&gt;\n#include &lt;pthread.h&gt;\n#include \"mylist.h\"\n// Mutex to protect access to the shared queue\npthread_mutex_t queue_lock;\n// Single condition variable used for both producers and consumers\npthread_cond_t cond_var;\n// Shared buffer (a custom linked list acting as a queue)\nlist_t buffer;\n// Counters for task management\nint count_proc = 0;\nint production_done = 0;\n/********************************************************/\n/* Function Declarations */\nstatic unsigned long fib(unsigned int n);\nstatic void create_data(elem_t **elem);\nstatic void *consumer_func(void *);\nstatic void *producer_func(void *);\n/********************************************************/\n/* Compute the nth Fibonacci number (CPU-intensive task) */\nstatic unsigned long fib(unsigned int n)\n{\n    if (n == 0 || n == 1) {\n        return n;\n    } else {\n        return fib(n - 1) + fib(n - 2);\n    }\n}\n/* Allocate and initialize a new task node */\nstatic void create_data(elem_t **elem)\n{\n    *elem = (elem_t*) malloc(sizeof(elem_t));\n    (*elem)-&gt;data = FIBONACCI_MAX;\n}\n/* Consumer thread function */\nstatic void *consumer_func(void *args) \n{\n    elem_t *elem;\n    while (1) {\n        pthread_mutex_lock(&queue_lock);\n        // Wait if the queue is empty and production is not yet complete\n        while (get_size(&buffer) == 0 && !production_done) {\n            pthread_cond_wait(&cond_var, &queue_lock);\n        }\n        // Exit condition: queue is empty and production has finished\n        if (get_size(&buffer) == 0 && production_done) {\n            pthread_mutex_unlock(&queue_lock);\n            break;\n        }\n        // Remove an item from the queue\n        remove_elem(&buffer, &elem);\n        // Wake up a potentially waiting producer\n        pthread_cond_signal(&cond_var);\n        pthread_mutex_unlock(&queue_lock);\n        // Process the task\n        fib(elem-&gt;data);\n        free(elem);\n        printf(\"item consumed\\n\");\n    }\n    return NULL;\n}\n/* Producer thread function */\nstatic void *producer_func(void *args) \n{\n    while (1) {\n        pthread_mutex_lock(&queue_lock);\n        // Wait if the buffer is full\n        while (get_size(&buffer) &gt;= MAX_QUEUE_LENGTH) {\n            pthread_cond_wait(&cond_var, &queue_lock);\n        }\n        if (count_proc &lt; MAX_COUNT) {\n            // Create and append a new task to the queue\n            elem_t *elem;\n            create_data(&elem);\n            append_elem(&buffer, elem);\n            count_proc++;\n            printf(\"item produced\\n\");\n            // Wake up one waiting consumer\n            pthread_cond_signal(&cond_var);\n        }\n        // If production is done, notify all consumers and exit\n        if (count_proc &gt;= MAX_COUNT) {\n            production_done = 1;\n            // Wake up all consumers waiting on cond_var so they can check the exit condition\n            pthread_cond_broadcast(&cond_var);\n            pthread_mutex_unlock(&queue_lock);\n            break;\n        }\n        pthread_mutex_unlock(&queue_lock);\n    }\n    return NULL;\n}\n/* Main function */\nint main (int argc, char *argv[])\n{\n    pthread_t cons_thread[NUM_CONSUMER];\n    pthread_t prod_thread;\n    int i;\n    // Initialize mutex and condition variable\n    pthread_mutex_init(&queue_lock, NULL);\n    pthread_cond_init(&cond_var, NULL);\n    init_list(&buffer);\n    // Start consumer threads\n    for (i = 0; i &lt; NUM_CONSUMER; i++) {\n        pthread_create(&cons_thread[i], NULL, &consumer_func, NULL);\n    }\n    // Start producer thread\n    pthread_create(&prod_thread, NULL, &producer_func, NULL);\n\n    // Wait for all consumer threads to finish\n    for (i = 0; i &lt; NUM_CONSUMER; i++) {\n        pthread_join(cons_thread[i], NULL);\n    }\n    // Wait for producer thread to finish\n    pthread_join(prod_thread, NULL);\n    // Cleanup\n    pthread_mutex_destroy(&queue_lock);\n    pthread_cond_destroy(&cond_var);\n    return 0;\n}\n\n\n\nLaufzeitvergleich von pc und pc2\nZur Überprüfung der Effizienzverbesserung durch den Einsatz von Condition Variables wurde folgendes Bash-Skript verwendet, das beide Programme je 10-mal ausführt und die durchschnittliche Laufzeit berechnet:\n#!/bin/bash\n\nRUNS=10\nPC=\"./pc\"\nPC2=\"./pc2\"\n\nmeasure_average_runtime() {\n    PROGRAM=$1\n    TOTAL=0\n    echo \"Running $PROGRAM...\"\n    for i in $(seq 1 $RUNS); do\n        START=$(date +%s.%N)\n        $PROGRAM &gt; /dev/null\n        END=$(date +%s.%N)\n        RUNTIME=$(echo \"$END - $START\" | bc)\n        echo \"  Run $i: $RUNTIME seconds\"\n        TOTAL=$(echo \"$TOTAL + $RUNTIME\" | bc)\n    done\n    AVG=$(echo \"scale=4; $TOTAL / $RUNS\" | bc)\n    echo \"Average runtime of $PROGRAM: $AVG seconds\"\n    echo\n}\n\necho \"Measuring $RUNS runs of $PC and $PC2...\"\necho\nmeasure_average_runtime $PC\nmeasure_average_runtime $PC2\nAusgeführt wurde das Skript mit:\n./benchmark_pc.sh\nDabei ergaben sich folgende Laufzeiten:\nMeasuring 10 runs of ./pc and ./pc2...\n\nRunning ./pc...\n  Run 1: 5.471139729 seconds\n  Run 2: 5.545249360 seconds\n  Run 3: 5.359090183 seconds\n  Run 4: 5.366634866 seconds\n  Run 5: 5.459910579 seconds\n  Run 6: 5.531161091 seconds\n  Run 7: 5.738575161 seconds\n  Run 8: 5.835055657 seconds\n  Run 9: 5.496744966 seconds\n  Run 10: 5.641529848 seconds\nAverage runtime of ./pc: 5.5445 seconds\n\nRunning ./pc2...\n  Run 1: 5.244080521 seconds\n  Run 2: 5.237442233 seconds\n  Run 3: 5.220517776 seconds\n  Run 4: 5.281094089 seconds\n  Run 5: 5.261722379 seconds\n  Run 6: 5.363685993 seconds\n  Run 7: 5.276107150 seconds\n  Run 8: 5.091557858 seconds\n  Run 9: 5.073267276 seconds\n  Run 10: 5.164472482 seconds\nAverage runtime of ./pc2: 5.2213 seconds\nDie Ergebnisse zeigen, dass pc2 im Schnitt etwas schneller ist als pc (5.22 s gegenüber 5.54 s), was den Effizienzgewinn durch den Verzicht auf aktives Warten bestätigt.\nDie Dateien benchmark_pc.sh und benchmark_results.txt befinden sich im Ordner A3 des ZIP-Archivs.\nZur Veranschaulichung wurde mit dem folgendnen Python script zusätzlich ein Diagramm erstellt, das die Laufzeiten von pc und pc2 über zehn Durchläufe hinweg zeigt. Die Durchschnittslinien verdeutlichen, dass pc2 im Mittel schneller und konsistenter ist als pc.\n\nimport matplotlib.pyplot as plt\n\n# Runtime data for each run (in seconds)\npc = [5.471139729, 5.545249360, 5.359090183, 5.366634866, 5.459910579,\n      5.531161091, 5.738575161, 5.835055657, 5.496744966, 5.641529848]\n\npc2 = [5.244080521, 5.237442233, 5.220517776, 5.281094089, 5.261722379,\n       5.363685993, 5.276107150, 5.091557858, 5.073267276, 5.164472482]\n\n# X-axis: run numbers\nruns = list(range(1, 11))\n\n# Calculate averages\navg_pc = sum(pc) / len(pc)\navg_pc2 = sum(pc2) / len(pc2)\n\n# Plot configuration\nplt.figure(figsize=(10, 6))\nplt.plot(runs, pc, marker='o', label='pc')\nplt.plot(runs, pc2, marker='o', label='pc2')\n\n# Average lines\nplt.axhline(avg_pc, color='red', linestyle='--', label=f'avg pc ({avg_pc:.3f}s)')\nplt.axhline(avg_pc2, color='green', linestyle='--', label=f'avg pc2 ({avg_pc2:.3f}s)')\n\n# Labels and title\nplt.xlabel('Run')\nplt.ylabel('Time (s)')\nplt.title('Runtime Comparison of pc vs. pc2')\nplt.xticks(runs)\nplt.grid(True)\nplt.legend()\nplt.tight_layout()\n\n# Display the plot\nplt.show()",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Blatt 03</span>"
    ]
  },
  {
    "objectID": "sh03/03.html#aufgabe-4",
    "href": "sh03/03.html#aufgabe-4",
    "title": "3  Blatt 03",
    "section": "Aufgabe 4",
    "text": "Aufgabe 4\n\nDie gegebene Implementierung kann zu einer Verletzung des gegenseitigen Ausschlusses führen, wenn zwei schreibende Threads gleichzeitig in die kritische Sektion gelangen.\nBeispiel: Angenommen N = 5. Thread A und Thread B rufen gleichzeitig lock_write() auf. Da der for-Loop nicht durch einen Mutex geschützt ist, können sich ihre wait(S)-Aufrufe gegenseitig durchmischen: A nimmt 1 Token → S = 4 B nimmt 1 Token → S = 3 A nimmt 1 → S = 2 B nimmt 1 → S = 1 … und so weiter. Wenn nun zufällig genug Tokens freigegeben werden (z. B. durch unlock_read()-Aufrufe), können beide Threads nacheinander die restlichen Semaphore erwerben und ihren Loop abschließen, ohne dass einer von ihnen jemals alle N Tokens exklusiv gehalten hat. Beide betreten anschließend die kritische Sektion, obwohl gegenseitiger Ausschluss nicht mehr gewährleistet ist.\nDas Problem wird behoben, indem ein zusätzlicher Mutex eingeführt wird, der verhindert, dass mehrere schreibende Threads gleichzeitig versuchen, die Semaphore S zu erwerben:\nS = Semaphore(N)\nM = Semaphore(1)  // neuer Mutex\n\ndef lock_read():\n    wait(S)\n\ndef unlock_read():\n    signal(S)\n\ndef lock_write():\n    wait(M)\n    for i in range(N): wait(S)\n    signal(M)\n\ndef unlock_write():\n    for i in range(N): signal(S)\nDurch den Mutex M ist sichergestellt, dass der Erwerb der Semaphore in lock_write() ausschließlich von einem Thread durchgeführt wird. So wird verhindert, dass mehrere schreibende Threads gleichzeitig in die kritische Sektion gelangen.\nHinweis: Diese Lösung stellt den gegenseitigen Ausschluss sicher, erlaubt jedoch theoretisch, dass ein schreibender Thread dauerhaft blockiert bleibt, wenn ständig neue Leser auftreten (Starvation). Für diese Aufgabe ist jedoch nur die Korrektur der Ausschlussverletzung relevant.\nDie Befehle upgrade_to_write() und downgrade_to_read() ermöglichen es einem Thread, während des laufenden Zugriffs die Art des Read-Write-Locks dynamisch zu wechseln – ohne dabei den kritischen Abschnitt vollständig zu verlassen. Dies verhindert Race Conditions und potenzielle Starvation.\nEin Thread, der upgrade_to_write() aufruft, hält bereits einen Lesezugriff (also eine Einheit der Semaphore S) und möchte exklusiven Schreibzugriff erhalten. Dafür müssen die verbleibenden N - 1 Einheiten erworben werden. Ein zusätzlicher Mutex M sorgt dafür, dass nicht mehrere Threads gleichzeitig versuchen, sich hochzustufen, was zu Deadlocks führen könnte.\nEin Thread, der downgrade_to_read() aufruft, hält alle N Einheiten (Schreibzugriff) und möchte auf geteilten Lesezugriff wechseln. Dazu werden N - 1 Einheiten freigegeben – eine Einheit bleibt erhalten.\nHinweis: Das hier verwendete Mutex M ist dasselbe wie in Teil b) und stellt sicher, dass nur ein Thread gleichzeitig exklusiven Zugriff auf die Semaphore S erwerben kann – sei es über lock_write() oder über upgrade_to_write().\nPseudocode:\nS = Semaphore(N)     // erlaubt bis zu N gleichzeitige Leser oder 1 Schreiber\nM = Semaphore(1)     // schützt exklusive Zugriffsversuche\n\ndef upgrade_to_write():\n    wait(M)\n    for i in range(N - 1):     // hält bereits 1 Einheit als Leser\n        wait(S)\n    signal(M)\n\ndef downgrade_to_read():\n    for i in range(N - 1):     // gibt N - 1 Einheiten frei, behält 1\n        signal(S)\nFazit: Diese Operationen garantieren einen sicheren Übergang zwischen Lese- und Schreibmodus, ohne Race Conditions oder Deadlocks, und basieren auf derselben Semaphor-Struktur wie in Teil b).",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Blatt 03</span>"
    ]
  },
  {
    "objectID": "sh04/04.html",
    "href": "sh04/04.html",
    "title": "4  Blatt 04",
    "section": "",
    "text": "Aufgabe 1",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Blatt 04</span>"
    ]
  },
  {
    "objectID": "sh04/04.html#aufgabe-1",
    "href": "sh04/04.html#aufgabe-1",
    "title": "4  Blatt 04",
    "section": "",
    "text": "Ein Nachteil benannter Pipes ist, dass sie manuell im Dateisystem erstellt und verwaltet werden müssen (z. B. mit mkfifo). Das macht die Handhabung aufwändiger und erfordert gegebenenfalls zusätzliche Aufräummaßnahmen.\nWenn zwei voneinander unabhängige Prozesse (z. B. zwei Terminals) Daten austauschen sollen, ist eine benannte Pipe erforderlich. Anonyme Pipes funktionieren nur zwischen verwandten Prozessen (z. B. Eltern-Kind).",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Blatt 04</span>"
    ]
  },
  {
    "objectID": "sh04/04.html#aufgabe-2",
    "href": "sh04/04.html#aufgabe-2",
    "title": "4  Blatt 04",
    "section": "Aufgabe 2",
    "text": "Aufgabe 2\n\nIm Win32-API ist ein Handle vom Typ:\ntypedef void* HANDLE;\nEs handelt sich also um einen Zeiger (bzw. zeigerbreiten Wert), der jedoch nicht dereferenziert werden soll. Ein Handle ist ein undurchsichtiger Verweis auf eine Ressource, die vom Windows-Kernel verwaltet wird – etwa eine Datei, ein Prozess, ein Event oder ein Fensterobjekt.\nWenn ein Programm zum Beispiel CreateFile() aufruft, gibt der Kernel einen solchen Handle zurück. Dieser verweist intern auf ein Objekt in der Handle-Tabelle des Prozesses. Diese Tabelle enthält Informationen wie Zugriffsrechte, aktuelle Dateiposition, Typ des Objekts usw.\nIm Unterschied zu Dateideskriptoren unter Unix/Linux (einfache Ganzzahlen) sind Win32-Handles allgemeiner gehalten und dienen zum Zugriff auf viele verschiedene Ressourcentypen – nicht nur auf Dateien.\nDie Umleitung der Standardausgabe erfolgt im Win32-API in zwei Schritten:\n\nEine Datei wird mit CreateFile() geöffnet oder erzeugt.\nDer Handle für STD_OUTPUT_HANDLE wird mit SetStdHandle() auf diesen Datei-Handle gesetzt.\n\nBeispiel:\n#include &lt;windows.h&gt;\n#include &lt;stdio.h&gt;\n\nint main() {\n    HANDLE hFile = CreateFile(\"output.txt\", GENERIC_WRITE, 0, NULL,\n                              CREATE_ALWAYS, FILE_ATTRIBUTE_NORMAL, NULL);\n\n    if (hFile == INVALID_HANDLE_VALUE) {\n        printf(\"Fehler beim Öffnen der Datei.\\n\");\n        return 1;\n    }\n\n    // Standardausgabe umleiten\n    SetStdHandle(STD_OUTPUT_HANDLE, hFile);\n\n    // Alles, was an STD_OUTPUT_HANDLE geschrieben wird, geht nun in die Datei\n    DWORD written;\n    WriteFile(GetStdHandle(STD_OUTPUT_HANDLE),\n              \"Hello redirected world!\\n\", 24, &written, NULL);\n\n    CloseHandle(hFile);\n    return 0;\n}\nDiese Umleitung wirkt sich auf Low-Level-Funktionen wie WriteFile() aus. Wenn man dagegen höhere Funktionen wie printf() oder std::cout umleiten will, muss zusätzlich die Laufzeitumgebung angepasst werden – etwa mit freopen() oder std::ios-Umleitungen.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Blatt 04</span>"
    ]
  },
  {
    "objectID": "sh04/04.html#aufgabe-3",
    "href": "sh04/04.html#aufgabe-3",
    "title": "4  Blatt 04",
    "section": "Aufgabe 3",
    "text": "Aufgabe 3\nDas Program: (Auch im Zip unter dem Verzeichniss A3 als reverse_pipechat.c enthalten)\n\n\nreverse_pipechat.c\n\n#include &lt;stdio.h&gt;\n#include &lt;stdlib.h&gt;\n#include &lt;unistd.h&gt;\n#include &lt;string.h&gt;\n#include &lt;sys/wait.h&gt;\n\n#define BUFFER_SIZE 1024\n\n// Utility: reverse a string in place\nvoid reverse_string(char *str) {\n    int len = strlen(str);\n    for (int i = 0; i &lt; len / 2; ++i) {\n        char tmp = str[i];\n        str[i] = str[len - 1 - i];\n        str[len - 1 - i] = tmp;\n    }\n}\n\nint main() {\n    int pipe_a_to_b[2]; // parent writes to child\n    int pipe_b_to_a[2]; // child writes to parent\n\n    if (pipe(pipe_a_to_b) == -1 || pipe(pipe_b_to_a) == -1) {\n        perror(\"pipe\");\n        exit(EXIT_FAILURE);\n    }\n\n    pid_t pid = fork();\n\n    if (pid &lt; 0) {\n        perror(\"fork\");\n        exit(EXIT_FAILURE);\n    }\n    else if (pid == 0) {\n        // Child process: Process B\n        close(pipe_a_to_b[1]); // Close write end of A→B\n        close(pipe_b_to_a[0]); // Close read end of B→A\n\n        char buffer[BUFFER_SIZE];\n\n        // Read message from parent\n        ssize_t bytes_read = read(pipe_a_to_b[0], buffer, BUFFER_SIZE - 1);\n        if (bytes_read &lt;= 0) {\n            perror(\"child read\");\n            exit(EXIT_FAILURE);\n        }\n\n        buffer[bytes_read] = '\\0'; // Null-terminate the string\n\n        reverse_string(buffer); // Reverse the string\n\n        // Send it back to parent\n        write(pipe_b_to_a[1], buffer, strlen(buffer));\n\n        // Close used pipe ends\n        close(pipe_a_to_b[0]);\n        close(pipe_b_to_a[1]);\n\n        exit(EXIT_SUCCESS);\n    } else {\n        // Parent process: Process A\n        close(pipe_a_to_b[0]); // Close read end of A→B\n        close(pipe_b_to_a[1]); // Close write end of B→A\n\n        char input[BUFFER_SIZE];\n        printf(\"Enter a string: \");\n        if (!fgets(input, BUFFER_SIZE, stdin)) {\n            perror(\"fgets\");\n            exit(EXIT_FAILURE);\n        }\n\n        // Remove newline if present\n        input[strcspn(input, \"\\n\")] = '\\0';\n\n        // Send input to child\n        write(pipe_a_to_b[1], input, strlen(input));\n\n        char reversed[BUFFER_SIZE];\n        ssize_t bytes_received = read(pipe_b_to_a[0], reversed, BUFFER_SIZE - 1);\n        if (bytes_received &lt;= 0) {\n            perror(\"parent read\");\n            exit(EXIT_FAILURE);\n        }\n\n        reversed[bytes_received] = '\\0'; // Null-terminate\n\n        printf(\"Reversed string: %s\\n\", reversed);\n\n        // Close used pipe ends\n        close(pipe_a_to_b[1]);\n        close(pipe_b_to_a[0]);\n\n        wait(NULL); // Wait for child to finish\n    }\n\n    return 0;\n}\n\nDas C-Programm demonstriert die Kommunikation zwischen zwei Prozessen über anonyme Pipes. Der Elternprozess (A) liest eine Zeichenkette von der Standardeingabe und sendet sie an den Kindprozess (B). Dieser kehrt die Zeichenkette um und schickt sie zurück. Der Elternprozess gibt das Ergebnis anschließend auf der Standardausgabe aus.\nTechnisch funktioniert das Programm so: Es erstellt zwei Pipes – eine für die Kommunikation von A nach B, die andere für die Rückrichtung. Nach dem Aufruf von fork() schließt jeder Prozess die jeweils nicht benötigten Enden der Pipes. Der Elternprozess sendet die Benutzereingabe an das Kind, das die Zeichenkette verarbeitet und die Antwort zurückschickt. Beide Prozesse verwenden read() und write() zur Datenübertragung und beenden sich danach.\nKompilieren und ausführen kann man das Programm unter Verzeichniss A3 mit:\nmake\n./pipe_example\nBeispielausgabe:\nEnter a string: hallo welt\nReversed string: tlew ollah",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Blatt 04</span>"
    ]
  },
  {
    "objectID": "sh04/04.html#aufgabe-4",
    "href": "sh04/04.html#aufgabe-4",
    "title": "4  Blatt 04",
    "section": "Aufgabe 4",
    "text": "Aufgabe 4\n\n:\n\nFragmentiuerung: Intern. (Eine geringe Anzahl von langlebigen Objekten existieren in einem Page, was zur internen Speicherverschwendung führt)\nDefinition der Internen Fragmentierung (in diesem Kontext): Speicherverschwendung innerhalb der Seite\n\n\n\n\ntiming diagram\n\n\n:\n\nsehr häufig: fast immer handelt es sich um einen Tradeoff, z.B. beim best fit vs first fit handelt es sich um das Tradeoff Speichereffizienz vs Zeiteffizienz\nTradeoff: Cache misses vs Interne Fragmentierung (Zeit vs Speicherplatz)\n\nKleine Seiten: Wenig interne Fragmentierung aber häufige Cache misses \\(\\Rightarrow\\) Zeitverschwendung\nGrosse Seiten: Seltene Cach misses aber sehr große interne Fragmentierung (da es häufig langlebige Objekte existieren) \\(\\Rightarrow\\) Speicherverscwendung",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Blatt 04</span>"
    ]
  },
  {
    "objectID": "sh04/04.html#aufgabe-5",
    "href": "sh04/04.html#aufgabe-5",
    "title": "4  Blatt 04",
    "section": "Aufgabe 5",
    "text": "Aufgabe 5\n\nInterne vs. externe Fragmentierung:\n\nInterne Fragmentierung entsteht, wenn ein Prozess mehr Speicher zugewiesen bekommt, als er tatsächlich benötigt – z. B. bei festen Block- oder Seitengrößen bleibt ungenutzter Speicher innerhalb des Blocks.\nExterne Fragmentierung tritt auf, wenn der freie Speicher zwar insgesamt groß genug ist, aber in viele kleine, nicht zusammenhängende Stücke aufgeteilt ist, sodass größere Prozesse keinen passenden Platz finden.\n\nLogische vs. physische Adressen:\n\nLogische Adressen (auch virtuelle Adressen) werden vom Prozess verwendet und beginnen meist bei 0 – sie sind unabhängig vom realen Speicherlayout.\nPhysische Adressen geben die tatsächliche Position im Hauptspeicher (RAM) an. Das Betriebssystem bzw. die Hardware (MMU) wandelt logische Adressen zur Laufzeit in physische Adressen um.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Blatt 04</span>"
    ]
  },
  {
    "objectID": "sh04/04.html#aufgabe-6",
    "href": "sh04/04.html#aufgabe-6",
    "title": "4  Blatt 04",
    "section": "Aufgabe 6",
    "text": "Aufgabe 6\nKurze erklärung zur Notation A:B: Der Segment der Größe A wurde der Speicherlücke der Größe B zugewiesen. (Das ist eindeutig, da die Größen der Segmente und der Lücken jeweils eindeutig sind.)\nDann:\n\nFirst fit:\n12:20\n11:18\n3:10\n5:7\nBest fit:\n12:12\n11:15\n3:4\n5:7\nWorst fit\n12:20\n11:18\n3:15\n5:12",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Blatt 04</span>"
    ]
  },
  {
    "objectID": "sh04/04.html#aufgabe-7",
    "href": "sh04/04.html#aufgabe-7",
    "title": "4  Blatt 04",
    "section": "Aufgabe 7",
    "text": "Aufgabe 7\nDa die Seitengröße 1 KB = 1024 Bytes = \\(2^{10}\\) beträgt, entsprechen die unteren 10 Bit des virtuellen Adresse die Offset, die restlichen höheren Bits geben die Seitennummer an.\n\nBerechnung der Seitennummern und Offsets:\n\n\n\nAdresse\nSeitennummer\nOffset\n\n\n\n\n2456\n2\n408\n\n\n16382\n15\n1022\n\n\n30000\n29\n304\n\n\n4385\n4\n289\n\n\n\n\n\nC-Code:\n// V - virtuelle Addresse, gegeben\nint p = V &gt;&gt; 10;        // Seitennummer\nint offset = V & 0x3FF; // Offset (2^10 - 1 = 1023)\nDurch die Verwendung von Bitoperationen ist die Berechnung effizient, da die Seitengröße eine Zweierpotenz ist.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Blatt 04</span>"
    ]
  },
  {
    "objectID": "sh05/05.html",
    "href": "sh05/05.html",
    "title": "5  Blatt 05",
    "section": "",
    "text": "Aufgabe 1",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Blatt 05</span>"
    ]
  },
  {
    "objectID": "sh05/05.html#aufgabe-1",
    "href": "sh05/05.html#aufgabe-1",
    "title": "5  Blatt 05",
    "section": "",
    "text": "Matrikelnummer: Seitennummer (Virtuelle Addresse)\nWohnaddresse: Rahmennummer (Physische Addresse)\nVerzeichniss: Seitentabelle (Index)\n\nkeine Entsprechung zum Offset\nMatrikelnummer hat 7 stellen: \\(10^7\\), d.h. 10 Mil Einträge.\nEs gibt 4.800 Wohnheimzimmer: relevanter Anteil = \\(\\frac{4.8 \\cdot 10^3}{10 \\cdot 10^6} \\approx 0.5 \\cdot 10^{-4} = 0.05 \\%\\)",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Blatt 05</span>"
    ]
  },
  {
    "objectID": "sh05/05.html#aufgabe-2",
    "href": "sh05/05.html#aufgabe-2",
    "title": "5  Blatt 05",
    "section": "Aufgabe 2",
    "text": "Aufgabe 2\n\nAdressübersetzung bei Paging mit Seitengröße \\(2^k\\)\nBei einem Paging-System mit fester Seitengröße von \\(2^k\\) Byte wird die virtuelle Adresse \\(V\\) wie folgt in eine physische Adresse übersetzt:\n\\[\n\\text{Physische Adresse} = \\left(F(V \\gg k) \\ll k\\right) + \\left(V \\& (2^k - 1)\\right)\n\\]\n\n\\(V \\gg k\\): virtuelle Seitennummer (Integer-Division durch \\(2^k\\))\n\\(V \\& (2^k - 1)\\): Offset innerhalb der Seite\n\\(F(n)\\): Seitentabelle, die virtuelle Seitennummer \\(n\\) auf Rahmennummer abbildet\n\n\n\nUmkehrung der Formel\nWenn die virtuelle Adresse \\(V\\) und die zugehörige physische Adresse \\(P\\) gegeben sind, lässt sich der Seitentabelleneintrag rekonstruieren:\n\\[\nF(V \\gg k) = \\frac{P - (V \\& (2^k - 1))}{2^k}\n\\]\nBei Seitengröße von 4 KB (\\(2^{12} = 4096\\)) gilt:\n\\[\nF(V \\gg 12) = \\frac{P - (V \\& 0xFFF)}{4096}\n\\]\n\n\nGegebene Zuordnungen\n\n\\(V = 8203 \\rightarrow P = 12229\\)\n\\(V = 4600 \\rightarrow P = 25080\\)\n\\(V = 16510 \\rightarrow P = 41086\\)\n\n\n\\(V = 8203 \\rightarrow P = 12229\\)\n\nVirtuelle Seite: \\(8203 \\gg 12 = \\lfloor \\frac{8203}{4096} \\rfloor = 2\\)\nOffset: \\(8203 \\& 0xFFF = 8203 \\mod 4096 = 8203 - 8192 = 11\\)\nFrame-Berechnung:\n\\[\nF(2) = \\frac{12229 - 11}{4096} = \\frac{12218}{4096} = 2\n\\]\n\n\\(V = 4600 \\rightarrow P = 25080\\)\n\nVirtuelle Seite: \\(4600 \\gg 12 = 1\\)\nOffset: \\(4600 \\& 0xFFF = 504\\)\nFrame-Berechnung:\n\\[\nF(1) = \\frac{25080 - 504}{4096} = \\frac{24576}{4096} = 6\n\\]\n\n\\(V = 16510 \\rightarrow P = 41086\\)\n\nVirtuelle Seite: \\(16510 \\gg 12 = \\lfloor \\frac{16510}{4096} \\rfloor = 4\\)\nOffset: \\(16510 \\& 0xFFF = 16510 - (4 \\cdot 4096) = 126\\)\nFrame-Berechnung:\n\\[\nF(4) = \\frac{41086 - 126}{4096} = \\frac{40960}{4096} = 10\n\\]\n\n\n\n\nEndgültige rekonstruierte Seitentabelle\n\n\n\nVirtuelle Seitennummer\nPhysischer Rahmen\n\n\n\n\n2\n2\n\n\n1\n6\n\n\n4\n10\n\n\n\n\n\nPython Implementierung\nDie Rekonstruktion der Tabelle kann mit python wie folgt implementiert werden:\n\ndef reconstruct_page_table(k, mappings):\n    page_size = 1 &lt;&lt; k  # 2^k\n    page_mask = page_size - 1\n\n    page_table = []\n    for virtual_address, physical_address in mappings:\n        virtual_page_number = virtual_address &gt;&gt; k\n        offset = virtual_address & page_mask\n        frame_number = (physical_address - offset) &gt;&gt; k\n        page_table.append((virtual_page_number, frame_number))\n\n    return page_table\n\nk = 12  # page size = 2^12 = 4096\nmappings = [\n    (8203, 12229),\n    (4600, 25080),\n    (16510, 41086)\n]\n\npage_table = reconstruct_page_table(k, mappings)\nfor vpn, frame in page_table:\n    print(f\"Virtuelle Seite {vpn} → Rahmen {frame}\")\n\nVirtuelle Seite 2 → Rahmen 2\nVirtuelle Seite 1 → Rahmen 6\nVirtuelle Seite 4 → Rahmen 10",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Blatt 05</span>"
    ]
  },
  {
    "objectID": "sh05/05.html#aufgabe-3",
    "href": "sh05/05.html#aufgabe-3",
    "title": "5  Blatt 05",
    "section": "Aufgabe 3",
    "text": "Aufgabe 3\n\nAusgangssituation:\nBetrachten wir die folgende C-Schleife auf einem System, bei dem gilt:\n\nint ist 4 Byte groß\nSeitengröße = 4 KB = 4096 Byte\nDer TLB (Translation Lookaside Buffer) hat 64 Einträge (d.h. er kann 64 Seiten zwischenspeichern)\n\nJede Seite enthält 1024 Integer, da 4 Byte pro Element. (4096 / 4 = 1024) Jeder Zugriff auf X[i] greift auf eine Speicherseite zu:\n\\[\n\\text{Seitenummer}(i) = \\left\\lfloor \\frac{i}{1024} \\right\\rfloor\n\\]\nMit den indizes \\(i\\):\n\\[\ni = 0, M, 2M, 3M, \\dots, \\text{solange } i &lt; N\n\\]\nSetzen wir \\(T = \\left\\lfloor \\frac{N - 1}{M} \\right\\rfloor\\), so gibt es \\(T + 1\\) Iterationen.\nJede Iteration greift also auf die Seite zu:\n\\[\n\\text{Seite}(j) = \\left\\lfloor \\frac{j \\cdot M}{1024} \\right\\rfloor \\quad \\text{für } j = 0, 1, ..., T\n\\]\nDie Anzahl der verschiedenen Seiten, die beim Schleifendurchlauf berührt werden, ist:\n\\[\n\\text{TLB\\_pages}(M, N) = \\left| \\left\\{ \\left\\lfloor \\frac{j \\cdot M}{1024} \\right\\rfloor \\;\\middle|\\; 0 \\le j \\le \\left\\lfloor \\frac{N - 1}{M} \\right\\rfloor \\right\\} \\right|\n\\]\nDies ist die Anzahl der eindeutig unterschiedlichen Seiten, auf die zugegriffen wird. TLB-Misses treten auf, wenn:\n\\[\n\\text{TLB\\_pages}(M, N) &gt; 64\n\\]\nDas heißt: Es werden mehr als 64 unterschiedliche virtuelle Seiten benötigt – mehr als der TLB speichern kann.\nBeispiele und wichtige Beobachtungen\n\nKleines M (z. B. M = 1):\n\n\nAufeinanderfolgende Elemente werden zugegriffen.\nAlle 1024 Zugriffe → 1 neue Seite.\nInsgesamt: ca. N / 1024 Seiten.\nTLB-Misses bei N &gt; 64 * 1024 = 65536.\n\n\nGroßes M (z. B. M ≥ 1024):\n\n\nJeder Zugriff springt zu einer neuen Seite.\nEs werden ca. N / M unterschiedliche Seiten berührt.\nWenn N / M &gt; 64, treten TLB-Misses auf.\n\n\nM teilt 1024 (z. B. M = 256, 512):\n\n\nMehrere Schleifendurchläufe landen auf derselben Seite.\nBeispiel: M = 256 → 4 Zugriffe pro Seite, bevor zur nächsten gewechselt wird.\nWeniger Seiten werden benötigt.\nWeniger TLB-Misses, auch bei großem N.\n\nSchlechtester Fall für den TLB\nTritt auf, wenn:\n\nM ≥ 1024 (jeder Zugriff auf eine neue Seite)\nund N / M &gt; 64 (mehr als 64 Seiten werden benötigt)\n\nDann wird bei jedem Zugriff eine andere Seite nachgeladen → viele TLB-Misses.\nFazit:\nDie Anzahl der verschiedenen Seiten, auf die beim Schleifendurchlauf zugegriffen wird, lautet:\n\\[\n\\boxed{\n\\text{TLB\\_pages}(M, N) = \\left| \\left\\{ \\left\\lfloor \\frac{j \\cdot M}{1024} \\right\\rfloor \\;\\middle|\\; 0 \\le j \\le \\left\\lfloor \\frac{N - 1}{M} \\right\\rfloor \\right\\} \\right|\n}\n\\]\n\nTLB-Misses treten auf, wenn TLB_pages(M, N) &gt; 64\nKleine M (vor allem wenn M &lt; 1024 und M ein Teiler von 1024 ist) → mehrere Zugriffe pro Seite → besseres TLB-Verhalten\nGroße M (≥ 1024) → jeder Zugriff auf neue Seite → mehr TLB-Misses\n\nDieses Verhalten kann mit der folgenden Python-funktion simuliert werden:\n\n\ndef tlb_pages(M, N, ints_per_page=1024):\n    \"\"\"\n    Simulates the number of distinct pages accessed in the loop:\n        for (int i = 0; i &lt; N; i += M) X[i]++;\n\n    Parameters:\n    - M: step size\n    - N: total number of elements in the array\n    - ints_per_page: number of integers that fit in a single page (default 4096 bytes / 4 bytes = 1024)\n\n    Returns:\n    - The number of distinct pages accessed\n    \"\"\"\n    pages = set()\n    for i in range(0, N, M):\n        page = i // ints_per_page\n        pages.add(page)\n    return len(pages)\n\nEinige Simulationen:\n\nprint(tlb_pages(1, 65536))        # Should be 64 → fills exactly 64 pages\nprint(tlb_pages(1024, 65536))     # Should be 64 → each access on a new page\nprint(tlb_pages(256, 65536))      # Should be 64 / 4 = 16 → reuse of pages\nprint(tlb_pages(2048, 65536))     # Should be 32 → skips every second page\nprint(tlb_pages(1, 100000))       # result: 98 → causes TLB misses\n\n64\n64\n64\n32\n98\n\n\n\nDas Verhalten des TLB ändert sich deutlich, wenn der Code mehrfach oder regelmäßig ausgeführt wird – etwa in einer oft aufgerufenen Funktion oder in einer heißen Schleife.\n\nTLB ist zustandsbehaftet und begrenzt\n\nEr kann nur eine bestimmte Anzahl an Seitenadressen speichern (z. B. 64).\nWird die Anzahl der zugreifenden Seiten pro Schleife &gt; 64, kommt es zu Ersetzungen (evictions), meist nach dem LRU-Prinzip.\n\nWiederholte Ausführung kann TLB verbessern – oder verschlechtern\n\nWenn dieselben Seiten wiederverwendet werden (z. B. bei N ≤ 64 * 1024), bleiben TLB-Einträge erhalten → nach der ersten Ausführung keine weiteren Misses.\nWenn mehr als 64 Seiten verwendet oder ständig neue Seiten benötigt werden, werden TLB-Einträge ständig ersetzt → TLB-Misses bei jedem Aufruf.\n\nTLB-Arbeitsmenge (working set)\n\nDie „TLB-Arbeitsmenge“ ist die Menge der Seiten, die eine Funktion während der Ausführung benötigt.\nPasst diese Menge vollständig in den TLB, funktioniert alles effizient.\nIst sie größer, kommt es zu wiederholten Zugriffen auf die Page Table → langsam.\n\nZugriffsart ist entscheidend\n\nKleine Schrittweite M → viele Zugriffe auf dieselbe Seite → hohe Wiederverwendung → TLB effizient.\nGroße Schrittweite M ≥ 1024 → jeder Zugriff auf eine neue Seite → hoher TLB-Druck, vor allem bei vielen Funktionsaufrufen.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Blatt 05</span>"
    ]
  },
  {
    "objectID": "sh05/05.html#aufgabe-4",
    "href": "sh05/05.html#aufgabe-4",
    "title": "5  Blatt 05",
    "section": "Aufgabe 4",
    "text": "Aufgabe 4\nBeim Wechsel zwischen Prozessen wird der TLB in der Regel geleert, da jeder Prozess einen eigenen virtuellen Adressraum mit einer eigenen Seitentabelle besitzt. Die im TLB gespeicherten Einträge des vorherigen Prozesses wären im neuen Kontext ungültig oder sogar sicherheitskritisch.\nBeim Wechsel zwischen Threads desselben Prozesses bleibt der TLB hingegen erhalten, da alle Threads denselben Adressraum und dieselbe Seitentabelle nutzen. Die vorhandenen TLB-Einträge bleiben daher gültig.\nModerne Systeme mit Address Space Identifiers (ASIDs) können einen vollständigen TLB-Flush beim Prozesswechsel vermeiden, indem sie TLB-Einträge pro Prozess kennzeichnen und nur die jeweils relevanten aktiv halten.\nKein Flush - Was kann Schiefgehen?\nWenn der TLB beim Kontextwechsel nicht geleert wird, kann es zu schwerwiegenden Sicherheitsproblemen kommen. Das folgende Beispiel zeigt, was konkret passieren kann:\nAngenommen, Prozess A greift auf die virtuelle Adresse 0x00400000 zu, welche in seiner Seitentabelle korrekt auf die physische Adresse 0x1A300000 abgebildet wird. Diese Übersetzung wird im TLB zwischengespeichert.\nNun findet ein Kontextwechsel zu Prozess B statt. Auch Prozess B verwendet die virtuelle Adresse 0x00400000, aber in seiner eigenen Seitentabelle sollte sie auf eine völlig andere physische Adresse zeigen, z. B. 0x2B400000.\nWenn der TLB nicht geleert wird, verwendet der Prozessor beim Zugriff durch Prozess B weiterhin die alte TLB-Eintragung von Prozess A. Das führt dazu, dass Prozess B auf den physischen Speicher von Prozess A zugreift.\nDie Folgen:\n\nSicherheitslücke: Prozess B kann sensible Daten von Prozess A einsehen.\nDatenkorruption: Schreibzugriffe von Prozess B verändern versehentlich die Daten von Prozess A.\nVerletzung der Speicherisolation: Ein zentrales Prinzip des Betriebssystems wird untergraben.\n\nUm das zu verhindern, wird der TLB beim Wechsel des Prozesses entweder vollständig geleert oder — bei moderner Hardware — es werden ASIDs (Address Space Identifiers) verwendet, die TLB-Einträge pro Prozess kennzeichnen und voneinander trennen.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Blatt 05</span>"
    ]
  },
  {
    "objectID": "sh05/05.html#aufgabe-6",
    "href": "sh05/05.html#aufgabe-6",
    "title": "5  Blatt 05",
    "section": "Aufgabe 6",
    "text": "Aufgabe 6\n\nEs gibt \\(\\frac{2^{32}}{2^{12}} = 2^{20}\\) Einträge in der Tabelle. Jeder Eintrag ist 4 Byte groß\n\n\\[\n\\Rightarrow \\text{ca. } 4\\,\\text{MB} \\text{ Größe der Seitentabelle pro Prozess}.\n\\]\n\nBei einer invertierten Seitentabelle gibt es genau einen Eintrag pro Frame. Deshalb entspricht das Verhältnis der Tabellengröße zum physischen Speicher exakt dem Verhältnis der Größe eines Eintrags zur Frame- bzw. Seitengröße:\n\n\\[\n\\Rightarrow \\frac{4\\,\\text{B}}{4\\,\\text{KB}} = \\frac{1}{1024}\n\\]",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Blatt 05</span>"
    ]
  },
  {
    "objectID": "sh05/05.html#aufgabe-7",
    "href": "sh05/05.html#aufgabe-7",
    "title": "5  Blatt 05",
    "section": "Aufgabe 7",
    "text": "Aufgabe 7\nGeg. sei ein System mit einem TLB und einer hierarchischen Seitentabelle mit \\(k\\) Stufen. Die TLB-Trefferquote sei \\(h\\), die Zugriffszeit auf den TLB sei \\(t_{\\text{TLB}}\\), und ein RAM-Zugriff dauere \\(t_{\\text{RAM}}\\). Um eine Seite im Speicher zu lesen, muss zunächst die Adresse übersetzt und anschließend auf die eigentlichen Daten zugegriffen werden. Es gibt zwei Fälle:\n\nBei einem TLB-Treffer (Wahrscheinlichkeit \\(h\\)) erfolgt die Übersetzung über den TLB, was \\(t_{\\text{TLB}}\\) dauert, gefolgt von einem Datenzugriff mit \\(t_{\\text{RAM}}\\). Gesamtzeit: \\(t_{\\text{TLB}} + t_{\\text{RAM}}\\)\nBei einem TLB-Fehlzugriff (Wahrscheinlichkeit \\(1 - h\\)) muss die Seitentabelle durchlaufen werden, wobei \\(k\\) RAM-Zugriffe nötig sind. Anschließend folgt der Zugriff auf die Daten mit weiteren \\(t_{\\text{RAM}}\\). Gesamtzeit: \\((k + 1) \\cdot t_{\\text{RAM}}\\)\n\nDie erwartete Zugriffszeit ergibt sich zu:\n\\[\nE = h(t_{\\text{TLB}} + t_{\\text{RAM}}) + (1 - h)(k + 1)t_{\\text{RAM}}\n\\]\nWenn \\(h\\) klein ist, dominiert der zweite Term, und der Zugriff ist im Mittel etwa \\((k + 1)\\)-mal so teuer wie bei einem TLB-Treffer.\nIn der Praxis tritt dieses Problem jedoch kaum auf, da reale Programme ausgeprägte Lokalität aufweisen. Aufgrund temporaler Lokalität (wiederholte Zugriffe auf kürzlich genutzte Seiten) und spatialer Lokalität (benachbarte Adressen werden gemeinsam genutzt, z. B. in Arrays) ist die TLB-Trefferquote typischerweise sehr hoch (oft über 95 %). Deshalb amortisiert sich die Existenz eines TLB deutlich.\nDas zugrunde liegende Modell ist jedoch in mehreren Punkten idealisiert und in der Praxis eingeschränkt:\n\nEs nimmt gleichverteilte Zugriffe auf alle Seitentabelleneinträge an, ignoriert also die reale Zugriffslokalität.\nSeitentabelleneinträge werden ggf. auch intern gecacht, was die Zahl tatsächlicher RAM-Zugriffe reduziert.\nEs berücksichtigt keine Nebeneffekte wie TLB-Flushes bei Kontextwechseln, Prefetching, oder andere Optimierungen der Speicherhierarchie.\n\nDaher ist das Modell gut geeignet für eine theoretische Analyse, aber es bildet die tatsächliche Effizienz realer Systeme nur vereinfacht ab.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Blatt 05</span>"
    ]
  },
  {
    "objectID": "sh05/05.html#aufgabe-8",
    "href": "sh05/05.html#aufgabe-8",
    "title": "5  Blatt 05",
    "section": "Aufgabe 8",
    "text": "Aufgabe 8\n\n\n\n3-level Seitentabelle\n\n\n\nDas Diagramm zeigt die Übersetzung einer 32-Bit-Adressierung in einem hypothetischen System mit einer dreistufigen Seitentabellenhierarchie und einer Seitengröße von 256 Byte (also \\(2^8\\)). Die logische Adresse wird dabei in vier gleich große Abschnitte zu je 8 Bit aufgeteilt:\n\nBits 31–24: Index in die oberste Tabelle (PD1)\nBits 23–16: Index in die zweite Tabelle (PD2)\nBits 15–8: Index in die dritte Tabelle (PT)\nBits 7–0: Offset innerhalb der Seite\n\nJede Tabelle hat \\(2^8 = 256\\) Einträge. Dies ergibt sich daraus, dass jeder Tabellenindex 8 Bit umfasst und damit 256 mögliche Positionen adressieren kann. Da alle drei Tabellenebenen mit 8-Bit-Indizes angesprochen werden, besitzen alle drei Stufen genau 256 Einträge. Die Offset-Breite von 8 Bit entspricht der Seitengröße von 256 Byte.\nDie 32-Bit-Adresse wird in vier Abschnitte zu je 8 Bit unterteilt. Jeder dieser Abschnitte dient als Index in eine bestimmte Stufe der Seitentabellenhierarchie:\n\nDer erste Abschnitt (Bits 31–24) indexiert einen Eintrag in der obersten Tabelle (PD1).\nDer zweite Abschnitt (Bits 23–16) indexiert die mittlere Tabelle (PD2).\nDer dritte Abschnitt (Bits 15–8) indexiert die unterste Tabelle (PT).\nDer vierte Abschnitt (Bits 7–0) ist der Offset innerhalb der Zielseite.\n\nJeder Eintrag in den Tabellen enthält eine physische Adresse, die zur nächsten Stufe führt:\n\nEin PD1-Eintrag enthält die physische Startadresse eines PD2-Tabellenrahmens.\nEin PD2-Eintrag enthält die Adresse eines PT-Tabellenrahmens.\nEin PT-Eintrag enthält die Adresse eines tatsächlichen physischen Seitenrahmens, also einer 256-Byte-Seite im Speicher.\n\nDurch schrittweises Nachschlagen entlang der drei Tabellenstufen wird so der physische Rahmen gefunden, in dem sich die gewünschte Adresse befindet. Der Offset gibt schließlich die genaue Position innerhalb dieser Seite an.\nGibt es signifikante Unterschiede zwischen den Stufen?\nJa, in der Funktion der Stufen:\n\nDie ersten beiden Stufen (PD1 und PD2) dienen rein der Navigation: Sie verweisen jeweils auf weitere Tabellen.\nErst die dritte Stufe (PT) enthält den tatsächlichen Verweis auf den physischen Speicherrahmen mit den Daten.\nAuch bei der Interpretation der Einträge kann es Unterschiede geben (z. B. zusätzliche Statusbits oder Flags auf unteren Ebenen), aber im Grundprinzip enthalten alle Einträge physische Adressen von Seitenrahmen — entweder von Tabellen oder von Daten.\n\nJede Tabelle hat maximal \\(2^8 = 256\\) Einträge pro Instanz. Da es sich um eine 3-stufige Hierarchie handelt, ergibt sich im Extremfall (voll belegter Adressraum):\n\nStufe 1 (PD1): 1 Tabelle × 256 Einträge\nStufe 2 (PD2): 256 Tabellen × 256 Einträge = 65 536\nStufe 3 (PT): 256 × 256 Tabellen × 256 Einträge = 16 777 216\n\nKumulative Maximalanzahl:\n\\[\n256 + 65\\,536 + 16\\,777\\,216 = 16\\,843\\,008 \\text{ Einträge}\n\\]\nSpeicherverbrauch bei 4 Byte pro Eintrag:\n\\[\n16\\,843\\,008 \\times 4\\ \\text{Bytes} = 67\\,372\\,032\\ \\text{Bytes} \\approx 64{,}25\\ \\text{MiB}\n\\]",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Blatt 05</span>"
    ]
  },
  {
    "objectID": "sh06/06.html",
    "href": "sh06/06.html",
    "title": "6  Blatt 06",
    "section": "",
    "text": "Aufgabe 1",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Blatt 06</span>"
    ]
  },
  {
    "objectID": "sh06/06.html#aufgabe-1",
    "href": "sh06/06.html#aufgabe-1",
    "title": "6  Blatt 06",
    "section": "",
    "text": ":\nFIFO (First-In, First-Out)\n\n\n\n\n\n\n\n\n\n\nStep\nAccessed Page\nFrame State\nFIFO Queue\nAction\n\n\n\n\n1\n0\n[0]\n[0]\nPage fault (load 0)\n\n\n2\n1\n[0, 1]\n[0, 1]\nPage fault (load 1)\n\n\n3\n7\n[0, 1, 7]\n[0, 1, 7]\nPage fault (load 7)\n\n\n4\n2\n[0, 1, 7, 2]\n[0, 1, 7, 2]\nPage fault (load 2)\n\n\n5\n3\n[3, 1, 7, 2]\n[1, 7, 2, 3]\nPage fault (evict 0, load 3)\n\n\n6\n2\n[3, 1, 7, 2]\n[1, 7, 2, 3]\nNo fault\n\n\n7\n7\n[3, 1, 7, 2]\n[1, 7, 2, 3]\nNo fault\n\n\n8\n1\n[3, 1, 7, 2]\n[1, 7, 2, 3]\nNo fault\n\n\n9\n0\n[3, 0, 7, 2]\n[7, 2, 3, 0]\nPage fault (evict 1, load 0)\n\n\n10\n3\n[3, 0, 7, 2]\n[7, 2, 3, 0]\nNo fault\n\n\n\nErklärung: Der FIFO-Algorithmus verwaltet Seiten nach dem Prinzip „First-In, First-Out“. Wenn der Speicher voll ist, wird stets die älteste Seite (ganz vorne in der Queue) entfernt. Hier entstehen 6 Seitenfehler bei den Zugriffen: 1, 2, 3, 4, 5, 9.\nLRU (Least Recently Used):\n\n\n\nStep\nAccessed Page\nFrame State\nAction\n\n\n\n\n1\n0\n[0]\nPage fault (load 0)\n\n\n2\n1\n[0, 1]\nPage fault (load 1)\n\n\n3\n7\n[0, 1, 7]\nPage fault (load 7)\n\n\n4\n2\n[0, 1, 7, 2]\nPage fault (load 2)\n\n\n5\n3\n[3, 1, 7, 2]\nPage fault (evict 0, load 3)\n\n\n6\n2\n[3, 1, 7, 2]\nNo fault\n\n\n7\n7\n[3, 1, 7, 2]\nNo fault\n\n\n8\n1\n[3, 1, 7, 2]\nNo fault\n\n\n9\n0\n[0, 1, 7, 2]\nPage fault (evict 3, load 0)\n\n\n10\n3\n[0, 1, 7, 3]\nPage fault (evict 2, load 3)\n\n\n\nErklärung: Der LRU-Algorithmus entfernt immer die Seite, die am längsten nicht verwendet wurde. Er berücksichtigt dabei die Reihenfolge der letzten Zugriffe. In diesem Beispiel entstehen 7 Seitenfehler bei den Zugriffen: 1, 2, 3, 4, 5, 9, 10.\n:\nVergleich: LRU vs. Clock – Erstes unterschiedliches Opfer\nWir suchen eine möglichst kurze Zugriffssequenz, bei der LRU und Clock beim ersten Page-Fault mit Ersetzung unterschiedliche Seiten auslagern. Dies zeigt konkret, wie Clock als Annäherung an LRU funktioniert, aber nicht exakt gleich entscheidet.\nRahmenbedingungen:\n\nAnzahl Frames: 3\nZugriffssequenz: 1, 2, 3, 1, 4\nSeitenzahlen stammen aus {1, ..., 9}\n\nLRU (Least Recently Used)\nLRU wählt die Seite aus, die am längsten nicht mehr verwendet wurde.\n\n\n\n\n\n\n\n\n\nStep\nAccessed Page\nFrame State (after access)\nAction\n\n\n\n\n1\n1\n[1]\nPage fault\n\n\n2\n2\n[1, 2]\nPage fault\n\n\n3\n3\n[1, 2, 3]\nPage fault\n\n\n4\n1\n[1, 2, 3]\nNo fault (update usage)\n\n\n5\n4\n[1, 4, 3]\nPage fault → evict 2\n\n\n\nErklärung: LRU entfernt Seite 2, da sie seit Schritt 2 nicht mehr verwendet wurde.\nClock (Second-Chance)\nClock gibt Seiten mit gesetztem R-Bit eine „zweite Chance“. Der Zeiger dreht sich zirkulär durch die Frames.\n\nNeue Seiten: R = 1\nZugriff: R ← 1\nBeim Ersetzen:\n\nWenn R = 1: R ← 0, weiter\nWenn R = 0: auslagern\n\n\nAnfangszustand vor Schritt 5:\n\nFrame: [1, 2, 3]\nR bits: [1, 1, 1]\nPointer: → 3 (zeigt auf Seite 3)\n\n\n\n\n\n\n\n\n\n\n\n\nStep\nAccessed Page\nFrame State (after access)\nR Bits\nPointer\nAction\n\n\n\n\n1\n1\n[1]\n[1]\n→1\nPage fault\n\n\n2\n2\n[1, 2]\n[1, 1]\n→2\nPage fault\n\n\n3\n3\n[1, 2, 3]\n[1, 1, 1]\n→3\nPage fault\n\n\n4\n1\n[1, 2, 3]\n[1, 1, 1]\n→3\nNo fault\n\n\n5\n4\n[1, 2, 4]\n[0, 0, 1]\n→1\nPage fault → evict 3\n\n\n\nErklärung: Clock entfernt Seite 3, da beim Durchlauf alle R-Bits auf 1 gesetzt waren und beim zweiten Umlauf zuerst Seite 3 mit R = 0 erreicht wurde.\nVergleich\n\n\n\nAlgorithmus\nErste ersetzte Seite\n\n\n\n\nLRU\n2\n\n\nClock\n3\n\n\n\nDiese kurze Sequenz zeigt präzise, wie Clock trotz ähnlicher Zielsetzung (ältere Seiten auslagern) durch seine heuristische Umsetzung (R-Bits und Zeiger) zu anderen Entscheidungen als LRU kommt.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Blatt 06</span>"
    ]
  },
  {
    "objectID": "sh06/06.html#aufgabe-2",
    "href": "sh06/06.html#aufgabe-2",
    "title": "6  Blatt 06",
    "section": "Aufgabe 2",
    "text": "Aufgabe 2\nObwohl LRU – im Gegensatz zu OPT – die Zukunft nicht kennt, kann es mit exakten Zeitstempeln vergangener Seitenzugriffe fundierte Entscheidungen treffen. OPT hingegen nutzt zukünftige Zugriffe zur Minimierung der Seitenfehler und stellt damit die theoretisch beste Strategie dar. Zwischen beiden Algorithmen besteht ein enger mathematischer Zusammenhang: Die Anzahl der Seitenfehler von LRU ist im schlimmsten Fall höchstens k-mal so groß wie die von OPT, wobei k die Anzahl der verfügbaren Seitenrahmen ist. Dieser Zusammenhang stammt aus der kompetitiven Analyse von Online-Algorithmen und ist zwar theoretisch interessant, jedoch nicht praktisch nutzbar, da er keine konkreten Verbesserungen im realen Betrieb ermöglicht.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Blatt 06</span>"
    ]
  },
  {
    "objectID": "sh06/06.html#aufgabe-3",
    "href": "sh06/06.html#aufgabe-3",
    "title": "6  Blatt 06",
    "section": "Aufgabe 3",
    "text": "Aufgabe 3\nIm folgenden Python-programm haben wir den LRU-Algorithmus implementiert (auch im Zip als ‘lru_sim.py’ enthalten):\nimport sys\n\ndef print_frame_state(step, page, frames, action):\n    print(f\"Step {step:2}: Page {page:2} → {action:6} | Frames: {frames}\")\n\ndef simulate_lru(num_frames, access_sequence):\n    frames = []\n    lru_order = []  # Tracks least to most recently used pages\n\n    for step, page in enumerate(access_sequence, 1):\n        if page in frames:\n            action = \"Hit\"\n            # Move page to most recently used\n            lru_order.remove(page)\n            lru_order.append(page)\n        else:\n            action = \"Fault\"\n            if len(frames) &lt; num_frames:\n                # Free space available\n                frames.append(page)\n            else:\n                # Evict least recently used page\n                lru_page = lru_order.pop(0)\n                index = frames.index(lru_page)\n                frames[index] = page\n            lru_order.append(page)\n        print_frame_state(step, page, frames.copy(), action)\n\nif __name__ == \"__main__\":\n    if len(sys.argv) &lt; 3:\n        print(\"Usage: python lru_sim.py &lt;num_frames&gt; &lt;page1&gt; &lt;page2&gt; ...\")\n        sys.exit(1)\n\n    try:\n        num_frames = int(sys.argv[1])\n        access_sequence = list(map(int, sys.argv[2:]))\n    except ValueError:\n        print(\"Error: All inputs must be integers.\")\n        sys.exit(1)\n\n    simulate_lru(num_frames, access_sequence)\nAlgorithmusprinzip (LRU)\nBeim LRU-Verfahren wird stets die am längsten nicht benutzte Seite entfernt, wenn ein neuer Seitenzugriff erfolgt und kein freier Rahmen mehr verfügbar ist. Das Verfahren benötigt daher eine Struktur, um die Zugriffshistorie zu verfolgen.\nUmsetzung in Python:\nFür die Python-Implementierung wurden folgende Datenstrukturen verwendet:\n\nframes: Eine Liste, die den aktuellen Inhalt der Seitenrahmen repräsentiert.\nlru_order: Eine separate Liste, die die Zugriffsreihenfolge der Seiten hält – von ältestem zu jüngstem Zugriff.\n\nAblauf:\n\nBei einem Treffer (Hit) wird die Seite in lru_order nach hinten verschoben (neuester Zugriff).\nBei einem Seitenfehler (Fault):\n\nWenn Platz frei ist → Seite wird einfach geladen.\nWenn kein Platz mehr frei ist → Die Seite ganz vorne in lru_order wird entfernt (am längsten unbenutzt) und im Rahmen ersetzt.\n\n\nBeispielaufruf:\npython lru_sim.py 3 7 0 1 2 0 3 0 4 2 3\nDer erste Input ist die Anzahl der Rahmen und die restlichen Zahlen stellen die Referenzfolge dar.\nHinweis:\nDie Implementierung nutzt bewusst nur grundlegende Datenstrukturen (list), um die LRU-Logik transparent und nachvollziehbar zu gestalten. Für größere Datenmengen könnten effizientere Strukturen wie collections.OrderedDict verwendet werden.\nAusgabelogs für Referenzfolgen A und B:\n\nA:\nA=\"7 0 1 2 0 3 0 4 2 3 0 3 2 1 2 0 1 7 0 1\"\npython lru_sim.py 3 $A\noutput (auch im ZIP als A-log.txt enthalten):\nStep  1: Page  7 → Fault  | Frames: [7]\nStep  2: Page  0 → Fault  | Frames: [7, 0]\nStep  3: Page  1 → Fault  | Frames: [7, 0, 1]\nStep  4: Page  2 → Fault  | Frames: [2, 0, 1]\nStep  5: Page  0 → Hit    | Frames: [2, 0, 1]\nStep  6: Page  3 → Fault  | Frames: [2, 0, 3]\nStep  7: Page  0 → Hit    | Frames: [2, 0, 3]\nStep  8: Page  4 → Fault  | Frames: [4, 0, 3]\nStep  9: Page  2 → Fault  | Frames: [4, 0, 2]\nStep 10: Page  3 → Fault  | Frames: [4, 3, 2]\nStep 11: Page  0 → Fault  | Frames: [0, 3, 2]\nStep 12: Page  3 → Hit    | Frames: [0, 3, 2]\nStep 13: Page  2 → Hit    | Frames: [0, 3, 2]\nStep 14: Page  1 → Fault  | Frames: [1, 3, 2]\nStep 15: Page  2 → Hit    | Frames: [1, 3, 2]\nStep 16: Page  0 → Fault  | Frames: [1, 0, 2]\nStep 17: Page  1 → Hit    | Frames: [1, 0, 2]\nStep 18: Page  7 → Fault  | Frames: [1, 0, 7]\nStep 19: Page  0 → Hit    | Frames: [1, 0, 7]\nStep 20: Page  1 → Hit    | Frames: [1, 0, 7]\nB:\nB=\"2 3 2 1 5 2 4 5 3 2 5 2\"\npython lru_sim.py 3 $B\noutput (auch zim als B-log.txt enthalten):\nStep  1: Page  2 → Fault  | Frames: [2]\nStep  2: Page  3 → Fault  | Frames: [2, 3]\nStep  3: Page  2 → Hit    | Frames: [2, 3]\nStep  4: Page  1 → Fault  | Frames: [2, 3, 1]\nStep  5: Page  5 → Fault  | Frames: [2, 5, 1]\nStep  6: Page  2 → Hit    | Frames: [2, 5, 1]\nStep  7: Page  4 → Fault  | Frames: [2, 5, 4]\nStep  8: Page  5 → Hit    | Frames: [2, 5, 4]\nStep  9: Page  3 → Fault  | Frames: [3, 5, 4]\nStep 10: Page  2 → Fault  | Frames: [3, 5, 2]\nStep 11: Page  5 → Hit    | Frames: [3, 5, 2]\nStep 12: Page  2 → Hit    | Frames: [3, 5, 2]",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Blatt 06</span>"
    ]
  },
  {
    "objectID": "sh06/06.html#aufgabe-4",
    "href": "sh06/06.html#aufgabe-4",
    "title": "6  Blatt 06",
    "section": "Aufgabe 4",
    "text": "Aufgabe 4\n\nIm Worst Case sind alle 20 Prozesse aktiv und belegen jeweils den gesamten virtuellen Adressraum des IA32-Systems. Bei einer Seitengröße von 4 KiB ergibt sich für jeden Prozess eine Seitentabelle mit\n\\[\n\\frac{2^{32}}{2^{12}} = 2^{20} = 1{.}048{.}576 \\text{ Seiten}\n\\]\nInsgesamt müssen also\n\\[\n20 \\times 2^{20} = 20{.}971{.}520 \\text{ Seiteneinträge}\n\\]\nbetrachtet werden. Da laut Aufgabenstellung das Lesen und Zurücksetzen des R-Bits pro Eintrag im Mittel 10 Taktzyklen benötigt, ergibt sich ein Gesamtaufwand von\n\\[\n20{.}971{.}520 \\times 10 = 209{.}715{.}200 \\text{ Taktzyklen}\n\\]\nBei einem Prozessor mit 1 GHz Taktfrequenz entspricht das einer Zeitdauer von\n\\[\n\\frac{209{.}715{.}200}{1{.}000{.}000{.}000} = 0{,}2097\\ \\text{Sekunden} \\approx 210\\ \\text{ms}\n\\]\nIm Worst Case benötigt das System also rund 210 ms pro Epoche, um alle R-Bits der Seitentabellen zu überprüfen und zurückzusetzen.\nDamit das regelmäßige Scannen der R-Bits die Gesamtleistung des Systems nicht spürbar beeinträchtigt, sollte die Epochendauer so gewählt werden, dass der dabei entstehende Rechenaufwand nur einen kleinen Teil der Zeit ausmacht. Eine sinnvolle Faustregel ist, dass der Verwaltungsaufwand maximal etwa 10 % der gesamten Rechenzeit betragen sollte.\nWenn das Scannen der R-Bits im Worst Case rund 210 ms dauert, ergibt sich daraus eine minimale sinnvolle Epochendauer von:\n\\[\n\\frac{210\\ \\text{ms}}{0{,}1} = 2100\\ \\text{ms}\n\\]\nEine Epoche von etwa 2 Sekunden ist somit eine sinnvolle Wahl. Dadurch bleibt der relative Aufwand für die Speicherverwaltung auch im ungünstigsten Fall moderat. In der Praxis würde dieser Aufwand meist noch deutlich geringer ausfallen, da typischerweise weniger Prozesse aktiv sind und nicht alle Seitentabellen vollständig gefüllt sind.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Blatt 06</span>"
    ]
  },
  {
    "objectID": "sh06/06.html#aufgabe-5",
    "href": "sh06/06.html#aufgabe-5",
    "title": "6  Blatt 06",
    "section": "Aufgabe 5",
    "text": "Aufgabe 5\nJa, eine Seite kann gleichzeitig zu mehreren Working Sets gehören, allerdings nur, wenn es sich um eine gemeinsam genutzte Seite handelt (zum Beispiel bei Shared Memory oder gemappten Dateien) und sie von jedem der betreffenden Prozesse kürzlich verwendet wurde. Da das Working Set prozessbezogen ist, enthält es nur Seiten, die der jeweilige Prozess innerhalb des festgelegten Zeitfensters selbst genutzt hat. Wird eine geteilte Seite von mehreren Prozessen aktiv verwendet, so gehört sie gleichzeitig zu den Working Sets dieser Prozesse.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Blatt 06</span>"
    ]
  },
  {
    "objectID": "sh06/06.html#aufgabe-6",
    "href": "sh06/06.html#aufgabe-6",
    "title": "6  Blatt 06",
    "section": "Aufgabe 6",
    "text": "Aufgabe 6\nProblemstellung\nGegeben ist:\n\neine Referenzfolge von Seitenzugriffen (z. B. als Datei gespeichert),\neine natürliche Zahl ∆ (Delta), die die Größe des betrachteten Fensters angibt.\n\nGesucht ist:\n\nEin größtes Working Set, d. h. eine Menge von Seiten, die in einem beliebigen Fenster der letzten ∆ Speicherzugriffe aufgetreten sind, wobei das Fenster über die gesamte Referenzfolge verschoben wird.\n\nDas Ziel ist also nicht das Working Set am Ende der Referenzfolge, sondern das Working Set, das an irgendeiner Stelle die größte Anzahl unterschiedlicher Seiten enthält.\nPseudocode:\nfunktion berechne_groesstes_working_set(delta, dateiname):\n    initialisiere leere Liste window\n    initialisiere leere Hashtabelle page_count\n    initialisiere leere Menge current_ws\n    initialisiere leere Menge max_ws_snapshot\n\n    lese Inhalt der Datei mit Dateiname in ein Array pageAccesses ein\n\n    für jede Seite page in pageAccesses:\n        // Seite zum Fenster hinzufügen\n        window.append(page)\n        falls page nicht in page_count:\n            page_count[page] = 1\n            current_ws.add(page)\n        sonst:\n            page_count[page] += 1\n\n        // Wenn das Fenster zu groß wird, älteste Seite entfernen\n        falls window.größe &gt; delta:\n            oldest = window.pop_links()\n            page_count[oldest] -= 1\n            falls page_count[oldest] == 0:\n                entferne page_count[oldest]\n                current_ws.remove(oldest)\n\n        // Maximales Working Set ggf. aktualisieren\n        falls current_ws.größe &gt; max_ws_snapshot.größe:\n            max_ws_snapshot = kopie_von(current_ws)\n\n    gib max_ws_snapshot aus\nVerwendete Datenstrukturen und Variablen:\n\n\n\n\n\n\n\n\nName\nTyp\nBeschreibung\n\n\n\n\nwindow\nWarteschlange / Liste\nEnthält die letzten ∆ Seitenzugriffe\n\n\npage_count\nHashtabelle (Seite → Anzahl)\nSpeichert, wie oft jede Seite im aktuellen Fenster vorkommt\n\n\ncurrent_ws\nMenge (Set)\nEnthält die aktuell im Fenster vorhandenen unterschiedlichen Seiten\n\n\nmax_ws_snapshot\nMenge (Set)\nEnthält ein Working Set mit der maximalen bisher gefundenen Größe\n\n\n\n\nBeispielausührung\nEingabeparameter:\n\n∆ = 5\nReferenzfolge (z. B. in Datei gespeichert):\n\n1\n2\n3\n4\n2\n1\n5\n2\n6\n7\n2\n3\nSchritt-für-Schritt-Auswertung:\nWir verschieben ein Fenster der Größe 5 über die Folge und bestimmen dabei das Working Set (WS), also die Menge der verschiedenen Seiten im aktuellen Fenster.\n\n\n\n\n\n\n\n\n\n\nFenster (Position)\nFensterinhalt\nAktuelles WS\nGröße\nGrößtes WS bisher\n\n\n\n\n1–5\n[1, 2, 3, 4, 2]\n{1, 2, 3, 4}\n4\n{1, 2, 3, 4}\n\n\n2–6\n[2, 3, 4, 2, 1]\n{1, 2, 3, 4}\n4\n—\n\n\n3–7\n[3, 4, 2, 1, 5]\n{1, 2, 3, 4, 5}\n5\n{1, 2, 3, 4, 5}\n\n\n4–8\n[4, 2, 1, 5, 2]\n{1, 2, 4, 5}\n4\n—\n\n\n5–9\n[2, 1, 5, 2, 6]\n{1, 2, 5, 6}\n4\n—\n\n\n6–10\n[1, 5, 2, 6, 7]\n{1, 2, 5, 6, 7}\n5\n— (ebenfalls max.)\n\n\n7–11\n[5, 2, 6, 7, 2]\n{2, 5, 6, 7}\n4\n—\n\n\n8–12\n[2, 6, 7, 2, 3]\n{2, 3, 6, 7}\n4\n—\n\n\n\nAusgabe des Programms:\nDas Programm gibt eines der größten Working Sets aus, z. B.:\n{1, 2, 3, 4, 5}\nGröße = 5\nDas entspricht der ersten Stelle im Verlauf, an der ein Fenster mit 5 verschiedenen Seiten auftritt.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Blatt 06</span>"
    ]
  },
  {
    "objectID": "sh07/07.html",
    "href": "sh07/07.html",
    "title": "7  Blatt 07",
    "section": "",
    "text": "Aufgabe 1",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Blatt 07</span>"
    ]
  },
  {
    "objectID": "sh07/07.html#aufgabe-1",
    "href": "sh07/07.html#aufgabe-1",
    "title": "7  Blatt 07",
    "section": "",
    "text": "Vorteil von Hard Links: Bleiben gültig, auch wenn die Originaldatei gelöscht wird, da sie direkt auf die Inode verweisen.\nVorteil von Soft Links: Können Dateien über verschiedene Dateisysteme hinweg verlinken, da sie den Pfad statt der Inode speichern.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Blatt 07</span>"
    ]
  },
  {
    "objectID": "sh07/07.html#aufgabe-2",
    "href": "sh07/07.html#aufgabe-2",
    "title": "7  Blatt 07",
    "section": "Aufgabe 2",
    "text": "Aufgabe 2",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Blatt 07</span>"
    ]
  },
  {
    "objectID": "sh07/07.html#aufgabe-3",
    "href": "sh07/07.html#aufgabe-3",
    "title": "7  Blatt 07",
    "section": "Aufgabe 3",
    "text": "Aufgabe 3\n\nAnalogien:\n\nlogische Seiten (logische Adressen) ↔︎ Datei-Block-Offsets (logische Datei-Adressen) : Eine logische Adresse besteht aus einer Seitennummer und einem Offset innerhalb der Seite; ebenso wird eine Datei über Blocknummer und Offset adressiert. Beide sind abstrakte Adressen, die auf reale Speicherorte abgebildet werden.\nphysische Frames (physische Adressen) ↔︎ Festplattenblöcke : Ein Frame im RAM und ein Block auf der Festplatte sind physische Einheiten fester Größe (z. B. 4 KB), in denen tatsächliche Daten gespeichert werden.\nFrame-Nummer ↔︎ Blocknummer : Die Seitentabelle enthält Frame-Nummern (Index im physischen RAM); der i-node enthält Blocknummern (Index auf der Festplatte). Beide zeigen, wo genau sich die Daten physisch befinden.\nSeitentabelle ↔︎ i-node : Die Seitentabelle eines Prozesses ordnet virtuelle Seiten physischen Frames zu; der i-node einer Datei ordnet logische Blöcke physischen Festplattenblöcken zu. Beide sind zentrale Strukturen für die Adressübersetzung.\n\nDas Problem der übergroßen Seitentabellen entspricht bei I-Nodes dem Problem, dass große Dateien nicht allein mit direkten Blockzeigern adressiert werden können, da die I-Nodes sonst zu groß würden. Die Lösung ist in beiden Fällen ähnlich: Hierarchische Seitentabellen in der Speicherverwaltung entsprechen ein- oder mehrstufigen indirekten Blöcken bei I-Nodes. Welche Form der „Seitentabelle“ (direkt oder indirekt) verwendet wird, hängt bei I-Nodes von der Dateigröße ab – kleine Dateien nutzen nur direkte Zeiger, große benötigen zusätzliche Indirektion.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Blatt 07</span>"
    ]
  },
  {
    "objectID": "sh07/07.html#aufgabe-4",
    "href": "sh07/07.html#aufgabe-4",
    "title": "7  Blatt 07",
    "section": "Aufgabe 4",
    "text": "Aufgabe 4\nJeder Zeiger adressiert Blöcke der Größe 1024 Byte, und jede Blockadresse belegt 4 Byte.\nDie 14 direkten Zeiger verweisen direkt auf Datenblöcke und ermöglichen somit den Zugriff auf 14 × 1024 = 14.336 Byte.\nJeder der beiden indirekten Zeiger zeigt auf einen weiteren Block, der ausschließlich Blockadressen enthält. Da ein Block 1024 Byte groß ist und jede Adresse 4 Byte belegt, passen 1024 / 4 = 256 Adressen in einen solchen Block. Diese 256 Adressen verweisen jeweils auf Datenblöcke zu je 1024 Byte, sodass pro indirektem Zeiger 256 × 1024 = 262.144 Byte adressiert werden können. Zwei indirekte Zeiger ergeben somit 2 × 262.144 = 524.288 Byte.\nDie maximale Dateigröße beträgt daher 14.336 + 524.288 = 538.624 Byte \\(\\approx\\) 0.5 MB.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Blatt 07</span>"
    ]
  },
  {
    "objectID": "sh07/07.html#aufgabe-5",
    "href": "sh07/07.html#aufgabe-5",
    "title": "7  Blatt 07",
    "section": "Aufgabe 5",
    "text": "Aufgabe 5\nZur Auflösung des Pfades /usr/aa/lehre/ibn/ueb7-ibn-2017.pdf müssen nacheinander die Verzeichniseinträge gelesen und die entsprechenden I-Nodes geladen werden. Der Pfad besteht aus fünf Komponenten: usr, aa, lehre, ibn und ueb7-ibn-2017.pdf.\nDa der I-Node des Wurzelverzeichnisses bereits im Hauptspeicher liegt, entfällt dafür eine Operation. Für jede der übrigen Komponenten müssen jedoch:\n\nDer Plattenblock des Verzeichnisses gelesen werden (um den Dateinamen → I-Node-Nummer zu finden),\nDer entsprechende I-Node geladen werden (um zum nächsten Element zu gelangen).\n\nSomit fallen für jede der 5 Komponenten 2 Plattenoperationen an: einmal Lesen des Verzeichnisblocks, einmal Lesen des I-Nodes.\n\\(\\Rightarrow\\) Es werden insgesamt 5 × 2 = 10 Plattenoperationen benötigt.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Blatt 07</span>"
    ]
  },
  {
    "objectID": "sh07/07.html#aufgabe-6",
    "href": "sh07/07.html#aufgabe-6",
    "title": "7  Blatt 07",
    "section": "Aufgabe 6",
    "text": "Aufgabe 6\n\nDie Drehzahl beträgt 1500 Umdrehungen pro Minute, also dauert eine vollständige Umdrehung 60 s / 1500 = 0,04 s = 40 ms. Da die mittlere Drehlatenz einer halben Umdrehung entspricht, ergibt sich 40 ms / 2 = 20 ms. Die mittlere Drehlatenz beträgt somit 20 Millisekunden.\nUm diese Aufgabe zu lösen, gilt: Die durchschnittliche Zugriffszeit ergibt sich aus der Summe von mittlerer Spurwechselzeit und mittlerer Drehlatenz.\nGegeben ist eine mittlere Spurwechselzeit von 85 ms und eine mittlere Drehlatenz von 20 ms. Damit ergibt sich eine durchschnittliche Zugriffszeit von 85 ms + 20 ms = 105 ms.\nFür den Zugriff auf 10 unabhängig und zufällig ausgewählte Datenblöcke ergibt sich eine mittlere Gesamtdauer von 10 × 105 ms = 1050 ms = 1,05 s.\nGegeben ist eine mittlere Drehlatenz von 8,333 ms. Da dies der Hälfte einer Umdrehung entspricht, dauert eine vollständige Umdrehung 2 × 8,333 ms = 16,666 ms.\nDie Anzahl der Umdrehungen pro Sekunde beträgt daher 1 s / 0,016666 s ≈ 60. Das entspricht 60 Umdrehungen pro Sekunde bzw. 60 × 60 = 3600 U/min.\nDie Platten drehten sich also mit 3600 Umdrehungen pro Minute.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Blatt 07</span>"
    ]
  },
  {
    "objectID": "sh07/07.html#aufgabe-7",
    "href": "sh07/07.html#aufgabe-7",
    "title": "7  Blatt 07",
    "section": "Aufgabe 7",
    "text": "Aufgabe 7\nWir haben:\n\nZylinderbereich: 0 bis 4999\nStartposition: Kopf ist aktuell bei 143, kam gerade von 125 (d. h. aktuelle Bewegungsrichtung ist aufwärts)\nAnfragen in Ankunftsreihenfolge: 86, 1470, 913, 1774, 948, 1509, 1022, 1750, 130\n\nWir berechnen nun für jeden Algorithmus:\n\nReihenfolge der bedienten Zylinder\nGesamte Bewegung des Kopfes (in Zylindern)\n\n\nFCFS (First-Come, First-Serve)\nVerarbeitung in Ankunftsreihenfolge, egal wie weit entfernt:\n\nStart bei 143\nFolge: 86, 1470, 913, 1774, 948, 1509, 1022, 1750, 130\n\nBerechnung der Bewegungen:\n\n|143 - 86| = 57\n|86 - 1470| = 1384\n|1470 - 913| = 557\n|913 - 1774| = 861\n|1774 - 948| = 826\n|948 - 1509| = 561\n|1509 - 1022| = 487\n|1022 - 1750| = 728\n|1750 - 130| = 1620\n\nSumme: 57 + 1384 + 557 + 861 + 826 + 561 + 487 + 728 + 1620 = 7081 Zylinder\n\n\nSSTF (Shortest Seek Time First)\nHier wird immer die nächstgelegene Anfrage (bezogen auf aktuelle Kopfposition) bedient.\n\nStartposition: 143\nOffene Anfragen: 86, 1470, 913, 1774, 948, 1509, 1022, 1750, 130\n\nWir wählen immer den nächstgelegenen Zylinder zur aktuellen Position. Schritte:\n\nAktuell bei 143 Nächster: 130 (|143–130| = 13)\n→ neue Position: 130\nÜbrig: 86, 1470, 913, 1774, 948, 1509, 1022, 1750\nAktuell bei 130\nNächster: 86 (|130–86| = 44)\n→ neue Position: 86\nÜbrig: 1470, 913, 1774, 948, 1509, 1022, 1750\nAktuell bei 86\nNächster: 913 (|86–913| = 827)\n→ neue Position: 913\nÜbrig: 1470, 1774, 948, 1509, 1022, 1750\nAktuell bei 913\nNächster: 948 (|913–948| = 35)\n→ neue Position: 948\nÜbrig: 1470, 1774, 1509, 1022, 1750\nAktuell bei 948\nNächster: 1022 (|948–1022| = 74)\n→ neue Position: 1022\nÜbrig: 1470, 1774, 1509, 1750\nAktuell bei 1022\nNächster: 1470 (|1022–1470| = 448)\n→ neue Position: 1470\nÜbrig: 1774, 1509, 1750\nAktuell bei 1470\nNächster: 1509 (|1470–1509| = 39)\n→ neue Position: 1509\nÜbrig: 1774, 1750\nAktuell bei 1509\nNächster: 1750 (|1509–1750| = 241)\n→ neue Position: 1750\nÜbrig: 1774\nAktuell bei 1750\nNächster: 1774 (|1750–1774| = 24)\n→ neue Position: 1774\nÜbrig: —\n\nReihenfolge: 143 → 130 → 86 → 913 → 948 → 1022 → 1470 → 1509 → 1750 → 1774\nBewegungen:\n\n|143–130| = 13\n|130–86| = 44\n|86–913| = 827\n|913–948| = 35\n|948–1022| = 74\n|1022–1470| = 448\n|1470–1509| = 39\n|1509–1750| = 241\n|1750–1774| = 24\n\nSumme: 13 + 44 + 827 + 35 + 74 + 448 + 39 + 241 + 24 = 1745 Zylinder\n\n\nSCAN (Fahrstuhl-Algorithmus)\nDer Lesekopf bewegt sich in eine Richtung (hier: aufwärts, weil er von 125 nach 143 kam), bedient dabei alle Anfragen auf dem Weg, und kehrt dann am Ende um.\n\nStartposition: 143\nBewegungsrichtung: aufwärts\nAnfragen: 86, 1470, 913, 1774, 948, 1509, 1022, 1750, 130\n\nWir teilen in zwei Gruppen:\n\nAufwärts (≥143): 1470, 913, 1774, 948, 1509, 1022\nAbwärts (&lt;143): 86, 130\n\nZuerst behandeln wir alle Anfragen in aufsteigender Reihenfolge ≥143: → 913, 948, 1022, 1470, 1509, 1774 Dann kehrt der Kopf um und bedient absteigend: → 130, 86\nReihenfolge des Besuchs: 143 → 913 → 948 → 1022 → 1470 → 1509 → 1774 → 130 → 86\nBewegungen:\n\n|143–913| = 770\n|913–948| = 35\n|948–1022| = 74\n|1022–1470| = 448\n|1470–1509| = 39\n|1509–1774| = 265\n|1774–130| = 1644\n|130–86| = 44\n\nSumme: 770 + 35 + 74 + 448 + 39 + 265 + 1644 + 44 = 3319 Zylinder\n\n\nC-SCAN (Circular SCAN)\nDer Kopf bewegt sich in eine Richtung (hier: aufwärts) und bedient alle Anfragen auf dem Weg. Am Ende (bei höchstem Zylinder) springt der Kopf ohne Bedienung zurück zum Anfang und beginnt erneut.\n\nStartposition: 143\nBewegungsrichtung: aufwärts\nAnfragen: 86, 1470, 913, 1774, 948, 1509, 1022, 1750, 130\n\nAnfragen ≥143 (in aufsteigender Reihenfolge): → 913, 948, 1022, 1470, 1509, 1750, 1774\nAnfragen &lt;143 (werden erst nach Rücksprung behandelt, ebenfalls aufsteigend): → 86, 130\nReihenfolge des Besuchs: 143 → 913 → 948 → 1022 → 1470 → 1509 → 1750 → 1774 → Sprung zu 0 → 86 → 130\nBewegungen:\n\n|143–913| = 770\n|913–948| = 35\n|948–1022| = 74\n|1022–1470| = 448\n|1470–1509| = 39\n|1509–1750| = 241\n|1750–1774| = 24\n|1774–0| = 1774 (Sprung ohne Bedienung)\n|0–86| = 86\n|86–130| = 44\n\nSumme: 770 + 35 + 74 + 448 + 39 + 241 + 24 + 1774 + 86 + 44 = 3535 Zylinder\n\n\nC-LOOK (Circular LOOK)\n\nStartposition: 143\nBewegungsrichtung: aufwärts\nAnfragen: 86, 1470, 913, 1774, 948, 1509, 1022, 1750, 130\n\nZuerst alle Anfragen ≥143 (sortiert): → 913, 948, 1022, 1470, 1509, 1750, 1774\nDann Sprung zurück zum niedrigsten angefragten Zylinder (&lt;143): → 86, 130 (ebenfalls aufsteigend)\nReihenfolge des Besuchs: 143 → 913 → 948 → 1022 → 1470 → 1509 → 1750 → 1774 → 86 → 130\nBewegungen:\n\n|143–913| = 770\n|913–948| = 35\n|948–1022| = 74\n|1022–1470| = 448\n|1470–1509| = 39\n|1509–1750| = 241\n|1750–1774| = 24\n|1774–86| = 1688 (Sprung zurück)\n|86–130| = 44\n\nSumme: 770 + 35 + 74 + 448 + 39 + 241 + 24 + 1688 + 44 = 3363 Zylinder\n\n\nVergleichstabelle: Disk Scheduling\n\n\n\n\n\n\n\n\nAlgorithmus\nReihenfolge der besuchten Zylinder\nGesamtdistanz (Zylinder)\n\n\n\n\nFCFS\n143 → 86 → 1470 → 913 → 1774 → 948 → 1509 → 1022 → 1750 → 130\n7081\n\n\nSSTF\n143 → 130 → 86 → 913 → 948 → 1022 → 1470 → 1509 → 1750 → 1774\n1897\n\n\nSCAN\n143 → 913 → 948 → 1022 → 1470 → 1509 → 1774 → 130 → 86\n3319\n\n\nC-SCAN\n143 → 913 → 948 → 1022 → 1470 → 1509 → 1750 → 1774 → 0 → 86 → 130\n3535\n\n\nC-LOOK\n143 → 913 → 948 → 1022 → 1470 → 1509 → 1750 → 1774 → 86 → 130\n3363",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Blatt 07</span>"
    ]
  },
  {
    "objectID": "sh07/07.html#aufgabe-8",
    "href": "sh07/07.html#aufgabe-8",
    "title": "7  Blatt 07",
    "section": "Aufgabe 8",
    "text": "Aufgabe 8\nEin RAID 0-System verteilt die Daten ohne Redundanz auf zwei Platten. Fällt auch nur eine Platte aus, ist das gesamte System unbrauchbar. Die Wahrscheinlichkeit, dass beide Platten überleben, beträgt 0,9 × 0,9 = 0,81. Daraus ergibt sich eine Ausfallwahrscheinlichkeit von 1 – 0,81 = 0,19, also 19 %.\nEin RAID 1-System spiegelt die Daten auf zwei Platten. Es bleibt funktionsfähig, solange mindestens eine Platte intakt ist. Nur wenn beide gleichzeitig ausfallen (0,1 × 0,1 = 0,01), kommt es zum Datenverlust. Die Ausfallwahrscheinlichkeit liegt also bei 1 %.\nEine weitere Reduktion der Ausfallwahrscheinlichkeit ist nur bei RAID 1 sinnvoll möglich. Dies gelingt z. B. durch den Einsatz von mehr als zwei Festplatten: Bei drei Platten beträgt die Wahrscheinlichkeit, dass alle gleichzeitig ausfallen, nur 0,1³ = 0,001. Auch sogenannte Hot-Spare-Platten können automatisch einspringen, wenn eine Platte ausfällt.\nBei RAID 0 hingegen erhöht jede zusätzliche Platte sogar das Risiko, da das System schon beim Ausfall einer einzigen Platte versagt. Eine Verbesserung der Sicherheit ist daher hier nicht möglich.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Blatt 07</span>"
    ]
  },
  {
    "objectID": "sh07/07.html#aufgabe-9",
    "href": "sh07/07.html#aufgabe-9",
    "title": "7  Blatt 07",
    "section": "Aufgabe 9",
    "text": "Aufgabe 9\nJa, in bestimmten Szenarien kann ein RAID 1-System eine höhere Leseleistung erreichen als RAID 0. Da die Daten auf mehreren Platten identisch vorliegen, kann der Controller parallele Lesezugriffe auf verschiedene Platten verteilen oder jeweils die am schnellsten zugängliche Platte nutzen.\nDies bringt Vorteile bei vielen gleichzeitigen, verteilten Lesezugriffen – etwa bei Datenbankanfragen oder Webservern. Vorausgesetzt ist, dass der Controller intelligentes Load Balancing unterstützt.\nBei rein sequentiellem Lesen großer Dateien ist RAID 0 meist schneller, da Striping die Datenrate erhöht.\n\\(\\Rightarrow\\) RAID 1 kann bei paralleler Last schneller sein, RAID 0 eher bei sequenziellen Zugriffen.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Blatt 07</span>"
    ]
  },
  {
    "objectID": "sh08/08.html",
    "href": "sh08/08.html",
    "title": "8  Blatt 8",
    "section": "",
    "text": "Aufgabe 1",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Blatt 8</span>"
    ]
  },
  {
    "objectID": "sh08/08.html#aufgabe-1",
    "href": "sh08/08.html#aufgabe-1",
    "title": "8  Blatt 8",
    "section": "",
    "text": "Allgemeiner Beweis: Es kann kein Deadlock auftreten\nGegeben sind:\n\nEin Ressourcentyp mit insgesamt \\(E = 4\\) Instanzen,\nDrei Prozesse \\(P_1, P_2, P_3\\),\nJeder Prozess kann maximal 2 Instanzen der Ressource anfordern.\n\nSei \\(C_i\\) die Anzahl der aktuell einem Prozess \\(P_i\\) zugeteilten Ressourceninstanzen, und \\(R_i\\) der verbleibende Bedarf, so dass gilt:\n\\[\nC_i + R_i \\leq 2 \\quad \\text{für alle } i = 1, 2, 3\n\\]\nund\n\\[\n\\sum_{i=1}^{3} C_i \\leq 4\n\\]\nEs soll gezeigt werden, dass unter diesen Bedingungen kein Deadlock entstehen kann.\nFall 1: Mindestens ein Prozess wartet nicht Falls für ein \\(i\\) gilt: \\(R_i = 0\\), dann kann \\(P_i\\) sofort fertigstellen und bis zu 2 Ressourcen freigeben. Dadurch erhöht sich die Anzahl der verfügbaren Instanzen \\(A\\), was anderen wartenden Prozessen die Fortsetzung erlaubt. Durch Induktion führt dies zu einer vollständigen Abarbeitung aller Prozesse. Ein Deadlock ist somit ausgeschlossen.\nFall 2: Alle Prozesse warten auf Ressourcen Angenommen, jeder Prozess hält mindestens eine Instanz und fordert eine weitere an, also:\n\\[\nC = \\begin{bmatrix} 1 \\\\ 1 \\\\ 1 \\end{bmatrix}, \\quad\nR = \\begin{bmatrix} 1 \\\\ 1 \\\\ 1 \\end{bmatrix}, \\quad\nA = \\begin{bmatrix} 1 \\end{bmatrix}\n\\]\nDies ist die kritischste zulässige Konfiguration: Alle 4 Ressourcen sind verteilt (3 belegt, 1 frei), und jeder Prozess wartet auf eine weitere Instanz.\nIn diesem Fall kann das System dennoch einer Anfrage (z. B. von \\(P_1\\)) nachkommen, sodass dieser Prozess abschließen kann. Danach gibt er seine 2 gehaltenen Ressourcen frei, und \\(A\\) steigt auf 3. Damit können die verbleibenden Prozesse nacheinander ebenfalls fertigstellen. Es kommt also auch im schlimmsten Fall nicht zu einem Deadlock.\nSchlussfolgerung In allen gültigen Konfigurationen – ob mindestens ein Prozess direkt fertigstellen kann oder alle warten – hat das System stets genug Ressourcen, um zumindest einem Prozess die Beendigung zu ermöglichen. Dadurch ist Fortschritt immer möglich, und eine zirkuläre Wartebedingung kann nie entstehen. Es folgt: Ein Deadlock ist unter den gegebenen Voraussetzungen ausgeschlossen.\nAufgabe 1b – Beispiel mit erfüllten Deadlock-Bedingungen, aber ohne Deadlock\nDieses Beispiel stammt direkt aus den Vorlesungsfolien und illustriert eine Systemkonfiguration, in der alle vier notwendigen Coffman-Bedingungen für einen Deadlock erfüllt sind, jedoch kein Deadlock vorliegt. Der Bankieralgorithmus bestätigt, dass der Zustand sicher ist.\nSystemdefinition\nRessourcentypen: \\(R_1\\) und \\(R_2\\) Gesamtressourcenvektor: \\(E = (2, 2)\\) Verfügbarkeitsvektor: \\(A = (0, 0)\\)\nAlle Ressourcen sind momentan vollständig vergeben.\nZuteilungsmatrix \\(C\\):\n\\[\nC = \\begin{bmatrix}\n0 & 1 \\\\\n1 & 0 \\\\\n1 & 0 \\\\\n0 & 1\n\\end{bmatrix}\n\\]\n\\(P_1\\) hält \\(R_2\\), \\(P_2\\) und \\(P_3\\) halten jeweils \\(R_1\\), \\(P_4\\) hält \\(R_2\\).\nAnforderungsmatrix \\(R\\):\n\\[\nR = \\begin{bmatrix}\n1 & 0 \\\\\n0 & 0 \\\\\n0 & 1 \\\\\n0 & 0\n\\end{bmatrix}\n\\]\n\\(P_1\\) fordert \\(R_1\\), \\(P_3\\) fordert \\(R_2\\).\nÜberprüfung der Coffman-Bedingungen\n1. Wechselseitiger Ausschluss: Ressourcen können nicht gemeinsam genutzt werden. 2. Hold-and-Wait: Prozesse halten Ressourcen und fordern weitere an (z. B. \\(P_1\\) hält \\(R_2\\), fordert \\(R_1\\)). 3. Keine Präemption: Ressourcen können Prozessen nicht zwangsweise entzogen werden. 4. Zyklisches Warten: Es existiert ein Zyklus im Ressourcen-Wartegraphen: \\(P_1 \\rightarrow R_1 \\rightarrow P_3 \\rightarrow R_2 \\rightarrow P_1\\).\n→ Alle vier Bedingungen sind erfüllt.\nAnwendung des Bankieralgorithmus\n\n\n\n\n\n\n\n\n\n\n\nSchritt\nProzess \\(i\\)\n\\(R_i\\)\nVerfügbar \\(A\\)\nBelegt \\(C_i\\)\nMarkierte Prozesse (P1, P2, P3, P4)\n\n\n\n\n1\n1\n(1, 0)\n(0, 0)\n(0, 1)\n× × × ×\n\n\n2\n2\n(0, 0)\n(0, 0)\n(1, 0)\n× ✓ × ×\n\n\n3\n3\n(0, 1)\n(1, 0)\n(1, 0)\n× ✓ × ×\n\n\n4\n4\n(0, 0)\n(1, 0)\n(0, 1)\n× ✓ × ✓\n\n\n5\n1\n(1, 0)\n(1, 1)\n(0, 1)\n✓ ✓ × ✓\n\n\n6\n3\n(0, 1)\n(1, 2)\n(1, 0)\n✓ ✓ ✓ ✓\n\n\n\nNach jedem Schritt wird der Verfügbarkeitsvektor \\(A\\) aktualisiert, sobald ein markierter Prozess seine gehaltenen Ressourcen freigibt. Am Ende sind alle Prozesse markiert → der Zustand ist sicher.\nSchlussfolgerung\nTrotz erfüllter Coffman-Bedingungen und zyklischer Abhängigkeiten liegt kein Deadlock vor, da der Bankieralgorithmus eine vollständige Abarbeitungsreihenfolge findet. Dieses Beispiel erfüllt somit exakt die Aufgabenstellung: Ein System mit potenzieller Deadlock-Struktur, das sich jedoch als sicher herausstellt.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Blatt 8</span>"
    ]
  },
  {
    "objectID": "sh08/08.html#aufgabe-2",
    "href": "sh08/08.html#aufgabe-2",
    "title": "8  Blatt 8",
    "section": "Aufgabe 2",
    "text": "Aufgabe 2\n\nBeschreiben Sie ein Szenario, in dem ein Deadlock entstehen kann:\nEin Deadlock kann entstehen, wenn die beiden Prozesse P1 und P2 die Ressourcen A und B in entgegengesetzter Reihenfolge anfordern, und beide jeweils bereits eine Ressource belegt haben, während sie auf die zweite warten.\nAblauf eines möglichen Szenarios:\n\nP1 startet zuerst und führt wait(sem_A) aus → Ressource A wird gesperrt.\nP2 startet kurz danach und führt wait(sem_B) aus → Ressource B wird gesperrt.\nNun versucht P1, wait(sem_B) auszuführen, blockiert aber, da B bereits durch P2 gesperrt ist.\nGleichzeitig versucht P2, wait(sem_A) auszuführen, blockiert aber, da A durch P1 gehalten wird.\n\n\nBeide Prozesse warten nun jeweils auf eine Ressource, die vom jeweils anderen gehalten wird.\nKeiner kann weitermachen oder Ressourcen freigeben.\nDies ist ein klassischer Deadlock-Zustand.\n\nDieses Szenario entspricht genau dem Beispiel auf Folie 15 der Vorlesung 15, bei dem zwei Prozesse in dieser Weise mit wait(A)/wait(B) agieren. Der Deadlock tritt bei einer ungünstigen Ausführungsreihenfolge auf.\nErklären Sie präzise, welche der notwendigen Bedingungen für einen Deadlock hier erfüllt sind.\nIn der dargestellten Situation (siehe Teil a) sind alle vier Coffman-Bedingungen erfüllt, die gemeinsam notwendig für das Entstehen eines Deadlocks sind:\n\nWechselseitiger Ausschluss (Mutual Exclusion) Beide Ressourcen (A und B) sind durch binäre Semaphore geschützt, d. h. sie können immer nur von einem Prozess gleichzeitig verwendet werden.\nHold-and-Wait (Belegen und Warten) Beide Prozesse belegen eine Ressource (z. B. P1 hält A, P2 hält B) und fordern gleichzeitig eine weitere Ressource an, auf die sie warten.\nKeine Präemption (No Preemption) Ressourcen können nicht gewaltsam entzogen werden. Ein Prozess gibt eine Ressource nur durch ein signal() freiwillig frei — aber das geschieht erst nach der kritischen Sektion, also nicht während des Wartens.\nZyklische Wartebedingung (Circular Wait) Es entsteht ein Zyklus im Ressourcen-Wartegraphen:\n\nP1 hält A und wartet auf B,\nP2 hält B und wartet auf A. → Zyklus: \\(P1 \\rightarrow B \\rightarrow P2 \\rightarrow A \\rightarrow P1\\)\n\n\n\\(\\Rightarrow\\) Alle vier notwendigen Bedingungen für einen Ressourcen-Deadlock sind in diesem Szenario gleichzeitig gegeben. Das erklärt, warum hier ein Deadlock möglich ist, wenn die Prozesse in genau dieser Reihenfolge ausgeführt werden.\nSchlagen Sie eine einfache konzeptionelle Änderung vor, die das Risiko eines Deadlocks in diesem Beispiel beseitigt.\n\nEine einfache, aber sehr effektive konzeptionelle Änderung besteht darin, eine einheitliche Reihenfolge für die Ressourcenzugriffe festzulegen.\nKonkret: Beide Prozesse sollen die Ressourcen immer in derselben Reihenfolge anfordern — zum Beispiel:\nwait(sem_A);\nwait(sem_B);\n// kritische Sektion\nsignal(sem_B);\nsignal(sem_A);\nSowohl P1 als auch P2 würden dann zuerst sem_A und danach sem_B anfordern.\nDurch die feste Anforderungsreihenfolge wird die zyklische Wartebedingung (circular wait) prinzipiell ausgeschlossen. Denn:\n\nWenn ein Prozess bereits sem_A belegt, ein anderer Prozess aber auf sem_B wartet, wird dieser niemals gleichzeitig sem_A anfordern, da er sem_A ja zuerst anfordern müsste.\nSomit kann kein zyklisches Warten entstehen.\n\n\\(\\Rightarrow\\) Die Einführung einer globalen, konsistenten Reihenfolge beim Zugriff auf mehrere Ressourcen ist eine einfache, konzeptionelle Methode zur Vermeidung von Deadlocks. Sie unterläuft gezielt eine der vier Deadlock-Bedingungen — nämlich die zyklische Wartebedingung.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Blatt 8</span>"
    ]
  },
  {
    "objectID": "sh08/08.html#aufgabe-4",
    "href": "sh08/08.html#aufgabe-4",
    "title": "8  Blatt 8",
    "section": "Aufgabe 4",
    "text": "Aufgabe 4\n\nRAG:\n\n\n\nRAG\n\n\nE, A, R, und C:\n\nGesamtressourcenvektor: \\(E = (2, 3, 1)\\)\nVerfügbarkeitsvektor (initial): \\(A = (0, 0, 1)\\)\nBelegungsmatrix \\(C\\):\n\n\\[\nC = \\begin{bmatrix}\n0 & 1 & 0 \\\\\n1 & 1 & 0 \\\\\n1 & 1 & 0\n\\end{bmatrix}\n\\]\n\nAnforderungsmatrix \\(R\\):\n\n\\[\nR = \\begin{bmatrix}\n1 & 0 & 0 \\\\\n0 & 1 & 1 \\\\\n0 & 0 & 1\n\\end{bmatrix}\n\\]\nDurchführung des Bankieralgorithmus\nAusführungstabelle des Bankieralgorithmus:\n\n\n\n\n\n\n\n\n\n\n\n\nSchritt\nProzess \\(i\\)\n\\(R_i\\)\n\\(A\\)\n\\(C_i\\)\nAnfrage erfüllbar?\nMarkiert\n\n\n\n\n1\n1\n(1, 0, 0)\n(0, 0, 1)\n(0, 1, 0)\nNein\n× × ×\n\n\n2\n2\n(0, 1, 1)\n(0, 0, 1)\n(1, 1, 0)\nNein\n× × ×\n\n\n3\n3\n(0, 0, 1)\n(0, 0, 1)\n(1, 1, 0)\nJa\n× × ✓\n\n\n4\n1\n(1, 0, 0)\n(1, 1, 1)\n(0, 1, 0)\nJa\n✓ × ✓\n\n\n5\n2\n(0, 1, 1)\n(1, 2, 1)\n(1, 1, 0)\nJa\n✓ ✓ ✓\n\n\n\nAlle Prozesse konnten schrittweise markiert werden. Es existiert also eine vollständige sichere Ausführungsreihenfolge.\n\\(\\Rightarrow\\) Es liegt kein Deadlock vor.\nErklärung der Tabelle: Die folgende Tabelle zeigt Schritt für Schritt, wie der Bankieralgorithmus arbeitet:\n\nIn jeder Zeile wird ein Prozess betrachtet.\nWenn seine Ressourcenanforderung \\(R_i\\) durch den aktuellen Verfügbarkeitsvektor \\(A\\) erfüllbar ist, wird der Prozess markiert.\nAnschließend wird seine Ressourcenbelegung \\(C_i\\) zu \\(A\\) addiert (Ressourcenfreigabe nach Terminierung).\nDie nächste Zeile prüft erneut alle Prozesse, bis keine unmarkierten Prozesse mehr übrig sind.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Blatt 8</span>"
    ]
  },
  {
    "objectID": "sh08/08.html#aufgabe-5",
    "href": "sh08/08.html#aufgabe-5",
    "title": "8  Blatt 8",
    "section": "Aufgabe 5",
    "text": "Aufgabe 5\nCPU-bound (rechenintensive) Prozesse führen lange Rechenoperationen mit wenigen I/O-Unterbrechungen aus. Sie profitieren von längerer CPU-Zuteilung ohne häufiges Umschalten, da Kontextwechsel mit Overhead verbunden sind (z. B. Speichern des Zustands, Leeren von Caches wie dem TLB).\nI/O-bound (ein-/ausgabeintensive) Prozesse führen dagegen viele kurze Rechenphasen aus, gefolgt von häufigem Warten auf I/O-Geräte. Würde man diese Prozesse zu lange auf der CPU halten, würde man wertvolle CPU-Zeit verschwenden, da sie häufig blockieren. Sie profitieren daher von kurzen, aber schnellen Reaktionszeiten, sodass sie nach Beendigung ihrer I/O-Phasen zügig wieder eingeplant werden können.\nEin intelligenter Scheduler erkennt diese Unterschiede und trifft darauf basierende Entscheidungen, um die Systemauslastung zu optimieren und sowohl Rechenzeit effizient zu verteilen als auch I/O-Wartezeiten zu minimieren.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Blatt 8</span>"
    ]
  },
  {
    "objectID": "sh08/08.html#aufgabe-6",
    "href": "sh08/08.html#aufgabe-6",
    "title": "8  Blatt 8",
    "section": "Aufgabe 6",
    "text": "Aufgabe 6\n\nGantt-Diagramme für verschiedene Scheduling-Strategien**\nGegeben sind fünf Prozesse mit folgenden CPU-Burst-Zeiten (in Millisekunden):\n\n\n\nProzess\nBurst-Zeit\n\n\n\n\nP1\n10\n\n\nP2\n1\n\n\nP3\n2\n\n\nP4\n1\n\n\nP5\n5\n\n\n\n\nFCFS (First-Come, First-Served)** Prozesse werden in ihrer Ankunftsreihenfolge vollständig abgearbeitet:\n| P1         | P2 | P3  | P4 | P5     |\n0           10    11    13   14       19\n\nSJF (Shortest Job First)** Prozesse werden nach aufsteigender Burst-Zeit sortiert und dann nacheinander ausgeführt:\n| P2 | P4 | P3  | P5     | P1          |\n0    1    2     4        9             19\nRR (Round-Robin), Quantum = 1 ms** Prozesse werden in Zeitscheiben von 1 ms ausgeführt und nach jedem Quantum ans Ende der Queue gesetzt. Ergebnis bis Zeit 16 ms:\n| P1 | P2 | P3 | P4 | P5 | P1 | P3 | P5 |\n0    1    2    3    4    5    6    7    8\n\n| P1 | P5 | P1 | P5 | P1 | P1 | P1 | P1 |\n8    9   10   11   12   13   14   15   16\n\n| P1 | P1 | P1 |\n16   17   18   19\n\nDurchlaufzeiten:\nDie Durchlaufzeit eines Prozesses ist definiert als:\n\\[\n\\text{Durchlaufzeit} = \\text{Fertigstellungszeit} - \\text{Ankunftszeit}\n\\]\nDa alle Prozesse bei \\(t = 0\\) starten, entspricht die Durchlaufzeit hier einfach der Zeit der letzten Ausführung eines Prozesses im jeweiligen Gantt-Diagramm.\n\nFCFS (First-Come, First-Served)\nGantt-Diagramm:\n| P1         | P2 | P3  | P4 | P5     |\n0           10  11  13  14  19\n\n\n\nProzess\nDurchlaufzeit\n\n\n\n\n\\(P_1\\)\n10\n\n\n\\(P_2\\)\n11\n\n\n\\(P_3\\)\n13\n\n\n\\(P_4\\)\n14\n\n\n\\(P_5\\)\n19\n\n\n\nSJF (Shortest Job First)**\nGantt-Diagramm:\n| P2 | P4 | P3  | P5     | P1          |\n0    1    2     4        9           19\n\n\n\nProzess\nDurchlaufzeit\n\n\n\n\n\\(P_1\\)\n19\n\n\n\\(P_2\\)\n1\n\n\n\\(P_3\\)\n4\n\n\n\\(P_4\\)\n2\n\n\n\\(P_5\\)\n9\n\n\n\nRR (Round-Robin), Quantum = 1 ms**\nGantt-Diagramm:\n| P1 | P2 | P3 | P4 | P5 | P1 | P3 | P5 | P1 | P5 | P1 | P5 | P1 | P1 | P1 | P1 | P1 | P1 | P1 |\n0    1    2    3    4    5    6    7    8    9   10   11   12   13   14   15   16   17   18  19\nFertigstellungszeitpunkte (letzte Ausführung pro Prozess):\n\n\\(P_2\\): endet bei t = 2\n\\(P_4\\): endet bei t = 4\n\\(P_3\\): endet bei t = 7\n\\(P_5\\): endet bei t = 14\n\\(P_1\\): endet bei t = 19\n\n\n\n\nProzess\nDurchlaufzeit\n\n\n\n\n\\(P_1\\)\n19\n\n\n\\(P_2\\)\n2\n\n\n\\(P_3\\)\n7\n\n\n\\(P_4\\)\n4\n\n\n\\(P_5\\)\n14\n\n\n\n\nVergleich der Durchlaufzeiten:\n\n\n\nProzess\nFCFS\nSJF\nRR (Q = 1 ms)\n\n\n\n\n\\(P_1\\)\n10\n19\n19\n\n\n\\(P_2\\)\n11\n1\n2\n\n\n\\(P_3\\)\n13\n4\n7\n\n\n\\(P_4\\)\n14\n2\n4\n\n\n\\(P_5\\)\n19\n9\n14\n\n\n\nWartezeit (Waiting Time)\n\nDie Wartezeit berechnet sich aus:\n\\[\n\\text{Wartezeit} = \\text{Durchlaufzeit} - \\text{Burst-Zeit}\n\\]\nDie Burst-Zeiten (CPU-Bedarf) der Prozesse sind:\n\n\n\nProzess\nBurst-Zeit\n\n\n\n\n\\(P_1\\)\n10\n\n\n\\(P_2\\)\n1\n\n\n\\(P_3\\)\n2\n\n\n\\(P_4\\)\n1\n\n\n\\(P_5\\)\n5\n\n\n\nBerechnung der Wartezeiten je Strategie\n\nFCFS\n\n\n\nProzess\nDurchlaufzeit\nBurst\nWartezeit\n\n\n\n\n\\(P_1\\)\n10\n10\n0\n\n\n\\(P_2\\)\n11\n1\n10\n\n\n\\(P_3\\)\n13\n2\n11\n\n\n\\(P_4\\)\n14\n1\n13\n\n\n\\(P_5\\)\n19\n5\n14\n\n\n\nSJF\n\n\n\nProzess\nDurchlaufzeit\nBurst\nWartezeit\n\n\n\n\n\\(P_1\\)\n19\n10\n9\n\n\n\\(P_2\\)\n1\n1\n0\n\n\n\\(P_3\\)\n4\n2\n2\n\n\n\\(P_4\\)\n2\n1\n1\n\n\n\\(P_5\\)\n9\n5\n4\n\n\n\nRR (Q = 1 ms)\n\n\n\nProzess\nDurchlaufzeit\nBurst\nWartezeit\n\n\n\n\n\\(P_1\\)\n19\n10\n9\n\n\n\\(P_2\\)\n2\n1\n1\n\n\n\\(P_3\\)\n7\n2\n5\n\n\n\\(P_4\\)\n4\n1\n3\n\n\n\\(P_5\\)\n14\n5\n9\n\n\n\nDurchschnittliche Wartezeiten:\n\nFCFS:\n\nWartezeiten: 0, 10, 11, 13, 14\n\\[\n\\text{Summe} = 48,\\quad \\text{Durchschnitt} = \\frac{48}{5} = 9{,}6\\ \\text{ms}\n\\]\n\nSJF:\n\nWartezeiten: 9, 0, 2, 1, 4\n\\[\n\\text{Summe} = 16,\\quad \\text{Durchschnitt} = \\frac{16}{5} = 3{,}2\\ \\text{ms}\n\\]\n\nRR (Quantum = 1 ms):\n\nWartezeiten: 9, 1, 5, 3, 9\n\\[\n\\text{Summe} = 27,\\quad \\text{Durchschnitt} = \\frac{27}{5} = 5{,}4\\ \\text{ms}\n\\]\nVergleich – Durchschnittliche Wartezeit\n\n\n\nStrategie\nDurchschnittliche Wartezeit\n\n\n\n\nFCFS\n9,6 ms\n\n\nSJF\n3,2 ms\n\n\nRR (Q=1)\n5,4 ms\n\n\n\n\\(\\Rightarrow\\)\nSJF liefert die geringste durchschnittliche Wartezeit, da kurze Prozesse bevorzugt behandelt werden. Allerdings kann SJF zu Verhungern (Starvation) führen, wenn lange Prozesse ständig verdrängt werden. Round-Robin bietet bessere Fairness, aber höhere Gesamtlatenz.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Blatt 8</span>"
    ]
  },
  {
    "objectID": "sh08/08.html#aufgabe-7",
    "href": "sh08/08.html#aufgabe-7",
    "title": "8  Blatt 8",
    "section": "Aufgabe 7",
    "text": "Aufgabe 7\n\nFCFS (First-Come, First-Served): Keine Gefahr von Verhungern, da Prozesse strikt in Ankunftsreihenfolge ausgeführt werden.\nSJF (Shortest Job First): Gefahr von Verhungern besteht, da lange Prozesse durch immer neu ankommende kurze Prozesse dauerhaft verdrängt werden können.\nSRTF (Shortest Remaining Time First): Ebenfalls anfällig für Verhungern, insbesondere für Prozesse mit langer verbleibender Laufzeit, da kürzere Prozesse bevorzugt werden.\nRR (Round-Robin): Keine Gefahr von Verhungern, da alle Prozesse regelmäßig CPU-Zeit erhalten.\nMultilevel Queue mit fixer Priorität: Hohe Verhungernsgefahr für Prozesse in unteren Prioritätsklassen, wenn ständig Prozesse mit höherer Priorität ankommen.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Blatt 8</span>"
    ]
  }
]