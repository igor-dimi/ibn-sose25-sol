[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Operating Systems and Networs SoSe 25 Solutions",
    "section": "",
    "text": "Preface",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "sh01/01.html",
    "href": "sh01/01.html",
    "title": "1  Blatt 01",
    "section": "",
    "text": "Aufgabe 1\nLearning how to Learn:\nJohn Cleese:",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Blatt 01</span>"
    ]
  },
  {
    "objectID": "sh01/01.html#aufgabe-1",
    "href": "sh01/01.html#aufgabe-1",
    "title": "1  Blatt 01",
    "section": "",
    "text": "Zwei Denkmodi aus „Learning How to Learn“\n\nFokussierter Modus: Zielgerichtetes, konzentriertes Denken. Gut für bekannte Aufgaben und Übung.\nDiffuser Modus: Entspanntes, offenes Denken. Hilft bei neuen Ideen und kreativen Verknüpfungen.\n\nAufgaben und passende Denkmodi\n\nFokussierter Modus\nWarum: Erfordert Konzentration und gezieltes Einprägen.\nZuerst diffuser, dann fokussierter Modus\nWarum: Erst Überblick und Verständnis aufbauen, dann vertiefen.\n\nFokussierter Modus\nWarum: Klare, schrittweise Übung – ideal für fokussiertes Denken.\nBeide Modi\nWarum: Fokussiert für Details & Übungen, diffus für Überblick & Vernetzung.\n\n\n\n\nZwei Denkmodi:\n\nOffener Modus: Locker, spielerisch, kreativ.\nBeispiel: Ideen für eine Geschichte sammeln.\nWarum: Offenheit fördert neue Einfälle.\n\nGeschlossener Modus: Zielgerichtet, angespannt, entscheidungsfreudig.\nBeispiel: Bericht überarbeiten und fertigstellen.\nWarum: Präzises Arbeiten und klare Entscheidungen nötig.\n\n\nVergleich mit „Learning How to Learn“\n\nOffen \\(\\Leftrightarrow\\) Diffus: Für Kreativität und Überblick.\n\nGeschlossen \\(\\Leftrightarrow\\) Fokussiert: Für Detailarbeit und Umsetzung.\n\nAlexander Fleming:\n\nModus: Offen\nWarum: Fleming entdeckte Penicillin zufällig, weil er offen und entspannt war – neugierig statt zielgerichtet. Im geschlossenen Modus hätte er die verschimmelte Petrischale wohl einfach weggeschmissen – zu fokussiert für zufällige Entdeckungen.\n\nAlfred Hitchcock:\n\nModus: Offen\n\nWie: Er erzählte lustige Anekdoten, um das Team zum Lachen zu bringen – so schuf er eine entspannte Atmosphäre, die kreatives Denken förderte.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Blatt 01</span>"
    ]
  },
  {
    "objectID": "sh01/01.html#aufgabe-2",
    "href": "sh01/01.html#aufgabe-2",
    "title": "1  Blatt 01",
    "section": "Aufgabe 2",
    "text": "Aufgabe 2\n\n\nx64: 16 64 Bit GPRs1 \\(\\Rightarrow\\) 16 x 64 b = 16 x 8 B = \\(2^7\\) B.\nAVX2: 16 256 Bit GPRs2 \\(\\Rightarrow\\) 16 x 256 b = 16 x 32 B = \\(2^9\\) B\n\n\nx64: \\(\\frac{2^7}{2^{30}} = \\frac{1}{2^{23}}\\)\nAVX2: \\(\\frac{2^9}{2^{30}} = \\frac{1}{2^{21}}\\)\n\nallgemein gilt: \\(10^3 \\approx 2^{10}\\), und \\(\\frac{2^x}{2^y} = \\frac{1}{2^{y-x}}\\)",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Blatt 01</span>"
    ]
  },
  {
    "objectID": "sh01/01.html#aufgabe-3",
    "href": "sh01/01.html#aufgabe-3",
    "title": "1  Blatt 01",
    "section": "Aufgabe 3",
    "text": "Aufgabe 3\n\nDer Zugriff scheitert, weil der Arbeitsspeicher durch die Memory Protection (z. B. Paging mit Zugriffsrechten) vom Betriebssystem isoliert wird. Nur der Kernel darf die Speicherbereiche aller Prozesse sehen und verwalten.\nEin Prozess kann trotzdem auf Ressourcen anderer Prozesse zugreifen über kontrollierte Schnittstellen wie IPC (Inter-Process Communication), Dateisysteme, Sockets oder Shared Memory, die vom Betriebssystem verwaltet und überwacht werden.\nWelche Risiken entstehen bei höchstem Privileg für alle Prozesse?\n\nSicherheitslücken: Jeder Prozess könnte beliebige Speicherbereiche lesen/schreiben.\n\nStabilitätsprobleme: Fehlerhafte Prozesse könnten das System zum Absturz bringen.\n\nKeine Isolation: Malware hätte vollen Systemzugriff, keine Schutzmechanismen.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Blatt 01</span>"
    ]
  },
  {
    "objectID": "sh01/01.html#aufgabe-4",
    "href": "sh01/01.html#aufgabe-4",
    "title": "1  Blatt 01",
    "section": "Aufgabe 4",
    "text": "Aufgabe 4\nKernel-Code benötigt einen sicheren, kontrollierten Speicherbereich (seinen eigenen Stack), um zu vermeiden:\n\nBeschädigung durch Benutzerprozesse\nAbstürze oder Rechteausweitung (Privilege Escalation)\n\nDaher hat jeder Prozess:\n\nEinen User-Mode-Stack (wird bei normaler Ausführung verwendet)\nEinen Kernel-Mode-Stack (wird bei System Calls und Interrupts verwendet)",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Blatt 01</span>"
    ]
  },
  {
    "objectID": "sh01/01.html#aufgabe-5",
    "href": "sh01/01.html#aufgabe-5",
    "title": "1  Blatt 01",
    "section": "Aufgabe 5",
    "text": "Aufgabe 5\nEntfernte Systemaufrufe\n\n\n\n\n\n\n\nSystemaufruf\nGrund für Entfernung\n\n\n\n\ncreat\nEntspricht vollständig open(path, O_CREAT | O_WRONLY | O_TRUNC, mode).\n\n\ndup\nEntspricht vollständig fcntl(fd, F_DUPFD, 0).\n\n\n\nAlle übrigen Systemaufrufe bieten essenzielle Funktionen, die nicht exakt durch andere ersetzt werden können.\nSie decken ab:\n\nDatei- und Verzeichnisoperationen (open, read, write, unlink, mkdir, etc.)\nProzessmanagement (fork, exec, wait, exit, etc.)\nMetadatenverwaltung (chmod, chown, utime, etc.)\nKommunikation und Steuerung (pipe, kill, ioctl, etc.)\nZeit- und Systemabfragen (time, times, stat, etc.)\n\nOhne sie wären bestimmte Kernfunktionen unmöglich.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Blatt 01</span>"
    ]
  },
  {
    "objectID": "sh01/01.html#aufgabe-6",
    "href": "sh01/01.html#aufgabe-6",
    "title": "1  Blatt 01",
    "section": "Aufgabe 6",
    "text": "Aufgabe 6\nscript.sh auch im Zip:\ncd $1\nwhile :\ndo\n    echo \"5 biggest files in $1:\"\n    ls -S | head -5\n    echo \"5 last modified files starting with '$2' in $1:\"\n    ls -t | grep ^$2 | head -5\n    sleep 5\ndone",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Blatt 01</span>"
    ]
  },
  {
    "objectID": "sh01/01.html#aufgabe-7",
    "href": "sh01/01.html#aufgabe-7",
    "title": "1  Blatt 01",
    "section": "Aufgabe 7",
    "text": "Aufgabe 7\nVorteile:\n\nKomplexitätsreduktion: Abstraktionen verbergen technische Details und erleichtern das Entwickeln und Verstehen von Systemen.\n\nWiederverwendbarkeit: Einmal geschaffene Abstraktionen (z.B. Dateisystem, Prozesse) können flexibel in verschiedenen Programmen genutzt werden.\n\nNachteile:\n\nLeistungsaufwand: Abstraktionsschichten können zusätzliche Rechenzeit und Speicherverbrauch verursachen.\n\nFehlerverdeckung: Probleme in tieferen Schichten bleiben oft verborgen und erschweren Fehlersuche und Optimierung.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Blatt 01</span>"
    ]
  },
  {
    "objectID": "sh01/01.html#footnotes",
    "href": "sh01/01.html#footnotes",
    "title": "1  Blatt 01",
    "section": "",
    "text": "https://www.wikiwand.com/en/articles/X86-64↩︎\nhttps://www.wikiwand.com/en/articles/Advanced_Vector_Extensions↩︎",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Blatt 01</span>"
    ]
  },
  {
    "objectID": "sh02/02.html",
    "href": "sh02/02.html",
    "title": "2  Blatt 02",
    "section": "",
    "text": "Aufgabe 1\nDie Datenstruktur task_struct ist im Linux-Kernel-Quellcode (Linux kernel Version 6.15.0) definiert unter:\ninclude/linux/sched.h\nDie Definition erstreckt sich über die Zeilen 813 bis 1664.\nDarin befinden sich etwa 320 Member-Variablen.\nBei einer Annahme von 8 Byte pro Variable ergibt sich eine geschätzte Größe von:\n2.560 Byte \\(\\approx\\) 2,5 KB",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Blatt 02</span>"
    ]
  },
  {
    "objectID": "sh02/02.html#aufgabe-2",
    "href": "sh02/02.html#aufgabe-2",
    "title": "2  Blatt 02",
    "section": "Aufgabe 2",
    "text": "Aufgabe 2\nDer Systemaufruf fork() erzeugt einen neuen Prozess, der eine Kopie des aufrufenden Prozesses ist (Kindprozess).\nRückgabewert:\n\n0 im Kindprozess\n\nPID des Kindes im Elternprozess\n\n−1 bei Fehler\n\n\nMit dem program:\n#include &lt;stdio.h&gt;\n\nint main(int argc, char const *argv[])\n{\n    int i = 0;\n    if (fork() != 0) i++;\n    if (i != 1) fork();\n    fork();\n    return 0;\n}\nwerden insgesammt 6 Prozesse erzeugt. Graph der enstehenden Prozess hierarchie:\nP1  \n├── P1.1  \n│   └── P1.1.1  \n│       └── P1.1.1.1  \n│   └── P1.1.2  \n└── P1.2  \nSchrittweise Erzeugung der Prozesse:\n\nP1 startet das Programm. Der Wert von i ist anfangs 0.\nDie erste fork()-Anweisung wird ausgeführt:\n\nP1 ist der Elternprozess, der einen neuen Kindprozess P1.1 erzeugt.\nIm Elternprozess (P1) ist das Rückgabewert von fork() ≠ 0 → i wird auf 1 gesetzt.\nIm Kindprozess (P1.1) ist das Rückgabewert 0 → i bleibt 0.\n\nDanach folgt die Bedingung if (i != 1) fork();:\n\nP1 hat i == 1 → keine Aktion.\nP1.1 hat i == 0 → führt eine fork() aus → erzeugt P1.1.1.\n\nSchließlich wird eine letzte fork(); von allen existierenden Prozessen ausgeführt:\n\nP1 erzeugt P1.2\nP1.1 erzeugt P1.1.2\nP1.1.1 erzeugt P1.1.1.1\n\n\n\nDas Programm führt fork() aus, bis ein Kindprozess mit einer durch 10 teilbaren PID entsteht. Jeder fork() erzeugt ein Kind, das sofort endet (die Rückgabe von fork() is 0 bei einem Kind), außer die Bedingung ist erfüllt. Da etwa jede zehnte PID durch 10 teilbar ist, liegt die maximale Prozessanzahl (inkl. Elternprozess) typischerweise bei etwa 11.\nDa PIDs vom Kernel in aufsteigender Reihenfolge als nächste freie Zahl vergeben werden, ist garantiert, dass früher oder später eine durch 10 teilbare PID erzeugt wird. Das Programm terminiert daher immer. Wären PIDs zufällig, könnte es theoretisch unendlich laufen.\nStartende oder endende Prozesse können die PID-Vergabe beeinflussen, da sie die Reihenfolge freier PIDs verändern – dadurch variiert die genaue Prozessanzahl je nach Systemzustand.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Blatt 02</span>"
    ]
  },
  {
    "objectID": "sh02/02.html#aufgabe-3",
    "href": "sh02/02.html#aufgabe-3",
    "title": "2  Blatt 02",
    "section": "Aufgabe 3",
    "text": "Aufgabe 3\n\nErklärung zur Ausgabe von ps -T -H\nDas C-Programm:\n#include &lt;stdio.h&gt;\n#include &lt;stdlib.h&gt;\n#include &lt;unistd.h&gt;\n\nint main(int argc, char const *argv[])\n{\n    if (fork() &gt; 0) sleep(1000);\n    else exit(0);\n    return 0;\n}\nerzeugt einen Kindprozess. Das Kind beendet sich sofort (exit(0)), während der Elternprozess 1000 Sekunden schläft (sleep(1000)).\nAblauf der Kommandos:\n\nDas Ausführen von ./test &:\n\nDas Programm läuft im Hintergrund.\nDie Shell gibt [1] 136620 aus → Prozess-ID (PID) 136620.\nDer Kindprozess wird erzeugt und terminiert sofort.\nDer Elternprozess schläft weiter.\nDa wait() nicht aufgerufen wird, wird der Kindprozess zu einem Zombie-Prozess.\n\nDas Ausführen von ./test und das drücken von &lt;Strg&gt;+Z danach:\n\nDas Programm startet im Vordergrund.\nMit &lt;Strg&gt;+Z wird es gestoppt.\nDie Shell zeigt: [2]+  Stopped ./test.\nAuch hier terminiert der Kindprozess sofort → Zombie-Prozess entsteht erneut.\n\n\nAusgabe von ps -T -H:\n    PID TTY      STAT   TIME COMMAND\n   1025 pts/0    Ss     0:00 /bin/bash --posix\n 136620 pts/0    S      0:00   ./test\n 136621 pts/0    Z      0:00     [test] &lt;defunct&gt;\n 136879 pts/0    T      0:00   ./test\n 136880 pts/0    Z      0:00     [test] &lt;defunct&gt;\n 136989 pts/0    R+     0:00   ps T -H\nErklärung:\n\n1025: Die Shell (bash), läuft im Terminal pts/0.\n136620: Erstes ./test-Programm, läuft im Hintergrund, schläft (S).\n136621: Dessen Kindprozess (Zombie, Z), da exit() aufgerufen wurde, aber vom Elternprozess nicht abgeholt.\n136879: Zweites ./test-Programm, wurde mit &lt;Strg+Z&gt; gestoppt (T).\n136880: Auch hier: Kindprozess wurde beendet, aber nicht „abgeholt“ → Zombie.\n136989: Der ps-Prozess selbst, der gerade die Ausgabe erzeugt (R+ = laufend im Vordergrund).\n\nDie Spalten\n\nPID: Prozess-ID.\nTTY: Terminal, dem der Prozess zugeordnet ist.\nSTAT: Prozessstatus:\n\nS: sleeping – schläft.\nT: stopped – gestoppt (z. B. durch SIGSTOP).\nZ: zombie – beendet, aber noch nicht „aufgeräumt“.\nR: running – aktuell laufend auf der CPU.\n+: Teil der Vordergrund-Prozessgruppe im Terminal.\n\nTIME: CPU-Zeit, die der Prozess verbraucht hat.\nCOMMAND: Der auszuführende Befehl.\n\n[test] &lt;defunct&gt; heißt, es handelt sich um einen Zombie-Prozess, dessen Kommandozeile nicht mehr verfügbar ist.\n\n\n\n\nProcess state Codes\nProzesszustände (erste Buchstaben):\n\n\n\n\n\n\n\n\nCode\nMeaning\nDescription\n\n\n\n\nR\nRunning\nCurrently running or ready to run (on CPU)\n\n\nS\nSleeping\nWaiting for an event (e.g., input, timer)\n\n\nD\nUninterruptible sleep\nWaiting for I/O (e.g., disk), cannot be killed easily\n\n\nT\nStopped\nProcess has been stopped (e.g., SIGSTOP, Ctrl+Z)\n\n\nZ\nZombie\nTerminated, but not yet cleaned up by its parent\n\n\nX\nDead\nProcess is terminated and should be gone (rarely shown)\n\n\n\nZusätzliche flags:\n\n\n\nFlag\nMeaning\n\n\n\n\n&lt;\nHigh priority (not nice to others)\n\n\nN\nLow priority (nice value &gt; 0)\n\n\nL\nHas pages locked in memory\n\n\ns\nSession leader\n\n\n+\nIn the foreground process group\n\n\nl\nMulti-threaded (using CLONE_THREAD)\n\n\np\nIn a separate process group\n\n\n\nZ.B. Ss+ beduetet: Sleeping (S), Session leader (s) & Foreground process (+).\n\n\nTiefe der Aktuellen Sitzung\nZuerst finden wir die PID der Aktuellen sitzung mit\necho $$\nheraus. Output: 1025.\nDanch führen wir das Command ps -eH | less aus und suchen im pager nach “1025”. In unserer Sitzung befand sich “bash” unter der Hierarchie:\n1 systemd\n    718 ssdm\n        766 ssdm-helper\n            859 i3\n                884 kitty\n                    1025 bash\nDas entspricht der Tiefe 5 des Prozessbaums.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Blatt 02</span>"
    ]
  },
  {
    "objectID": "sh02/02.html#aufgabe-4",
    "href": "sh02/02.html#aufgabe-4",
    "title": "2  Blatt 02",
    "section": "Aufgabe 4",
    "text": "Aufgabe 4\nÜbersicht der Varianten mit Signaturen:\n\n\n\n\n\n\n\nFunktion\nSignatur\n\n\n\n\nexecl\nint execl(const char *path, const char *arg0, ..., NULL);\n\n\nexecle\nint execle(const char *path, const char *arg0, ..., NULL, char *const envp[]);\n\n\nexeclp\nint execlp(const char *file, const char *arg0, ..., NULL);\n\n\nexecv\nint execv(const char *path, char *const argv[]);\n\n\nexecvp\nint execvp(const char *file, char *const argv[]);\n\n\nexecvpe\nint execvpe(const char *file, char *const argv[], char *const envp[]);\n\n\nexecve\nint execve(const char *filename, char *const argv[], char *const envp[]);\n\n\n\nWichtige Unterschiede:\n\nl = Argumente als Liste (z. B. execl)\nv = Argumente als Array (vector) (z. B. execv)\np = PATH-Suche aktiv (z. B. execvp)\ne = eigene Umgebung (envp[]) möglich (z. B. execle, execvpe)\nKein p = voller Pfad zur Datei nötig\nKein e = aktuelle Umgebungsvariablen werden übernommen\n\nWann welche Variante?\n\n\n\n\n\n\n\nVariante\nTypischer Einsatzzweck\n\n\n\n\nexecl\nFester Pfad und Argumente direkt im Code als Liste\n\n\nexecle\nWie execl, aber mit eigener Umgebung\n\n\nexeclp\nWie execl, aber PATH-Suche aktiviert (z. B. ls statt /bin/ls)\n\n\nexecv\nPfad bekannt, Argumente liegen als Array vor (z. B. aus main)\n\n\nexecvp\nWie execv, aber mit PATH-Suche (typisch für Shells)\n\n\nexecvpe\nWie execvp, aber mit eigener Umgebung (GNU-spezifisch)\n\n\nexecve\nLow-Level, volle Kontrolle über Pfad, Argumente und Umgebung",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Blatt 02</span>"
    ]
  },
  {
    "objectID": "sh02/02.html#aufgabe-5",
    "href": "sh02/02.html#aufgabe-5",
    "title": "2  Blatt 02",
    "section": "Aufgabe 5",
    "text": "Aufgabe 5\nEin Prozesswechsel (Context Switch) tritt auf, wenn das Betriebssystem (OS) die Ausführung eines Prozesses stoppt und zu einem anderen wechselt. Dabei entsteht Overhead, weil:\n\nDer aktuelle CPU-Zustand (Register, Programmzähler etc.) gespeichert werden muss\nDieser Zustand im Prozesskontrollblock (PCB) abgelegt wird\nDer Zustand des neuen Prozesses aus seinem PCB geladen wird\nDie Speicherverwaltungsstrukturen (z. B. Seitentabellen der MMU) aktualisiert werden müssen\nDer TLB (Translation Lookaside Buffer) meist ungültig wird und geleert werden muss\nWeitere OS-Daten wie Datei-Deskriptoren oder Signale angepasst werden müssen\n\nDer PCB enthält:\n\nProzess-ID, Zustand\nRegister, Programmzähler\nSpeicherinfos, geöffnete Dateien\nScheduling-Infos\n\nBeim Prozesswechsel speichert das OS den PCB des alten Prozesses und lädt den neuen, um eine korrekte Fortsetzung zu ermöglichen. Da jeder Prozess einen eigenen Adressraum besitzt, ist der Aufwand für das Umschalten entsprechend hoch.\nThreads desselben Prozesses teilen sich hingegen denselben Adressraum (also denselben Code, Heap, offene Dateien etc.). Das bedeutet:\n\nEs ist kein Wechsel des Adressraums nötig\nDie MMU- und TLB-Einträge bleiben gültig\nNur der Thread-spezifische Kontext (Register, Stack-Pointer etc.) muss gespeichert werden\n\nFazit: Ein Threadwechsel ist viel leichter und schneller**, da kein teurer Speicherverwaltungswechsel nötig ist.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Blatt 02</span>"
    ]
  },
  {
    "objectID": "sh02/02.html#aufgabe-6",
    "href": "sh02/02.html#aufgabe-6",
    "title": "2  Blatt 02",
    "section": "Aufgabe 6",
    "text": "Aufgabe 6\n\nIn der ursprünglichen Version werden alle Threads schnell hintereinander gestartet, ohne aufeinander zu warten. Da die Ausführung der Threads vom Scheduler (Betriebssystem) abhängt und parallel erfolgt, kann die Ausgabe beliebig vermischt erscheinen – z. B. kann ein Thread seine Nachricht „number: i“ ausgeben, noch bevor die Hauptfunktion „creating thread i“ gedruckt hat.\nIn der überarbeiteten Version hingegen wird jeder Thread direkt nach dem Start mit pthread_join wieder eingesammelt. Dadurch läuft immer nur ein Thread zur Zeit, und seine Ausgabe erfolgt vollständig, bevor der nächste beginnt. So entsteht eine streng sequentielle Ausgabe:\n\n„creating thread i“\n„number: i“\n„ending thread i“\n\nDiese einfache Struktur vermeidet Race Conditions und benötigt keine zusätzlichen Synchronisationsmechanismen wie Semaphoren oder Locks.\nÜberarbeitete Version (auch im zip als threads_example.c enthalten):\n\n\nthreads_example.c\n\n#include &lt;pthread.h&gt;\n#include &lt;stdio.h&gt; \n#include &lt;stdlib.h&gt;\n#include &lt;assert.h&gt;\n\n#define NUM_THREADS 200000\n\nvoid* TaskCode (void* argument)\n{\n   int tid = *((int*) argument);\n   printf(\"number: %d\\n\", tid);\n   printf(\"ending thread %d\\n\", tid);\n   return NULL;\n}\n\nint main()\n{\n   pthread_t thread;\n   int thread_arg;\n\n   for (int i = 0; i &lt; NUM_THREADS; i++) {\n      thread_arg = i;\n      printf(\"creating thread %d\\n\", i);\n      int rc = pthread_create(&thread, NULL, TaskCode, &thread_arg);\n      assert(rc == 0);\n      rc = pthread_join(thread, NULL);\n      assert(rc == 0);\n   }\n\n   return 0;\n}\n\nIn unserem System \\(N_{\\text{max}} \\approx 200000\\).\nIm folgenden Program wird TaskCode() \\(N_\\text{max}\\) mal in einer einfachen Schleife aufgerufen:\n#include &lt;pthread.h&gt;\n#include &lt;stdio.h&gt; \n#include &lt;stdlib.h&gt;\n#include &lt;assert.h&gt;\n\n#define NUM_THREADS 200000\n\nvoid* TaskCode (void* argument)\n{\n   int tid = *((int*) argument);\n   printf(\"number: %d\\n\", tid);\n   printf(\"ending thread %d\\n\", tid);\n   return NULL;\n}\n\nint main()\n{\n   for (int i = 0; i &lt; NUM_THREADS; i++) {\n      TaskCode(&i);\n   }\n\n   return 0;\n}\nDie Ausführung dieses Programs dauerte c. 2 Sekunden auf unserem System. D.h. die fehlenden zwei pthread_* aufrufe kosten\n\n8 Sekunden für 200000 Schleifen. Das entspricht c. 20 millisekunden pro pthread_* Aufruf.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Blatt 02</span>"
    ]
  },
  {
    "objectID": "sh03/03.html",
    "href": "sh03/03.html",
    "title": "3  Blatt 03",
    "section": "",
    "text": "Aufgabe 1\nErklärung:\nDieses Programm vermeidet das Race Condition-Problem, indem beide Threads einen synchronized-Block verwenden, der auf Counter.class synchronisiert ist. Das bedeutet:",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Blatt 03</span>"
    ]
  },
  {
    "objectID": "sh03/03.html#aufgabe-1",
    "href": "sh03/03.html#aufgabe-1",
    "title": "3  Blatt 03",
    "section": "",
    "text": "Die Ausgabe ist inkonsistent – bei mehreren Programmausführungen erscheinen unterschiedliche Werte für counter. Dies liegt an einer Race Condition, da beide Threads gleichzeitig und ohne Synchronisation auf die gemeinsame Variable counter zugreifen. Dadurch können Zwischenergebnisse überschrieben oder verloren gehen, je nachdem, wie der Scheduler die Threads abwechselnd ausführt.\nSynchronisierte Lösung (Java-Code):\n\npublic class Counter {\n    static int counter = 0;\n\n    public static class Counter_Thread_A extends Thread {\n        public void run() {\n            synchronized (Counter.class) {\n                counter = 5;\n                counter++;\n                counter++;\n                System.out.println(\"A-Counter: \" + counter);\n            }\n        }\n    }\n\n    public static class Counter_Thread_B extends Thread {\n        public void run() {\n            synchronized (Counter.class) {\n                counter = 6;\n                counter++;\n                counter++;\n                counter++;\n                counter++;\n                System.out.println(\"B-Counter: \" + counter);\n            }\n        }\n    }\n\n    public static void main(String[] args) {\n        Thread a = new Counter_Thread_A();\n        Thread b = new Counter_Thread_B();\n        a.start();\n        b.start();\n    }\n}\n\n\n\nNur ein Thread darf gleichzeitig den Block betreten.\nDer andere Thread muss warten, bis der erste fertig ist und den Lock freigibt.\nDadurch wird sichergestellt, dass keine gleichzeitigen Zugriffe auf die gemeinsame Variable counter stattfinden.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Blatt 03</span>"
    ]
  },
  {
    "objectID": "sh03/03.html#aufgabe-3",
    "href": "sh03/03.html#aufgabe-3",
    "title": "3  Blatt 03",
    "section": "Aufgabe 3",
    "text": "Aufgabe 3\n\nUnten folgt der Quellcode zur verbesserten Lösung des Producer-Consumer-Problems (pc2.c am Ende des Dokuments). In dieser Version wird Busy Waiting durch eine effiziente Synchronisation mithilfe eines Mutexes und einer Condition Variable ersetzt.\nDer Code befindet sich auch im beigefügten Zip-Archiv im Ordner A3. Dort kann das Programm wie folgt kompiliert und ausgeführt werden:\n\nmake  \n./pc2\nDiese Implementierung gewährleistet eine korrekte und effiziente Koordination zwischen Producer- und Consumer-Threads:\n\nDie gemeinsame Warteschlange wird durch einen Mutex geschützt.\nThreads, die auf eine Bedingung warten, verwenden pthread_cond_wait() innerhalb einer while-Schleife, um Spurious Wakeups korrekt zu behandeln.\nIst die Warteschlange leer, schlafen die Consumer, bis sie ein Signal erhalten; ist sie voll, wartet der Producer entsprechend.\nDurch das gezielte Aufwecken via pthread_cond_signal() oder pthread_cond_broadcast() wird unnötiger CPU-Verbrauch durch aktives Warten vermieden.\n\nInsgesamt ist diese Lösung robuster und skalierbarer als die ursprüngliche Variante mit Busy Waiting – insbesondere bei mehreren Consumer-Threads und höherer Auslastung.\n\n\n\npc2.c\n\n#include &lt;stdio.h&gt;\n#include &lt;stdlib.h&gt;\n#include &lt;pthread.h&gt;\n#include \"mylist.h\"\n// Mutex to protect access to the shared queue\npthread_mutex_t queue_lock;\n// Single condition variable used for both producers and consumers\npthread_cond_t cond_var;\n// Shared buffer (a custom linked list acting as a queue)\nlist_t buffer;\n// Counters for task management\nint count_proc = 0;\nint production_done = 0;\n/********************************************************/\n/* Function Declarations */\nstatic unsigned long fib(unsigned int n);\nstatic void create_data(elem_t **elem);\nstatic void *consumer_func(void *);\nstatic void *producer_func(void *);\n/********************************************************/\n/* Compute the nth Fibonacci number (CPU-intensive task) */\nstatic unsigned long fib(unsigned int n)\n{\n    if (n == 0 || n == 1) {\n        return n;\n    } else {\n        return fib(n - 1) + fib(n - 2);\n    }\n}\n/* Allocate and initialize a new task node */\nstatic void create_data(elem_t **elem)\n{\n    *elem = (elem_t*) malloc(sizeof(elem_t));\n    (*elem)-&gt;data = FIBONACCI_MAX;\n}\n/* Consumer thread function */\nstatic void *consumer_func(void *args) \n{\n    elem_t *elem;\n    while (1) {\n        pthread_mutex_lock(&queue_lock);\n        // Wait if the queue is empty and production is not yet complete\n        while (get_size(&buffer) == 0 && !production_done) {\n            pthread_cond_wait(&cond_var, &queue_lock);\n        }\n        // Exit condition: queue is empty and production has finished\n        if (get_size(&buffer) == 0 && production_done) {\n            pthread_mutex_unlock(&queue_lock);\n            break;\n        }\n        // Remove an item from the queue\n        remove_elem(&buffer, &elem);\n        // Wake up a potentially waiting producer\n        pthread_cond_signal(&cond_var);\n        pthread_mutex_unlock(&queue_lock);\n        // Process the task\n        fib(elem-&gt;data);\n        free(elem);\n        printf(\"item consumed\\n\");\n    }\n    return NULL;\n}\n/* Producer thread function */\nstatic void *producer_func(void *args) \n{\n    while (1) {\n        pthread_mutex_lock(&queue_lock);\n        // Wait if the buffer is full\n        while (get_size(&buffer) &gt;= MAX_QUEUE_LENGTH) {\n            pthread_cond_wait(&cond_var, &queue_lock);\n        }\n        if (count_proc &lt; MAX_COUNT) {\n            // Create and append a new task to the queue\n            elem_t *elem;\n            create_data(&elem);\n            append_elem(&buffer, elem);\n            count_proc++;\n            printf(\"item produced\\n\");\n            // Wake up one waiting consumer\n            pthread_cond_signal(&cond_var);\n        }\n        // If production is done, notify all consumers and exit\n        if (count_proc &gt;= MAX_COUNT) {\n            production_done = 1;\n            // Wake up all consumers waiting on cond_var so they can check the exit condition\n            pthread_cond_broadcast(&cond_var);\n            pthread_mutex_unlock(&queue_lock);\n            break;\n        }\n        pthread_mutex_unlock(&queue_lock);\n    }\n    return NULL;\n}\n/* Main function */\nint main (int argc, char *argv[])\n{\n    pthread_t cons_thread[NUM_CONSUMER];\n    pthread_t prod_thread;\n    int i;\n    // Initialize mutex and condition variable\n    pthread_mutex_init(&queue_lock, NULL);\n    pthread_cond_init(&cond_var, NULL);\n    init_list(&buffer);\n    // Start consumer threads\n    for (i = 0; i &lt; NUM_CONSUMER; i++) {\n        pthread_create(&cons_thread[i], NULL, &consumer_func, NULL);\n    }\n    // Start producer thread\n    pthread_create(&prod_thread, NULL, &producer_func, NULL);\n\n    // Wait for all consumer threads to finish\n    for (i = 0; i &lt; NUM_CONSUMER; i++) {\n        pthread_join(cons_thread[i], NULL);\n    }\n    // Wait for producer thread to finish\n    pthread_join(prod_thread, NULL);\n    // Cleanup\n    pthread_mutex_destroy(&queue_lock);\n    pthread_cond_destroy(&cond_var);\n    return 0;\n}\n\n\n\nLaufzeitvergleich von pc und pc2\nZur Überprüfung der Effizienzverbesserung durch den Einsatz von Condition Variables wurde folgendes Bash-Skript verwendet, das beide Programme je 10-mal ausführt und die durchschnittliche Laufzeit berechnet:\n#!/bin/bash\n\nRUNS=10\nPC=\"./pc\"\nPC2=\"./pc2\"\n\nmeasure_average_runtime() {\n    PROGRAM=$1\n    TOTAL=0\n    echo \"Running $PROGRAM...\"\n    for i in $(seq 1 $RUNS); do\n        START=$(date +%s.%N)\n        $PROGRAM &gt; /dev/null\n        END=$(date +%s.%N)\n        RUNTIME=$(echo \"$END - $START\" | bc)\n        echo \"  Run $i: $RUNTIME seconds\"\n        TOTAL=$(echo \"$TOTAL + $RUNTIME\" | bc)\n    done\n    AVG=$(echo \"scale=4; $TOTAL / $RUNS\" | bc)\n    echo \"Average runtime of $PROGRAM: $AVG seconds\"\n    echo\n}\n\necho \"Measuring $RUNS runs of $PC and $PC2...\"\necho\nmeasure_average_runtime $PC\nmeasure_average_runtime $PC2\nAusgeführt wurde das Skript mit:\n./benchmark_pc.sh\nDabei ergaben sich folgende Laufzeiten:\nMeasuring 10 runs of ./pc and ./pc2...\n\nRunning ./pc...\n  Run 1: 5.471139729 seconds\n  Run 2: 5.545249360 seconds\n  Run 3: 5.359090183 seconds\n  Run 4: 5.366634866 seconds\n  Run 5: 5.459910579 seconds\n  Run 6: 5.531161091 seconds\n  Run 7: 5.738575161 seconds\n  Run 8: 5.835055657 seconds\n  Run 9: 5.496744966 seconds\n  Run 10: 5.641529848 seconds\nAverage runtime of ./pc: 5.5445 seconds\n\nRunning ./pc2...\n  Run 1: 5.244080521 seconds\n  Run 2: 5.237442233 seconds\n  Run 3: 5.220517776 seconds\n  Run 4: 5.281094089 seconds\n  Run 5: 5.261722379 seconds\n  Run 6: 5.363685993 seconds\n  Run 7: 5.276107150 seconds\n  Run 8: 5.091557858 seconds\n  Run 9: 5.073267276 seconds\n  Run 10: 5.164472482 seconds\nAverage runtime of ./pc2: 5.2213 seconds\nDie Ergebnisse zeigen, dass pc2 im Schnitt etwas schneller ist als pc (5.22 s gegenüber 5.54 s), was den Effizienzgewinn durch den Verzicht auf aktives Warten bestätigt.\nDie Dateien benchmark_pc.sh und benchmark_results.txt befinden sich im Ordner A3 des ZIP-Archivs.\nZur Veranschaulichung wurde mit dem folgendnen Python script zusätzlich ein Diagramm erstellt, das die Laufzeiten von pc und pc2 über zehn Durchläufe hinweg zeigt. Die Durchschnittslinien verdeutlichen, dass pc2 im Mittel schneller und konsistenter ist als pc.\n\nimport matplotlib.pyplot as plt\n\n# Runtime data for each run (in seconds)\npc = [5.471139729, 5.545249360, 5.359090183, 5.366634866, 5.459910579,\n      5.531161091, 5.738575161, 5.835055657, 5.496744966, 5.641529848]\n\npc2 = [5.244080521, 5.237442233, 5.220517776, 5.281094089, 5.261722379,\n       5.363685993, 5.276107150, 5.091557858, 5.073267276, 5.164472482]\n\n# X-axis: run numbers\nruns = list(range(1, 11))\n\n# Calculate averages\navg_pc = sum(pc) / len(pc)\navg_pc2 = sum(pc2) / len(pc2)\n\n# Plot configuration\nplt.figure(figsize=(10, 6))\nplt.plot(runs, pc, marker='o', label='pc')\nplt.plot(runs, pc2, marker='o', label='pc2')\n\n# Average lines\nplt.axhline(avg_pc, color='red', linestyle='--', label=f'avg pc ({avg_pc:.3f}s)')\nplt.axhline(avg_pc2, color='green', linestyle='--', label=f'avg pc2 ({avg_pc2:.3f}s)')\n\n# Labels and title\nplt.xlabel('Run')\nplt.ylabel('Time (s)')\nplt.title('Runtime Comparison of pc vs. pc2')\nplt.xticks(runs)\nplt.grid(True)\nplt.legend()\nplt.tight_layout()\n\n# Display the plot\nplt.show()",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Blatt 03</span>"
    ]
  },
  {
    "objectID": "sh03/03.html#aufgabe-4",
    "href": "sh03/03.html#aufgabe-4",
    "title": "3  Blatt 03",
    "section": "Aufgabe 4",
    "text": "Aufgabe 4\n\nDie gegebene Implementierung kann zu einer Verletzung des gegenseitigen Ausschlusses führen, wenn zwei schreibende Threads gleichzeitig in die kritische Sektion gelangen.\nBeispiel: Angenommen N = 5. Thread A und Thread B rufen gleichzeitig lock_write() auf. Da der for-Loop nicht durch einen Mutex geschützt ist, können sich ihre wait(S)-Aufrufe gegenseitig durchmischen: A nimmt 1 Token → S = 4 B nimmt 1 Token → S = 3 A nimmt 1 → S = 2 B nimmt 1 → S = 1 … und so weiter. Wenn nun zufällig genug Tokens freigegeben werden (z. B. durch unlock_read()-Aufrufe), können beide Threads nacheinander die restlichen Semaphore erwerben und ihren Loop abschließen, ohne dass einer von ihnen jemals alle N Tokens exklusiv gehalten hat. Beide betreten anschließend die kritische Sektion, obwohl gegenseitiger Ausschluss nicht mehr gewährleistet ist.\nDas Problem wird behoben, indem ein zusätzlicher Mutex eingeführt wird, der verhindert, dass mehrere schreibende Threads gleichzeitig versuchen, die Semaphore S zu erwerben:\nS = Semaphore(N)\nM = Semaphore(1)  // neuer Mutex\n\ndef lock_read():\n    wait(S)\n\ndef unlock_read():\n    signal(S)\n\ndef lock_write():\n    wait(M)\n    for i in range(N): wait(S)\n    signal(M)\n\ndef unlock_write():\n    for i in range(N): signal(S)\nDurch den Mutex M ist sichergestellt, dass der Erwerb der Semaphore in lock_write() ausschließlich von einem Thread durchgeführt wird. So wird verhindert, dass mehrere schreibende Threads gleichzeitig in die kritische Sektion gelangen.\nHinweis: Diese Lösung stellt den gegenseitigen Ausschluss sicher, erlaubt jedoch theoretisch, dass ein schreibender Thread dauerhaft blockiert bleibt, wenn ständig neue Leser auftreten (Starvation). Für diese Aufgabe ist jedoch nur die Korrektur der Ausschlussverletzung relevant.\nDie Befehle upgrade_to_write() und downgrade_to_read() ermöglichen es einem Thread, während des laufenden Zugriffs die Art des Read-Write-Locks dynamisch zu wechseln – ohne dabei den kritischen Abschnitt vollständig zu verlassen. Dies verhindert Race Conditions und potenzielle Starvation.\nEin Thread, der upgrade_to_write() aufruft, hält bereits einen Lesezugriff (also eine Einheit der Semaphore S) und möchte exklusiven Schreibzugriff erhalten. Dafür müssen die verbleibenden N - 1 Einheiten erworben werden. Ein zusätzlicher Mutex M sorgt dafür, dass nicht mehrere Threads gleichzeitig versuchen, sich hochzustufen, was zu Deadlocks führen könnte.\nEin Thread, der downgrade_to_read() aufruft, hält alle N Einheiten (Schreibzugriff) und möchte auf geteilten Lesezugriff wechseln. Dazu werden N - 1 Einheiten freigegeben – eine Einheit bleibt erhalten.\nHinweis: Das hier verwendete Mutex M ist dasselbe wie in Teil b) und stellt sicher, dass nur ein Thread gleichzeitig exklusiven Zugriff auf die Semaphore S erwerben kann – sei es über lock_write() oder über upgrade_to_write().\nPseudocode:\nS = Semaphore(N)     // erlaubt bis zu N gleichzeitige Leser oder 1 Schreiber\nM = Semaphore(1)     // schützt exklusive Zugriffsversuche\n\ndef upgrade_to_write():\n    wait(M)\n    for i in range(N - 1):     // hält bereits 1 Einheit als Leser\n        wait(S)\n    signal(M)\n\ndef downgrade_to_read():\n    for i in range(N - 1):     // gibt N - 1 Einheiten frei, behält 1\n        signal(S)\nFazit: Diese Operationen garantieren einen sicheren Übergang zwischen Lese- und Schreibmodus, ohne Race Conditions oder Deadlocks, und basieren auf derselben Semaphor-Struktur wie in Teil b).",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Blatt 03</span>"
    ]
  },
  {
    "objectID": "sh04/04.html",
    "href": "sh04/04.html",
    "title": "4  Blatt 04",
    "section": "",
    "text": "Aufgabe 1",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Blatt 04</span>"
    ]
  },
  {
    "objectID": "sh04/04.html#aufgabe-1",
    "href": "sh04/04.html#aufgabe-1",
    "title": "4  Blatt 04",
    "section": "",
    "text": "Ein Nachteil benannter Pipes ist, dass sie manuell im Dateisystem erstellt und verwaltet werden müssen (z. B. mit mkfifo). Das macht die Handhabung aufwändiger und erfordert gegebenenfalls zusätzliche Aufräummaßnahmen.\nWenn zwei voneinander unabhängige Prozesse (z. B. zwei Terminals) Daten austauschen sollen, ist eine benannte Pipe erforderlich. Anonyme Pipes funktionieren nur zwischen verwandten Prozessen (z. B. Eltern-Kind).",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Blatt 04</span>"
    ]
  },
  {
    "objectID": "sh04/04.html#aufgabe-2",
    "href": "sh04/04.html#aufgabe-2",
    "title": "4  Blatt 04",
    "section": "Aufgabe 2",
    "text": "Aufgabe 2\n\nIm Win32-API ist ein Handle vom Typ:\ntypedef void* HANDLE;\nEs handelt sich also um einen Zeiger (bzw. zeigerbreiten Wert), der jedoch nicht dereferenziert werden soll. Ein Handle ist ein undurchsichtiger Verweis auf eine Ressource, die vom Windows-Kernel verwaltet wird – etwa eine Datei, ein Prozess, ein Event oder ein Fensterobjekt.\nWenn ein Programm zum Beispiel CreateFile() aufruft, gibt der Kernel einen solchen Handle zurück. Dieser verweist intern auf ein Objekt in der Handle-Tabelle des Prozesses. Diese Tabelle enthält Informationen wie Zugriffsrechte, aktuelle Dateiposition, Typ des Objekts usw.\nIm Unterschied zu Dateideskriptoren unter Unix/Linux (einfache Ganzzahlen) sind Win32-Handles allgemeiner gehalten und dienen zum Zugriff auf viele verschiedene Ressourcentypen – nicht nur auf Dateien.\nDie Umleitung der Standardausgabe erfolgt im Win32-API in zwei Schritten:\n\nEine Datei wird mit CreateFile() geöffnet oder erzeugt.\nDer Handle für STD_OUTPUT_HANDLE wird mit SetStdHandle() auf diesen Datei-Handle gesetzt.\n\nBeispiel:\n#include &lt;windows.h&gt;\n#include &lt;stdio.h&gt;\n\nint main() {\n    HANDLE hFile = CreateFile(\"output.txt\", GENERIC_WRITE, 0, NULL,\n                              CREATE_ALWAYS, FILE_ATTRIBUTE_NORMAL, NULL);\n\n    if (hFile == INVALID_HANDLE_VALUE) {\n        printf(\"Fehler beim Öffnen der Datei.\\n\");\n        return 1;\n    }\n\n    // Standardausgabe umleiten\n    SetStdHandle(STD_OUTPUT_HANDLE, hFile);\n\n    // Alles, was an STD_OUTPUT_HANDLE geschrieben wird, geht nun in die Datei\n    DWORD written;\n    WriteFile(GetStdHandle(STD_OUTPUT_HANDLE),\n              \"Hello redirected world!\\n\", 24, &written, NULL);\n\n    CloseHandle(hFile);\n    return 0;\n}\nDiese Umleitung wirkt sich auf Low-Level-Funktionen wie WriteFile() aus. Wenn man dagegen höhere Funktionen wie printf() oder std::cout umleiten will, muss zusätzlich die Laufzeitumgebung angepasst werden – etwa mit freopen() oder std::ios-Umleitungen.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Blatt 04</span>"
    ]
  },
  {
    "objectID": "sh04/04.html#aufgabe-3",
    "href": "sh04/04.html#aufgabe-3",
    "title": "4  Blatt 04",
    "section": "Aufgabe 3",
    "text": "Aufgabe 3\nDas Program: (Auch im Zip unter dem Verzeichniss A3 als reverse_pipechat.c enthalten)\n\n\nreverse_pipechat.c\n\n#include &lt;stdio.h&gt;\n#include &lt;stdlib.h&gt;\n#include &lt;unistd.h&gt;\n#include &lt;string.h&gt;\n#include &lt;sys/wait.h&gt;\n\n#define BUFFER_SIZE 1024\n\n// Utility: reverse a string in place\nvoid reverse_string(char *str) {\n    int len = strlen(str);\n    for (int i = 0; i &lt; len / 2; ++i) {\n        char tmp = str[i];\n        str[i] = str[len - 1 - i];\n        str[len - 1 - i] = tmp;\n    }\n}\n\nint main() {\n    int pipe_a_to_b[2]; // parent writes to child\n    int pipe_b_to_a[2]; // child writes to parent\n\n    if (pipe(pipe_a_to_b) == -1 || pipe(pipe_b_to_a) == -1) {\n        perror(\"pipe\");\n        exit(EXIT_FAILURE);\n    }\n\n    pid_t pid = fork();\n\n    if (pid &lt; 0) {\n        perror(\"fork\");\n        exit(EXIT_FAILURE);\n    }\n    else if (pid == 0) {\n        // Child process: Process B\n        close(pipe_a_to_b[1]); // Close write end of A→B\n        close(pipe_b_to_a[0]); // Close read end of B→A\n\n        char buffer[BUFFER_SIZE];\n\n        // Read message from parent\n        ssize_t bytes_read = read(pipe_a_to_b[0], buffer, BUFFER_SIZE - 1);\n        if (bytes_read &lt;= 0) {\n            perror(\"child read\");\n            exit(EXIT_FAILURE);\n        }\n\n        buffer[bytes_read] = '\\0'; // Null-terminate the string\n\n        reverse_string(buffer); // Reverse the string\n\n        // Send it back to parent\n        write(pipe_b_to_a[1], buffer, strlen(buffer));\n\n        // Close used pipe ends\n        close(pipe_a_to_b[0]);\n        close(pipe_b_to_a[1]);\n\n        exit(EXIT_SUCCESS);\n    } else {\n        // Parent process: Process A\n        close(pipe_a_to_b[0]); // Close read end of A→B\n        close(pipe_b_to_a[1]); // Close write end of B→A\n\n        char input[BUFFER_SIZE];\n        printf(\"Enter a string: \");\n        if (!fgets(input, BUFFER_SIZE, stdin)) {\n            perror(\"fgets\");\n            exit(EXIT_FAILURE);\n        }\n\n        // Remove newline if present\n        input[strcspn(input, \"\\n\")] = '\\0';\n\n        // Send input to child\n        write(pipe_a_to_b[1], input, strlen(input));\n\n        char reversed[BUFFER_SIZE];\n        ssize_t bytes_received = read(pipe_b_to_a[0], reversed, BUFFER_SIZE - 1);\n        if (bytes_received &lt;= 0) {\n            perror(\"parent read\");\n            exit(EXIT_FAILURE);\n        }\n\n        reversed[bytes_received] = '\\0'; // Null-terminate\n\n        printf(\"Reversed string: %s\\n\", reversed);\n\n        // Close used pipe ends\n        close(pipe_a_to_b[1]);\n        close(pipe_b_to_a[0]);\n\n        wait(NULL); // Wait for child to finish\n    }\n\n    return 0;\n}\n\nDas C-Programm demonstriert die Kommunikation zwischen zwei Prozessen über anonyme Pipes. Der Elternprozess (A) liest eine Zeichenkette von der Standardeingabe und sendet sie an den Kindprozess (B). Dieser kehrt die Zeichenkette um und schickt sie zurück. Der Elternprozess gibt das Ergebnis anschließend auf der Standardausgabe aus.\nTechnisch funktioniert das Programm so: Es erstellt zwei Pipes – eine für die Kommunikation von A nach B, die andere für die Rückrichtung. Nach dem Aufruf von fork() schließt jeder Prozess die jeweils nicht benötigten Enden der Pipes. Der Elternprozess sendet die Benutzereingabe an das Kind, das die Zeichenkette verarbeitet und die Antwort zurückschickt. Beide Prozesse verwenden read() und write() zur Datenübertragung und beenden sich danach.\nKompilieren und ausführen kann man das Programm unter Verzeichniss A3 mit:\nmake\n./pipe_example\nBeispielausgabe:\nEnter a string: hallo welt\nReversed string: tlew ollah",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Blatt 04</span>"
    ]
  },
  {
    "objectID": "sh04/04.html#aufgabe-4",
    "href": "sh04/04.html#aufgabe-4",
    "title": "4  Blatt 04",
    "section": "Aufgabe 4",
    "text": "Aufgabe 4\n\n:\n\nFragmentiuerung: Intern. (Eine geringe Anzahl von langlebigen Objekten existieren in einem Page, was zur internen Speicherverschwendung führt)\nDefinition der Internen Fragmentierung (in diesem Kontext): Speicherverschwendung innerhalb der Seite\n\n\n\n\ntiming diagram\n\n\n:\n\nsehr häufig: fast immer handelt es sich um einen Tradeoff, z.B. beim best fit vs first fit handelt es sich um das Tradeoff Speichereffizienz vs Zeiteffizienz\nTradeoff: Cache misses vs Interne Fragmentierung (Zeit vs Speicherplatz)\n\nKleine Seiten: Wenig interne Fragmentierung aber häufige Cache misses \\(\\Rightarrow\\) Zeitverschwendung\nGrosse Seiten: Seltene Cach misses aber sehr große interne Fragmentierung (da es häufig langlebige Objekte existieren) \\(\\Rightarrow\\) Speicherverscwendung",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Blatt 04</span>"
    ]
  },
  {
    "objectID": "sh04/04.html#aufgabe-5",
    "href": "sh04/04.html#aufgabe-5",
    "title": "4  Blatt 04",
    "section": "Aufgabe 5",
    "text": "Aufgabe 5\n\nInterne vs. externe Fragmentierung:\n\nInterne Fragmentierung entsteht, wenn ein Prozess mehr Speicher zugewiesen bekommt, als er tatsächlich benötigt – z. B. bei festen Block- oder Seitengrößen bleibt ungenutzter Speicher innerhalb des Blocks.\nExterne Fragmentierung tritt auf, wenn der freie Speicher zwar insgesamt groß genug ist, aber in viele kleine, nicht zusammenhängende Stücke aufgeteilt ist, sodass größere Prozesse keinen passenden Platz finden.\n\nLogische vs. physische Adressen:\n\nLogische Adressen (auch virtuelle Adressen) werden vom Prozess verwendet und beginnen meist bei 0 – sie sind unabhängig vom realen Speicherlayout.\nPhysische Adressen geben die tatsächliche Position im Hauptspeicher (RAM) an. Das Betriebssystem bzw. die Hardware (MMU) wandelt logische Adressen zur Laufzeit in physische Adressen um.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Blatt 04</span>"
    ]
  },
  {
    "objectID": "sh04/04.html#aufgabe-6",
    "href": "sh04/04.html#aufgabe-6",
    "title": "4  Blatt 04",
    "section": "Aufgabe 6",
    "text": "Aufgabe 6\nKurze erklärung zur Notation A:B: Der Segment der Größe A wurde der Speicherlücke der Größe B zugewiesen. (Das ist eindeutig, da die Größen der Segmente und der Lücken jeweils eindeutig sind.)\nDann:\n\nFirst fit:\n12:20\n11:18\n3:10\n5:7\nBest fit:\n12:12\n11:15\n3:4\n5:7\nWorst fit\n12:20\n11:18\n3:15\n5:12",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Blatt 04</span>"
    ]
  },
  {
    "objectID": "sh04/04.html#aufgabe-7",
    "href": "sh04/04.html#aufgabe-7",
    "title": "4  Blatt 04",
    "section": "Aufgabe 7",
    "text": "Aufgabe 7\nDa die Seitengröße 1 KB = 1024 Bytes = \\(2^{10}\\) beträgt, entsprechen die unteren 10 Bit des virtuellen Adresse die Offset, die restlichen höheren Bits geben die Seitennummer an.\n\nBerechnung der Seitennummern und Offsets:\n\n\n\nAdresse\nSeitennummer\nOffset\n\n\n\n\n2456\n2\n408\n\n\n16382\n15\n1022\n\n\n30000\n29\n304\n\n\n4385\n4\n289\n\n\n\n\n\nC-Code:\n// V - virtuelle Addresse, gegeben\nint p = V &gt;&gt; 10;        // Seitennummer\nint offset = V & 0x3FF; // Offset (2^10 - 1 = 1023)\nDurch die Verwendung von Bitoperationen ist die Berechnung effizient, da die Seitengröße eine Zweierpotenz ist.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Blatt 04</span>"
    ]
  },
  {
    "objectID": "sh05/05.html",
    "href": "sh05/05.html",
    "title": "5  Blatt 05",
    "section": "",
    "text": "Aufgabe 1",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Blatt 05</span>"
    ]
  },
  {
    "objectID": "sh05/05.html#aufgabe-1",
    "href": "sh05/05.html#aufgabe-1",
    "title": "5  Blatt 05",
    "section": "",
    "text": "Matrikelnummer: Seitennummer (Virtuelle Addresse)\nWohnaddresse: Rahmennummer (Physische Addresse)\nVerzeichniss: Seitentabelle (Index)\n\nkeine Entsprechung zum Offset\nMatrikelnummer hat 7 stellen: \\(10^7\\), d.h. 10 Mil Einträge.\nEs gibt 4.800 Wohnheimzimmer: relevanter Anteil = \\(\\frac{4.8 \\cdot 10^3}{10 \\cdot 10^6} \\approx 0.5 \\cdot 10^{-4} = 0.05 \\%\\)",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Blatt 05</span>"
    ]
  },
  {
    "objectID": "sh05/05.html#aufgabe-2",
    "href": "sh05/05.html#aufgabe-2",
    "title": "5  Blatt 05",
    "section": "Aufgabe 2",
    "text": "Aufgabe 2\n\nAdressübersetzung bei Paging mit Seitengröße \\(2^k\\)\nBei einem Paging-System mit fester Seitengröße von \\(2^k\\) Byte wird die virtuelle Adresse \\(V\\) wie folgt in eine physische Adresse übersetzt:\n\\[\n\\text{Physische Adresse} = \\left(F(V \\gg k) \\ll k\\right) + \\left(V \\& (2^k - 1)\\right)\n\\]\n\n\\(V \\gg k\\): virtuelle Seitennummer (Integer-Division durch \\(2^k\\))\n\\(V \\& (2^k - 1)\\): Offset innerhalb der Seite\n\\(F(n)\\): Seitentabelle, die virtuelle Seitennummer \\(n\\) auf Rahmennummer abbildet\n\n\n\nUmkehrung der Formel\nWenn die virtuelle Adresse \\(V\\) und die zugehörige physische Adresse \\(P\\) gegeben sind, lässt sich der Seitentabelleneintrag rekonstruieren:\n\\[\nF(V \\gg k) = \\frac{P - (V \\& (2^k - 1))}{2^k}\n\\]\nBei Seitengröße von 4 KB (\\(2^{12} = 4096\\)) gilt:\n\\[\nF(V \\gg 12) = \\frac{P - (V \\& 0xFFF)}{4096}\n\\]\n\n\nGegebene Zuordnungen\n\n\\(V = 8203 \\rightarrow P = 12229\\)\n\\(V = 4600 \\rightarrow P = 25080\\)\n\\(V = 16510 \\rightarrow P = 41086\\)\n\n\n\\(V = 8203 \\rightarrow P = 12229\\)\n\nVirtuelle Seite: \\(8203 \\gg 12 = \\lfloor \\frac{8203}{4096} \\rfloor = 2\\)\nOffset: \\(8203 \\& 0xFFF = 8203 \\mod 4096 = 8203 - 8192 = 11\\)\nFrame-Berechnung:\n\\[\nF(2) = \\frac{12229 - 11}{4096} = \\frac{12218}{4096} = 2\n\\]\n\n\\(V = 4600 \\rightarrow P = 25080\\)\n\nVirtuelle Seite: \\(4600 \\gg 12 = 1\\)\nOffset: \\(4600 \\& 0xFFF = 504\\)\nFrame-Berechnung:\n\\[\nF(1) = \\frac{25080 - 504}{4096} = \\frac{24576}{4096} = 6\n\\]\n\n\\(V = 16510 \\rightarrow P = 41086\\)\n\nVirtuelle Seite: \\(16510 \\gg 12 = \\lfloor \\frac{16510}{4096} \\rfloor = 4\\)\nOffset: \\(16510 \\& 0xFFF = 16510 - (4 \\cdot 4096) = 126\\)\nFrame-Berechnung:\n\\[\nF(4) = \\frac{41086 - 126}{4096} = \\frac{40960}{4096} = 10\n\\]\n\n\n\n\nEndgültige rekonstruierte Seitentabelle\n\n\n\nVirtuelle Seitennummer\nPhysischer Rahmen\n\n\n\n\n2\n2\n\n\n1\n6\n\n\n4\n10\n\n\n\n\n\nPython Implementierung\nDie Rekonstruktion der Tabelle kann mit python wie folgt implementiert werden:\n\ndef reconstruct_page_table(k, mappings):\n    page_size = 1 &lt;&lt; k  # 2^k\n    page_mask = page_size - 1\n\n    page_table = []\n    for virtual_address, physical_address in mappings:\n        virtual_page_number = virtual_address &gt;&gt; k\n        offset = virtual_address & page_mask\n        frame_number = (physical_address - offset) &gt;&gt; k\n        page_table.append((virtual_page_number, frame_number))\n\n    return page_table\n\nk = 12  # page size = 2^12 = 4096\nmappings = [\n    (8203, 12229),\n    (4600, 25080),\n    (16510, 41086)\n]\n\npage_table = reconstruct_page_table(k, mappings)\nfor vpn, frame in page_table:\n    print(f\"Virtuelle Seite {vpn} → Rahmen {frame}\")\n\nVirtuelle Seite 2 → Rahmen 2\nVirtuelle Seite 1 → Rahmen 6\nVirtuelle Seite 4 → Rahmen 10",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Blatt 05</span>"
    ]
  },
  {
    "objectID": "sh05/05.html#aufgabe-3",
    "href": "sh05/05.html#aufgabe-3",
    "title": "5  Blatt 05",
    "section": "Aufgabe 3",
    "text": "Aufgabe 3\n\nAusgangssituation:\nBetrachten wir die folgende C-Schleife auf einem System, bei dem gilt:\n\nint ist 4 Byte groß\nSeitengröße = 4 KB = 4096 Byte\nDer TLB (Translation Lookaside Buffer) hat 64 Einträge (d.h. er kann 64 Seiten zwischenspeichern)\n\nJede Seite enthält 1024 Integer, da 4 Byte pro Element. (4096 / 4 = 1024) Jeder Zugriff auf X[i] greift auf eine Speicherseite zu:\n\\[\n\\text{Seitenummer}(i) = \\left\\lfloor \\frac{i}{1024} \\right\\rfloor\n\\]\nMit den indizes \\(i\\):\n\\[\ni = 0, M, 2M, 3M, \\dots, \\text{solange } i &lt; N\n\\]\nSetzen wir \\(T = \\left\\lfloor \\frac{N - 1}{M} \\right\\rfloor\\), so gibt es \\(T + 1\\) Iterationen.\nJede Iteration greift also auf die Seite zu:\n\\[\n\\text{Seite}(j) = \\left\\lfloor \\frac{j \\cdot M}{1024} \\right\\rfloor \\quad \\text{für } j = 0, 1, ..., T\n\\]\nDie Anzahl der verschiedenen Seiten, die beim Schleifendurchlauf berührt werden, ist:\n\\[\n\\text{TLB\\_pages}(M, N) = \\left| \\left\\{ \\left\\lfloor \\frac{j \\cdot M}{1024} \\right\\rfloor \\;\\middle|\\; 0 \\le j \\le \\left\\lfloor \\frac{N - 1}{M} \\right\\rfloor \\right\\} \\right|\n\\]\nDies ist die Anzahl der eindeutig unterschiedlichen Seiten, auf die zugegriffen wird. TLB-Misses treten auf, wenn:\n\\[\n\\text{TLB\\_pages}(M, N) &gt; 64\n\\]\nDas heißt: Es werden mehr als 64 unterschiedliche virtuelle Seiten benötigt – mehr als der TLB speichern kann.\nBeispiele und wichtige Beobachtungen\n\nKleines M (z. B. M = 1):\n\n\nAufeinanderfolgende Elemente werden zugegriffen.\nAlle 1024 Zugriffe → 1 neue Seite.\nInsgesamt: ca. N / 1024 Seiten.\nTLB-Misses bei N &gt; 64 * 1024 = 65536.\n\n\nGroßes M (z. B. M ≥ 1024):\n\n\nJeder Zugriff springt zu einer neuen Seite.\nEs werden ca. N / M unterschiedliche Seiten berührt.\nWenn N / M &gt; 64, treten TLB-Misses auf.\n\n\nM teilt 1024 (z. B. M = 256, 512):\n\n\nMehrere Schleifendurchläufe landen auf derselben Seite.\nBeispiel: M = 256 → 4 Zugriffe pro Seite, bevor zur nächsten gewechselt wird.\nWeniger Seiten werden benötigt.\nWeniger TLB-Misses, auch bei großem N.\n\nSchlechtester Fall für den TLB\nTritt auf, wenn:\n\nM ≥ 1024 (jeder Zugriff auf eine neue Seite)\nund N / M &gt; 64 (mehr als 64 Seiten werden benötigt)\n\nDann wird bei jedem Zugriff eine andere Seite nachgeladen → viele TLB-Misses.\nFazit:\nDie Anzahl der verschiedenen Seiten, auf die beim Schleifendurchlauf zugegriffen wird, lautet:\n\\[\n\\boxed{\n\\text{TLB\\_pages}(M, N) = \\left| \\left\\{ \\left\\lfloor \\frac{j \\cdot M}{1024} \\right\\rfloor \\;\\middle|\\; 0 \\le j \\le \\left\\lfloor \\frac{N - 1}{M} \\right\\rfloor \\right\\} \\right|\n}\n\\]\n\nTLB-Misses treten auf, wenn TLB_pages(M, N) &gt; 64\nKleine M (vor allem wenn M &lt; 1024 und M ein Teiler von 1024 ist) → mehrere Zugriffe pro Seite → besseres TLB-Verhalten\nGroße M (≥ 1024) → jeder Zugriff auf neue Seite → mehr TLB-Misses\n\nDieses Verhalten kann mit der folgenden Python-funktion simuliert werden:\n\n\ndef tlb_pages(M, N, ints_per_page=1024):\n    \"\"\"\n    Simulates the number of distinct pages accessed in the loop:\n        for (int i = 0; i &lt; N; i += M) X[i]++;\n\n    Parameters:\n    - M: step size\n    - N: total number of elements in the array\n    - ints_per_page: number of integers that fit in a single page (default 4096 bytes / 4 bytes = 1024)\n\n    Returns:\n    - The number of distinct pages accessed\n    \"\"\"\n    pages = set()\n    for i in range(0, N, M):\n        page = i // ints_per_page\n        pages.add(page)\n    return len(pages)\n\nEinige Simulationen:\n\nprint(tlb_pages(1, 65536))        # Should be 64 → fills exactly 64 pages\nprint(tlb_pages(1024, 65536))     # Should be 64 → each access on a new page\nprint(tlb_pages(256, 65536))      # Should be 64 / 4 = 16 → reuse of pages\nprint(tlb_pages(2048, 65536))     # Should be 32 → skips every second page\nprint(tlb_pages(1, 100000))       # result: 98 → causes TLB misses\n\n64\n64\n64\n32\n98\n\n\n\nDas Verhalten des TLB ändert sich deutlich, wenn der Code mehrfach oder regelmäßig ausgeführt wird – etwa in einer oft aufgerufenen Funktion oder in einer heißen Schleife.\n\nTLB ist zustandsbehaftet und begrenzt\n\nEr kann nur eine bestimmte Anzahl an Seitenadressen speichern (z. B. 64).\nWird die Anzahl der zugreifenden Seiten pro Schleife &gt; 64, kommt es zu Ersetzungen (evictions), meist nach dem LRU-Prinzip.\n\nWiederholte Ausführung kann TLB verbessern – oder verschlechtern\n\nWenn dieselben Seiten wiederverwendet werden (z. B. bei N ≤ 64 * 1024), bleiben TLB-Einträge erhalten → nach der ersten Ausführung keine weiteren Misses.\nWenn mehr als 64 Seiten verwendet oder ständig neue Seiten benötigt werden, werden TLB-Einträge ständig ersetzt → TLB-Misses bei jedem Aufruf.\n\nTLB-Arbeitsmenge (working set)\n\nDie „TLB-Arbeitsmenge“ ist die Menge der Seiten, die eine Funktion während der Ausführung benötigt.\nPasst diese Menge vollständig in den TLB, funktioniert alles effizient.\nIst sie größer, kommt es zu wiederholten Zugriffen auf die Page Table → langsam.\n\nZugriffsart ist entscheidend\n\nKleine Schrittweite M → viele Zugriffe auf dieselbe Seite → hohe Wiederverwendung → TLB effizient.\nGroße Schrittweite M ≥ 1024 → jeder Zugriff auf eine neue Seite → hoher TLB-Druck, vor allem bei vielen Funktionsaufrufen.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Blatt 05</span>"
    ]
  },
  {
    "objectID": "sh05/05.html#aufgabe-4",
    "href": "sh05/05.html#aufgabe-4",
    "title": "5  Blatt 05",
    "section": "Aufgabe 4",
    "text": "Aufgabe 4\nBeim Wechsel zwischen Prozessen wird der TLB in der Regel geleert, da jeder Prozess einen eigenen virtuellen Adressraum mit einer eigenen Seitentabelle besitzt. Die im TLB gespeicherten Einträge des vorherigen Prozesses wären im neuen Kontext ungültig oder sogar sicherheitskritisch.\nBeim Wechsel zwischen Threads desselben Prozesses bleibt der TLB hingegen erhalten, da alle Threads denselben Adressraum und dieselbe Seitentabelle nutzen. Die vorhandenen TLB-Einträge bleiben daher gültig.\nModerne Systeme mit Address Space Identifiers (ASIDs) können einen vollständigen TLB-Flush beim Prozesswechsel vermeiden, indem sie TLB-Einträge pro Prozess kennzeichnen und nur die jeweils relevanten aktiv halten.\nKein Flush - Was kann Schiefgehen?\nWenn der TLB beim Kontextwechsel nicht geleert wird, kann es zu schwerwiegenden Sicherheitsproblemen kommen. Das folgende Beispiel zeigt, was konkret passieren kann:\nAngenommen, Prozess A greift auf die virtuelle Adresse 0x00400000 zu, welche in seiner Seitentabelle korrekt auf die physische Adresse 0x1A300000 abgebildet wird. Diese Übersetzung wird im TLB zwischengespeichert.\nNun findet ein Kontextwechsel zu Prozess B statt. Auch Prozess B verwendet die virtuelle Adresse 0x00400000, aber in seiner eigenen Seitentabelle sollte sie auf eine völlig andere physische Adresse zeigen, z. B. 0x2B400000.\nWenn der TLB nicht geleert wird, verwendet der Prozessor beim Zugriff durch Prozess B weiterhin die alte TLB-Eintragung von Prozess A. Das führt dazu, dass Prozess B auf den physischen Speicher von Prozess A zugreift.\nDie Folgen:\n\nSicherheitslücke: Prozess B kann sensible Daten von Prozess A einsehen.\nDatenkorruption: Schreibzugriffe von Prozess B verändern versehentlich die Daten von Prozess A.\nVerletzung der Speicherisolation: Ein zentrales Prinzip des Betriebssystems wird untergraben.\n\nUm das zu verhindern, wird der TLB beim Wechsel des Prozesses entweder vollständig geleert oder — bei moderner Hardware — es werden ASIDs (Address Space Identifiers) verwendet, die TLB-Einträge pro Prozess kennzeichnen und voneinander trennen.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Blatt 05</span>"
    ]
  },
  {
    "objectID": "sh05/05.html#aufgabe-6",
    "href": "sh05/05.html#aufgabe-6",
    "title": "5  Blatt 05",
    "section": "Aufgabe 6",
    "text": "Aufgabe 6\n\nEs gibt \\(\\frac{2^{32}}{2^{12}} = 2^{20}\\) Einträge in der Tabelle. Jeder Eintrag ist 4 Byte groß\n\n\\[\n\\Rightarrow \\text{ca. } 4\\,\\text{MB} \\text{ Größe der Seitentabelle pro Prozess}.\n\\]\n\nBei einer invertierten Seitentabelle gibt es genau einen Eintrag pro Frame. Deshalb entspricht das Verhältnis der Tabellengröße zum physischen Speicher exakt dem Verhältnis der Größe eines Eintrags zur Frame- bzw. Seitengröße:\n\n\\[\n\\Rightarrow \\frac{4\\,\\text{B}}{4\\,\\text{KB}} = \\frac{1}{1024}\n\\]",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Blatt 05</span>"
    ]
  },
  {
    "objectID": "sh05/05.html#aufgabe-7",
    "href": "sh05/05.html#aufgabe-7",
    "title": "5  Blatt 05",
    "section": "Aufgabe 7",
    "text": "Aufgabe 7\nGeg. sei ein System mit einem TLB und einer hierarchischen Seitentabelle mit \\(k\\) Stufen. Die TLB-Trefferquote sei \\(h\\), die Zugriffszeit auf den TLB sei \\(t_{\\text{TLB}}\\), und ein RAM-Zugriff dauere \\(t_{\\text{RAM}}\\). Um eine Seite im Speicher zu lesen, muss zunächst die Adresse übersetzt und anschließend auf die eigentlichen Daten zugegriffen werden. Es gibt zwei Fälle:\n\nBei einem TLB-Treffer (Wahrscheinlichkeit \\(h\\)) erfolgt die Übersetzung über den TLB, was \\(t_{\\text{TLB}}\\) dauert, gefolgt von einem Datenzugriff mit \\(t_{\\text{RAM}}\\). Gesamtzeit: \\(t_{\\text{TLB}} + t_{\\text{RAM}}\\)\nBei einem TLB-Fehlzugriff (Wahrscheinlichkeit \\(1 - h\\)) muss die Seitentabelle durchlaufen werden, wobei \\(k\\) RAM-Zugriffe nötig sind. Anschließend folgt der Zugriff auf die Daten mit weiteren \\(t_{\\text{RAM}}\\). Gesamtzeit: \\((k + 1) \\cdot t_{\\text{RAM}}\\)\n\nDie erwartete Zugriffszeit ergibt sich zu:\n\\[\nE = h(t_{\\text{TLB}} + t_{\\text{RAM}}) + (1 - h)(k + 1)t_{\\text{RAM}}\n\\]\nWenn \\(h\\) klein ist, dominiert der zweite Term, und der Zugriff ist im Mittel etwa \\((k + 1)\\)-mal so teuer wie bei einem TLB-Treffer.\nIn der Praxis tritt dieses Problem jedoch kaum auf, da reale Programme ausgeprägte Lokalität aufweisen. Aufgrund temporaler Lokalität (wiederholte Zugriffe auf kürzlich genutzte Seiten) und spatialer Lokalität (benachbarte Adressen werden gemeinsam genutzt, z. B. in Arrays) ist die TLB-Trefferquote typischerweise sehr hoch (oft über 95 %). Deshalb amortisiert sich die Existenz eines TLB deutlich.\nDas zugrunde liegende Modell ist jedoch in mehreren Punkten idealisiert und in der Praxis eingeschränkt:\n\nEs nimmt gleichverteilte Zugriffe auf alle Seitentabelleneinträge an, ignoriert also die reale Zugriffslokalität.\nSeitentabelleneinträge werden ggf. auch intern gecacht, was die Zahl tatsächlicher RAM-Zugriffe reduziert.\nEs berücksichtigt keine Nebeneffekte wie TLB-Flushes bei Kontextwechseln, Prefetching, oder andere Optimierungen der Speicherhierarchie.\n\nDaher ist das Modell gut geeignet für eine theoretische Analyse, aber es bildet die tatsächliche Effizienz realer Systeme nur vereinfacht ab.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Blatt 05</span>"
    ]
  },
  {
    "objectID": "sh05/05.html#aufgabe-8",
    "href": "sh05/05.html#aufgabe-8",
    "title": "5  Blatt 05",
    "section": "Aufgabe 8",
    "text": "Aufgabe 8\n\n\n\n3-level Seitentabelle\n\n\n\nDas Diagramm zeigt die Übersetzung einer 32-Bit-Adressierung in einem hypothetischen System mit einer dreistufigen Seitentabellenhierarchie und einer Seitengröße von 256 Byte (also \\(2^8\\)). Die logische Adresse wird dabei in vier gleich große Abschnitte zu je 8 Bit aufgeteilt:\n\nBits 31–24: Index in die oberste Tabelle (PD1)\nBits 23–16: Index in die zweite Tabelle (PD2)\nBits 15–8: Index in die dritte Tabelle (PT)\nBits 7–0: Offset innerhalb der Seite\n\nJede Tabelle hat \\(2^8 = 256\\) Einträge. Dies ergibt sich daraus, dass jeder Tabellenindex 8 Bit umfasst und damit 256 mögliche Positionen adressieren kann. Da alle drei Tabellenebenen mit 8-Bit-Indizes angesprochen werden, besitzen alle drei Stufen genau 256 Einträge. Die Offset-Breite von 8 Bit entspricht der Seitengröße von 256 Byte.\nDie 32-Bit-Adresse wird in vier Abschnitte zu je 8 Bit unterteilt. Jeder dieser Abschnitte dient als Index in eine bestimmte Stufe der Seitentabellenhierarchie:\n\nDer erste Abschnitt (Bits 31–24) indexiert einen Eintrag in der obersten Tabelle (PD1).\nDer zweite Abschnitt (Bits 23–16) indexiert die mittlere Tabelle (PD2).\nDer dritte Abschnitt (Bits 15–8) indexiert die unterste Tabelle (PT).\nDer vierte Abschnitt (Bits 7–0) ist der Offset innerhalb der Zielseite.\n\nJeder Eintrag in den Tabellen enthält eine physische Adresse, die zur nächsten Stufe führt:\n\nEin PD1-Eintrag enthält die physische Startadresse eines PD2-Tabellenrahmens.\nEin PD2-Eintrag enthält die Adresse eines PT-Tabellenrahmens.\nEin PT-Eintrag enthält die Adresse eines tatsächlichen physischen Seitenrahmens, also einer 256-Byte-Seite im Speicher.\n\nDurch schrittweises Nachschlagen entlang der drei Tabellenstufen wird so der physische Rahmen gefunden, in dem sich die gewünschte Adresse befindet. Der Offset gibt schließlich die genaue Position innerhalb dieser Seite an.\nGibt es signifikante Unterschiede zwischen den Stufen?\nJa, in der Funktion der Stufen:\n\nDie ersten beiden Stufen (PD1 und PD2) dienen rein der Navigation: Sie verweisen jeweils auf weitere Tabellen.\nErst die dritte Stufe (PT) enthält den tatsächlichen Verweis auf den physischen Speicherrahmen mit den Daten.\nAuch bei der Interpretation der Einträge kann es Unterschiede geben (z. B. zusätzliche Statusbits oder Flags auf unteren Ebenen), aber im Grundprinzip enthalten alle Einträge physische Adressen von Seitenrahmen — entweder von Tabellen oder von Daten.\n\nJede Tabelle hat maximal \\(2^8 = 256\\) Einträge pro Instanz. Da es sich um eine 3-stufige Hierarchie handelt, ergibt sich im Extremfall (voll belegter Adressraum):\n\nStufe 1 (PD1): 1 Tabelle × 256 Einträge\nStufe 2 (PD2): 256 Tabellen × 256 Einträge = 65 536\nStufe 3 (PT): 256 × 256 Tabellen × 256 Einträge = 16 777 216\n\nKumulative Maximalanzahl:\n\\[\n256 + 65\\,536 + 16\\,777\\,216 = 16\\,843\\,008 \\text{ Einträge}\n\\]\nSpeicherverbrauch bei 4 Byte pro Eintrag:\n\\[\n16\\,843\\,008 \\times 4\\ \\text{Bytes} = 67\\,372\\,032\\ \\text{Bytes} \\approx 64{,}25\\ \\text{MiB}\n\\]",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Blatt 05</span>"
    ]
  }
]